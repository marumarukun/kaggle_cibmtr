{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import config  # edit config.py as needed\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy as sp\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter, NelsonAalenFitter\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from metric import score  # edit metric.py as needed\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import rankdata\n",
    "from seed import seed_everything  # edit seed.py as needed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    DRY_RUN = False\n",
    "    EXP_NAME = config.EXP_NAME\n",
    "    AUTHOR = \"marumarukun\"\n",
    "    COMPETITION = config.KAGGLE_COMPETITION_NAME\n",
    "    DATA_PATH = config.COMP_DATASET_DIR\n",
    "    OUTPUT_DIR = config.OUTPUT_DIR\n",
    "    MODEL_PATH = config.OUTPUT_DIR / \"models\"  # モデル作成・実験時はこちらを使用\n",
    "    # MODEL_PATH = config.ARTIFACT_EXP_DIR(config.EXP_NAME) / \"models\"  # 提出時はこちらを使用\n",
    "    METHOD_LIST = [\"xgboost_cox\", \"catboost_cox\", \"lightgbm\", \"xgboost\", \"catboost\"]\n",
    "    SEED = 42\n",
    "    n_folds = 2 if DRY_RUN else 10\n",
    "    target_col_list = [\"y_kaplan\", \"y_nelson\"]\n",
    "    cox_target_col_list = [\"efs_time2\"]\n",
    "    # group_col = \"race_group\"  # Required for GroupKFold (edit as needed)\n",
    "    stratified_col = \"race_group_efs\"  # Required for StratifiedKFold (edit as needed)\n",
    "    num_boost_round = 100 if DRY_RUN else 1000000\n",
    "    early_stopping_round = 10 if DRY_RUN else 500  # 10÷lrで設定\n",
    "    verbose = 500\n",
    "\n",
    "    # https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
    "    regression_lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        # \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"seed\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
    "    regression_xgb_params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # \"eval_metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    regression_xgb_cox_params = {\n",
    "        \"objective\": \"survival:cox\",\n",
    "        \"eval_metric\": \"cox-nloglik\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 3,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 80,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://catboost.ai/docs/en/references/training-parameters/\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostregressor\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostclassifier\n",
    "    regression_cat_params = {\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"GPU\",  # CPU/GPU\n",
    "    }\n",
    "    regression_cat_cox_params = {\n",
    "        \"loss_function\": \"Cox\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"CPU\",  # CPU/GPU\n",
    "    }\n",
    "\n",
    "    model_weight_dict = {\"lightgbm\": 0.40, \"xgboost\": 0.30, \"catboost\": 0.30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "seed_everything(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "train = pl.read_csv(CFG.DATA_PATH / \"train.csv\", try_parse_dates=True)\n",
    "test = pl.read_csv(CFG.DATA_PATH / \"test.csv\", try_parse_dates=True)\n",
    "# make index column\n",
    "# train = train.with_row_index()\n",
    "# test = test.with_row_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Preprocess(ここに前処理や特徴量エンジニアリングを記述)\n",
    "# ====================================================\n",
    "def transform_survival_probability(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "\n",
    "\n",
    "def transform_cumulative_hazard(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    naf = NelsonAalenFitter()\n",
    "    naf.fit(durations=df[time_col], event_observed=df[event_col])\n",
    "    y = naf.cumulative_hazard_at_times(df[time_col]).to_numpy()\n",
    "    return -y\n",
    "\n",
    "\n",
    "def preprocess(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    output = df.clone()\n",
    "    # 欠損値のカウント（最初に行う）\n",
    "    output = output.with_columns(pl.sum_horizontal(pl.all().is_null()).alias(\"null_count\"))\n",
    "\n",
    "    # ドナーと患者の性別マッチング\n",
    "    output = output.with_columns(\n",
    "        pl.when(pl.col(\"sex_match\").str.contains_any([\"M-M\", \"F-F\"]))\n",
    "        .then(1)\n",
    "        .when(pl.col(\"sex_match\").is_null())\n",
    "        .then(None)\n",
    "        .otherwise(0)\n",
    "        .alias(\"is_sex_match\"),\n",
    "    )\n",
    "\n",
    "    # # hla再計算\n",
    "    # output = output.with_columns(\n",
    "    #     (\n",
    "    #         pl.col(\"hla_match_a_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_b_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_drb1_high\").fill_null(0)\n",
    "    #     ).alias(\"hla_nmdp_6\"),\n",
    "    #     (\n",
    "    #         pl.col(\"hla_match_a_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_b_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_drb1_low\").fill_null(0)\n",
    "    #     ).alias(\"hla_low_res_6\"),\n",
    "    #     (\n",
    "    #         pl.col(\"hla_match_a_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_b_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_drb1_high\").fill_null(0)\n",
    "    #     ).alias(\"hla_high_res_6\"),\n",
    "    #     (\n",
    "    #         pl.col(\"hla_match_a_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_b_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_c_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_drb1_low\").fill_null(0)\n",
    "    #     ).alias(\"hla_low_res_8\"),\n",
    "    #     (\n",
    "    #         pl.col(\"hla_match_a_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_b_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_c_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_drb1_high\").fill_null(0)\n",
    "    #     ).alias(\"hla_high_res_8\"),\n",
    "    #     (\n",
    "    #         pl.col(\"hla_match_a_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_b_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_c_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_drb1_low\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_dqb1_low\").fill_null(0)\n",
    "    #     ).alias(\"hla_low_res_10\"),\n",
    "    #     (\n",
    "    #         pl.col(\"hla_match_a_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_b_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_c_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_drb1_high\").fill_null(0)\n",
    "    #         + pl.col(\"hla_match_dqb1_high\").fill_null(0)\n",
    "    #     ).alias(\"hla_high_res_10\"),\n",
    "    # )\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "\n",
    "# apply Kaplan-Meier\n",
    "y_kaplan = transform_survival_probability(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_kaplan).alias(\"y_kaplan\"))\n",
    "\n",
    "# apply Nelson-Aalen\n",
    "y_nelson = transform_cumulative_hazard(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_nelson).alias(\"y_nelson\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Make fold column\n",
    "# ====================================================\n",
    "# race_group_efs列を作成\n",
    "train = train.with_columns((pl.col(\"race_group\").cast(str) + \"_\" + pl.col(\"efs\").cast(str)).alias(\"race_group_efs\"))\n",
    "\n",
    "fold_array = np.zeros(train.height)\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train, train[CFG.stratified_col]), start=1):\n",
    "    fold_array[val_idx] = fold\n",
    "train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n",
    "\n",
    "# fold_array = np.zeros(train.height)\n",
    "# kf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "# for fold, (_, val_idx) in enumerate(kf.split(train), start=1):\n",
    "#     fold_array[val_idx] = fold\n",
    "# train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10', 'null_count', 'is_sex_match']\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Column selection\n",
    "# ====================================================\n",
    "# Feature columns\n",
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"y_kaplan\", \"y_nelson\", \"fold\", \"race_group_efs\", \"efs_time2\"]\n",
    "FEATURES = [c for c in train.columns if c not in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35 CATEGORICAL FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n"
     ]
    }
   ],
   "source": [
    "# Categorical features\n",
    "CATS = []\n",
    "cat_count = 0\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype == pl.String:\n",
    "        cat_count += 1\n",
    "        CATS.append(c)\n",
    "print(f\"There are {cat_count} CATEGORICAL FEATURES: {CATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "for c in CATS:\n",
    "    # train, testで分けているのはkaggle対策（本来のtestにアクセスできないため）\n",
    "    # OrdinalEncoderを使用しているのはtestに未知の値あっても指定の値(-1)に変換できるため\n",
    "    oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    train = train.with_columns(\n",
    "        pl.Series(oe.fit_transform(train[c].fill_null(\"\").to_numpy().reshape(-1, 1)).reshape(-1))\n",
    "        .cast(pl.String)\n",
    "        .cast(pl.Categorical)\n",
    "        .alias(c)\n",
    "    )\n",
    "    test = test.with_columns(\n",
    "        pl.Series(oe.transform(test[c].fill_null(\"\").to_numpy().reshape(-1, 1)).reshape(-1))\n",
    "        .cast(pl.String)\n",
    "        .cast(pl.Categorical)\n",
    "        .alias(c)\n",
    "    )\n",
    "    # # 本来のtestにアクセスできるコンペではtrain, testを結合してLabelEncodeすればよい\n",
    "    # train_test_df = pl.concat([train, test], how=\"diagonal\")\n",
    "    # le = LabelEncoder()\n",
    "    # train_test_df = train_test_df.with_columns(\n",
    "    #     pl.Series(le.fit_transform(train_test_df[c].fill_null(\"\"))).alias(c)\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Survival Cox model用のターゲット作成\n",
    "# ====================================================\n",
    "# create cox model's target\n",
    "train = train.with_columns(\n",
    "    pl.when(pl.col(\"efs\") == 0).then(pl.col(\"efs_time\") * -1).otherwise(pl.col(\"efs_time\")).alias(\"efs_time2\")\n",
    ")\n",
    "\n",
    "# train[\"efs_time2\"] = train.efs_time.copy()\n",
    "# train.loc[train.efs == 0, \"efs_time2\"] *= -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Training functions\n",
    "# ====================================================\n",
    "def lightgbm_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    model = LGBMRegressor(\n",
    "        **CFG.regression_lgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=CFG.early_stopping_round),\n",
    "            lgb.log_evaluation(CFG.verbose),\n",
    "        ],\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def xgboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_cox_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_cox_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def gradient_boosting_model_cv_training(\n",
    "    method: str, train_df: pd.DataFrame, target_col_list: list, features: list, categorical_features: list\n",
    "):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    for target_col in target_col_list:\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{method} training fold {fold+1} {target_col}\")\n",
    "            x_train = train_df[train_df[\"fold\"] != fold + 1][features]\n",
    "            y_train = train_df[train_df[\"fold\"] != fold + 1][target_col]\n",
    "            x_valid = train_df[train_df[\"fold\"] == fold + 1][features]\n",
    "            y_valid = train_df[train_df[\"fold\"] == fold + 1][target_col]\n",
    "            if method == \"lightgbm\":\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            elif method == \"xgboost\":\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost\":\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            # Cox models\n",
    "            elif method == \"xgboost_cox\":\n",
    "                model, valid_pred = xgboost_cox_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost_cox\":\n",
    "                model, valid_pred = catboost_cox_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "            # Save best model\n",
    "            save_model_path = (\n",
    "                CFG.MODEL_PATH / f\"{method}_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\"\n",
    "            )\n",
    "            save_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            pickle.dump(\n",
    "                model,\n",
    "                open(\n",
    "                    save_model_path,\n",
    "                    \"wb\",\n",
    "                ),\n",
    "            )\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[train_df[\"fold\"] == fold + 1] = valid_pred\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_predictions_df = pd.DataFrame()\n",
    "        oof_predictions_df[\"ID\"] = train_df[\"ID\"].values\n",
    "        oof_predictions_df[\"prediction\"] = oof_predictions\n",
    "        oof_predictions_df.to_csv(\n",
    "            CFG.OUTPUT_DIR / f\"oof_{method}_{target_col}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\", index=False\n",
    "        )\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        y_true = train_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        m = score(y_true.copy(), oof_predictions_df.copy(), \"ID\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{method} our out of folds CV score is {m}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 882\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606188\n",
      "Training until validation scores don't improve for 500 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500]\tvalid_0's l2: 0.0245586\n",
      "[1000]\tvalid_0's l2: 0.0242751\n",
      "[1500]\tvalid_0's l2: 0.0242783\n",
      "Early stopping, best iteration is:\n",
      "[1163]\tvalid_0's l2: 0.0242285\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606660\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0237896\n",
      "[1000]\tvalid_0's l2: 0.0236395\n",
      "Early stopping, best iteration is:\n",
      "[961]\tvalid_0's l2: 0.0236247\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606506\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0237476\n",
      "[1000]\tvalid_0's l2: 0.0234223\n",
      "[1500]\tvalid_0's l2: 0.0233561\n",
      "Early stopping, best iteration is:\n",
      "[1477]\tvalid_0's l2: 0.0233487\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606093\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246306\n",
      "[1000]\tvalid_0's l2: 0.0243421\n",
      "[1500]\tvalid_0's l2: 0.0242934\n",
      "[2000]\tvalid_0's l2: 0.0242459\n",
      "[2500]\tvalid_0's l2: 0.0242566\n",
      "Early stopping, best iteration is:\n",
      "[2337]\tvalid_0's l2: 0.0242395\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606420\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246044\n",
      "[1000]\tvalid_0's l2: 0.0244684\n",
      "[1500]\tvalid_0's l2: 0.0244346\n",
      "[2000]\tvalid_0's l2: 0.024455\n",
      "Early stopping, best iteration is:\n",
      "[1672]\tvalid_0's l2: 0.0244179\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606450\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0239831\n",
      "[1000]\tvalid_0's l2: 0.0239323\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's l2: 0.0239012\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605974\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246908\n",
      "[1000]\tvalid_0's l2: 0.024416\n",
      "[1500]\tvalid_0's l2: 0.0244096\n",
      "Early stopping, best iteration is:\n",
      "[1366]\tvalid_0's l2: 0.0243884\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605772\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246274\n",
      "[1000]\tvalid_0's l2: 0.0242934\n",
      "[1500]\tvalid_0's l2: 0.0242536\n",
      "[2000]\tvalid_0's l2: 0.0242644\n",
      "[2500]\tvalid_0's l2: 0.024254\n",
      "Early stopping, best iteration is:\n",
      "[2272]\tvalid_0's l2: 0.0242175\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606044\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0247866\n",
      "[1000]\tvalid_0's l2: 0.0246338\n",
      "[1500]\tvalid_0's l2: 0.0246235\n",
      "Early stopping, best iteration is:\n",
      "[1329]\tvalid_0's l2: 0.0245964\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605780\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0244455\n",
      "[1000]\tvalid_0's l2: 0.024086\n",
      "[1500]\tvalid_0's l2: 0.0239606\n",
      "[2000]\tvalid_0's l2: 0.0239296\n",
      "Early stopping, best iteration is:\n",
      "[1977]\tvalid_0's l2: 0.0239246\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.672958237804527\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 882\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539329\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0587616\n",
      "[1000]\tvalid_0's l2: 0.0581525\n",
      "[1500]\tvalid_0's l2: 0.0581657\n",
      "Early stopping, best iteration is:\n",
      "[1181]\tvalid_0's l2: 0.0580944\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538669\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0572351\n",
      "[1000]\tvalid_0's l2: 0.0568617\n",
      "[1500]\tvalid_0's l2: 0.0568299\n",
      "[2000]\tvalid_0's l2: 0.05691\n",
      "Early stopping, best iteration is:\n",
      "[1680]\tvalid_0's l2: 0.056763\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538856\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0563962\n",
      "[1000]\tvalid_0's l2: 0.0556066\n",
      "[1500]\tvalid_0's l2: 0.0556739\n",
      "Early stopping, best iteration is:\n",
      "[1053]\tvalid_0's l2: 0.0555462\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539416\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0587299\n",
      "[1000]\tvalid_0's l2: 0.0581886\n",
      "[1500]\tvalid_0's l2: 0.058031\n",
      "Early stopping, best iteration is:\n",
      "[1382]\tvalid_0's l2: 0.0580039\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539032\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0585421\n",
      "[1000]\tvalid_0's l2: 0.0582934\n",
      "Early stopping, best iteration is:\n",
      "[984]\tvalid_0's l2: 0.0582642\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538973\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0576411\n",
      "[1000]\tvalid_0's l2: 0.0574417\n",
      "Early stopping, best iteration is:\n",
      "[966]\tvalid_0's l2: 0.057431\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539673\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0589786\n",
      "[1000]\tvalid_0's l2: 0.0585482\n",
      "[1500]\tvalid_0's l2: 0.0586145\n",
      "Early stopping, best iteration is:\n",
      "[1186]\tvalid_0's l2: 0.0584854\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539898\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0584779\n",
      "[1000]\tvalid_0's l2: 0.0578918\n",
      "[1500]\tvalid_0's l2: 0.0579176\n",
      "Early stopping, best iteration is:\n",
      "[1162]\tvalid_0's l2: 0.0578441\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539493\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0589524\n",
      "[1000]\tvalid_0's l2: 0.0586769\n",
      "Early stopping, best iteration is:\n",
      "[909]\tvalid_0's l2: 0.0586463\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539971\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0580336\n",
      "[1000]\tvalid_0's l2: 0.0570677\n",
      "[1500]\tvalid_0's l2: 0.0566853\n",
      "[2000]\tvalid_0's l2: 0.0567038\n",
      "Early stopping, best iteration is:\n",
      "[1542]\tvalid_0's l2: 0.0566575\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.6762156520632903\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17639\n",
      "[500]\tvalidation_0-rmse:0.15693\n",
      "[1000]\tvalidation_0-rmse:0.15583\n",
      "[1500]\tvalidation_0-rmse:0.15553\n",
      "[2000]\tvalidation_0-rmse:0.15574\n",
      "[2308]\tvalidation_0-rmse:0.15581\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17322\n",
      "[500]\tvalidation_0-rmse:0.15474\n",
      "[1000]\tvalidation_0-rmse:0.15382\n",
      "[1500]\tvalidation_0-rmse:0.15369\n",
      "[1922]\tvalidation_0-rmse:0.15382\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17501\n",
      "[500]\tvalidation_0-rmse:0.15454\n",
      "[1000]\tvalidation_0-rmse:0.15327\n",
      "[1500]\tvalidation_0-rmse:0.15309\n",
      "[2000]\tvalidation_0-rmse:0.15328\n",
      "[2166]\tvalidation_0-rmse:0.15335\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17815\n",
      "[500]\tvalidation_0-rmse:0.15732\n",
      "[1000]\tvalidation_0-rmse:0.15585\n",
      "[1500]\tvalidation_0-rmse:0.15550\n",
      "[2000]\tvalidation_0-rmse:0.15554\n",
      "[2078]\tvalidation_0-rmse:0.15558\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17425\n",
      "[500]\tvalidation_0-rmse:0.15726\n",
      "[1000]\tvalidation_0-rmse:0.15662\n",
      "[1500]\tvalidation_0-rmse:0.15643\n",
      "[2000]\tvalidation_0-rmse:0.15657\n",
      "[2099]\tvalidation_0-rmse:0.15653\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17421\n",
      "[500]\tvalidation_0-rmse:0.15519\n",
      "[1000]\tvalidation_0-rmse:0.15454\n",
      "[1456]\tvalidation_0-rmse:0.15456\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17675\n",
      "[500]\tvalidation_0-rmse:0.15820\n",
      "[1000]\tvalidation_0-rmse:0.15721\n",
      "[1500]\tvalidation_0-rmse:0.15728\n",
      "[1766]\tvalidation_0-rmse:0.15734\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17938\n",
      "[500]\tvalidation_0-rmse:0.15691\n",
      "[1000]\tvalidation_0-rmse:0.15561\n",
      "[1500]\tvalidation_0-rmse:0.15536\n",
      "[2000]\tvalidation_0-rmse:0.15536\n",
      "[2184]\tvalidation_0-rmse:0.15545\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17816\n",
      "[500]\tvalidation_0-rmse:0.15799\n",
      "[1000]\tvalidation_0-rmse:0.15698\n",
      "[1500]\tvalidation_0-rmse:0.15704\n",
      "[1715]\tvalidation_0-rmse:0.15713\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17741\n",
      "[500]\tvalidation_0-rmse:0.15707\n",
      "[1000]\tvalidation_0-rmse:0.15530\n",
      "[1500]\tvalidation_0-rmse:0.15482\n",
      "[2000]\tvalidation_0-rmse:0.15453\n",
      "[2500]\tvalidation_0-rmse:0.15446\n",
      "[3000]\tvalidation_0-rmse:0.15462\n",
      "[3005]\tvalidation_0-rmse:0.15462\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6722085282189766\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27150\n",
      "[500]\tvalidation_0-rmse:0.24251\n",
      "[1000]\tvalidation_0-rmse:0.24094\n",
      "[1500]\tvalidation_0-rmse:0.24061\n",
      "[1762]\tvalidation_0-rmse:0.24067\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26740\n",
      "[500]\tvalidation_0-rmse:0.23968\n",
      "[1000]\tvalidation_0-rmse:0.23829\n",
      "[1500]\tvalidation_0-rmse:0.23812\n",
      "[1818]\tvalidation_0-rmse:0.23807\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26940\n",
      "[500]\tvalidation_0-rmse:0.23823\n",
      "[1000]\tvalidation_0-rmse:0.23657\n",
      "[1500]\tvalidation_0-rmse:0.23616\n",
      "[2000]\tvalidation_0-rmse:0.23644\n",
      "[2166]\tvalidation_0-rmse:0.23659\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27373\n",
      "[500]\tvalidation_0-rmse:0.24305\n",
      "[1000]\tvalidation_0-rmse:0.24095\n",
      "[1500]\tvalidation_0-rmse:0.24044\n",
      "[2000]\tvalidation_0-rmse:0.24049\n",
      "[2364]\tvalidation_0-rmse:0.24043\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26868\n",
      "[500]\tvalidation_0-rmse:0.24242\n",
      "[1000]\tvalidation_0-rmse:0.24148\n",
      "[1500]\tvalidation_0-rmse:0.24148\n",
      "[1732]\tvalidation_0-rmse:0.24163\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26903\n",
      "[500]\tvalidation_0-rmse:0.24037\n",
      "[1000]\tvalidation_0-rmse:0.23965\n",
      "[1435]\tvalidation_0-rmse:0.23994\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27211\n",
      "[500]\tvalidation_0-rmse:0.24409\n",
      "[1000]\tvalidation_0-rmse:0.24270\n",
      "[1500]\tvalidation_0-rmse:0.24263\n",
      "[1787]\tvalidation_0-rmse:0.24283\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27535\n",
      "[500]\tvalidation_0-rmse:0.24213\n",
      "[1000]\tvalidation_0-rmse:0.24013\n",
      "[1500]\tvalidation_0-rmse:0.23963\n",
      "[2000]\tvalidation_0-rmse:0.23975\n",
      "[2350]\tvalidation_0-rmse:0.23996\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27381\n",
      "[500]\tvalidation_0-rmse:0.24323\n",
      "[1000]\tvalidation_0-rmse:0.24200\n",
      "[1500]\tvalidation_0-rmse:0.24216\n",
      "[1723]\tvalidation_0-rmse:0.24209\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27295\n",
      "[500]\tvalidation_0-rmse:0.24217\n",
      "[1000]\tvalidation_0-rmse:0.23960\n",
      "[1500]\tvalidation_0-rmse:0.23881\n",
      "[2000]\tvalidation_0-rmse:0.23893\n",
      "[2136]\tvalidation_0-rmse:0.23897\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6755384377204804\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_kaplan\n",
      "0:\tlearn: 0.1762565\ttest: 0.1763624\tbest: 0.1763624 (0)\ttotal: 13.1ms\tremaining: 3h 38m 10s\n",
      "500:\tlearn: 0.1513720\ttest: 0.1580527\tbest: 0.1580518 (499)\ttotal: 5.85s\tremaining: 3h 14m 36s\n",
      "1000:\tlearn: 0.1461889\ttest: 0.1566658\tbest: 0.1566533 (995)\ttotal: 12s\tremaining: 3h 18m 53s\n",
      "1500:\tlearn: 0.1422160\ttest: 0.1560282\tbest: 0.1560282 (1500)\ttotal: 18.2s\tremaining: 3h 21m 33s\n",
      "2000:\tlearn: 0.1388201\ttest: 0.1558004\tbest: 0.1557884 (1947)\ttotal: 24.2s\tremaining: 3h 20m 47s\n",
      "2500:\tlearn: 0.1356700\ttest: 0.1556734\tbest: 0.1556623 (2472)\ttotal: 30.4s\tremaining: 3h 22m 9s\n",
      "3000:\tlearn: 0.1328834\ttest: 0.1554872\tbest: 0.1554841 (2992)\ttotal: 36.5s\tremaining: 3h 22m 18s\n",
      "3500:\tlearn: 0.1301959\ttest: 0.1554123\tbest: 0.1553951 (3468)\ttotal: 42.5s\tremaining: 3h 21m 48s\n",
      "4000:\tlearn: 0.1276687\ttest: 0.1553265\tbest: 0.1553261 (3992)\ttotal: 48.3s\tremaining: 3h 20m 34s\n",
      "4500:\tlearn: 0.1253720\ttest: 0.1553453\tbest: 0.1553181 (4012)\ttotal: 54.3s\tremaining: 3h 20m 12s\n",
      "bestTest = 0.1553181475\n",
      "bestIteration = 4012\n",
      "Shrink model to first 4013 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_kaplan\n",
      "0:\tlearn: 0.1766079\ttest: 0.1731950\tbest: 0.1731950 (0)\ttotal: 9.34ms\tremaining: 2h 35m 44s\n",
      "500:\tlearn: 0.1515206\ttest: 0.1556704\tbest: 0.1556704 (500)\ttotal: 5.6s\tremaining: 3h 6m 18s\n",
      "1000:\tlearn: 0.1464432\ttest: 0.1546741\tbest: 0.1546710 (993)\ttotal: 11.9s\tremaining: 3h 17m 15s\n",
      "1500:\tlearn: 0.1427198\ttest: 0.1544828\tbest: 0.1544828 (1500)\ttotal: 18.2s\tremaining: 3h 22m 4s\n",
      "2000:\tlearn: 0.1394797\ttest: 0.1542931\tbest: 0.1542792 (1932)\ttotal: 24s\tremaining: 3h 19m 14s\n",
      "2500:\tlearn: 0.1365463\ttest: 0.1541044\tbest: 0.1541020 (2489)\ttotal: 30.2s\tremaining: 3h 20m 50s\n",
      "3000:\tlearn: 0.1337984\ttest: 0.1540319\tbest: 0.1540090 (2914)\ttotal: 36.1s\tremaining: 3h 19m 57s\n",
      "bestTest = 0.15400896\n",
      "bestIteration = 2914\n",
      "Shrink model to first 2915 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_kaplan\n",
      "0:\tlearn: 0.1764088\ttest: 0.1750198\tbest: 0.1750198 (0)\ttotal: 11.2ms\tremaining: 3h 6m 41s\n",
      "500:\tlearn: 0.1515135\ttest: 0.1558871\tbest: 0.1558871 (500)\ttotal: 5.37s\tremaining: 2h 58m 40s\n",
      "1000:\tlearn: 0.1462435\ttest: 0.1545906\tbest: 0.1545824 (997)\ttotal: 10.8s\tremaining: 2h 59m 32s\n",
      "1500:\tlearn: 0.1423217\ttest: 0.1540298\tbest: 0.1540298 (1500)\ttotal: 16.2s\tremaining: 2h 59m 11s\n",
      "2000:\tlearn: 0.1389840\ttest: 0.1538302\tbest: 0.1538275 (1997)\ttotal: 21.4s\tremaining: 2h 58m\n",
      "2500:\tlearn: 0.1360555\ttest: 0.1537349\tbest: 0.1537303 (2489)\ttotal: 27.3s\tremaining: 3h 1m 10s\n",
      "3000:\tlearn: 0.1333345\ttest: 0.1535476\tbest: 0.1535476 (3000)\ttotal: 33.1s\tremaining: 3h 3m 21s\n",
      "3500:\tlearn: 0.1308207\ttest: 0.1534374\tbest: 0.1534259 (3482)\ttotal: 39.1s\tremaining: 3h 5m 24s\n",
      "4000:\tlearn: 0.1283987\ttest: 0.1534613\tbest: 0.1534022 (3646)\ttotal: 45.2s\tremaining: 3h 7m 23s\n",
      "bestTest = 0.1534022065\n",
      "bestIteration = 3646\n",
      "Shrink model to first 3647 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_kaplan\n",
      "0:\tlearn: 0.1760690\ttest: 0.1781163\tbest: 0.1781163 (0)\ttotal: 8.39ms\tremaining: 2h 19m 46s\n",
      "500:\tlearn: 0.1510184\ttest: 0.1586641\tbest: 0.1586641 (500)\ttotal: 5.45s\tremaining: 3h 1m 18s\n",
      "1000:\tlearn: 0.1458109\ttest: 0.1571779\tbest: 0.1571777 (999)\ttotal: 11s\tremaining: 3h 3m 5s\n",
      "1500:\tlearn: 0.1419099\ttest: 0.1565073\tbest: 0.1565008 (1466)\ttotal: 17.2s\tremaining: 3h 10m 31s\n",
      "2000:\tlearn: 0.1383160\ttest: 0.1561365\tbest: 0.1561348 (1997)\ttotal: 23.4s\tremaining: 3h 14m 8s\n",
      "2500:\tlearn: 0.1352121\ttest: 0.1560044\tbest: 0.1559868 (2409)\ttotal: 29.3s\tremaining: 3h 14m 26s\n",
      "3000:\tlearn: 0.1323155\ttest: 0.1557569\tbest: 0.1557510 (2999)\ttotal: 35.6s\tremaining: 3h 16m 53s\n",
      "3500:\tlearn: 0.1296443\ttest: 0.1557160\tbest: 0.1556935 (3174)\ttotal: 41.2s\tremaining: 3h 15m 27s\n",
      "bestTest = 0.1556934703\n",
      "bestIteration = 3174\n",
      "Shrink model to first 3175 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_kaplan\n",
      "0:\tlearn: 0.1764712\ttest: 0.1742453\tbest: 0.1742453 (0)\ttotal: 7.94ms\tremaining: 2h 12m 24s\n",
      "500:\tlearn: 0.1512013\ttest: 0.1577415\tbest: 0.1577408 (499)\ttotal: 5.07s\tremaining: 2h 48m 35s\n",
      "1000:\tlearn: 0.1458925\ttest: 0.1570228\tbest: 0.1570228 (1000)\ttotal: 11.1s\tremaining: 3h 5m 25s\n",
      "1500:\tlearn: 0.1418082\ttest: 0.1568826\tbest: 0.1568616 (1466)\ttotal: 17.3s\tremaining: 3h 12m 16s\n",
      "2000:\tlearn: 0.1383245\ttest: 0.1567876\tbest: 0.1567857 (1999)\ttotal: 22.8s\tremaining: 3h 9m 7s\n",
      "2500:\tlearn: 0.1351907\ttest: 0.1566998\tbest: 0.1566998 (2500)\ttotal: 28.5s\tremaining: 3h 9m 40s\n",
      "3000:\tlearn: 0.1322327\ttest: 0.1567078\tbest: 0.1566923 (2931)\ttotal: 34.3s\tremaining: 3h 10m 8s\n",
      "3500:\tlearn: 0.1294172\ttest: 0.1567118\tbest: 0.1566834 (3044)\ttotal: 40.2s\tremaining: 3h 10m 45s\n",
      "bestTest = 0.1566834097\n",
      "bestIteration = 3044\n",
      "Shrink model to first 3045 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_kaplan\n",
      "0:\tlearn: 0.1764928\ttest: 0.1741790\tbest: 0.1741790 (0)\ttotal: 8.45ms\tremaining: 2h 20m 48s\n",
      "500:\tlearn: 0.1515514\ttest: 0.1563570\tbest: 0.1563570 (500)\ttotal: 4.9s\tremaining: 2h 43m 3s\n",
      "1000:\tlearn: 0.1463578\ttest: 0.1552994\tbest: 0.1552852 (996)\ttotal: 10.7s\tremaining: 2h 57m 49s\n",
      "1500:\tlearn: 0.1424088\ttest: 0.1548863\tbest: 0.1548863 (1500)\ttotal: 16.3s\tremaining: 3h 52s\n",
      "2000:\tlearn: 0.1389584\ttest: 0.1546072\tbest: 0.1546036 (1996)\ttotal: 22.1s\tremaining: 3h 3m 53s\n",
      "2500:\tlearn: 0.1358795\ttest: 0.1545715\tbest: 0.1545576 (2489)\ttotal: 28s\tremaining: 3h 6m 10s\n",
      "3000:\tlearn: 0.1330614\ttest: 0.1545323\tbest: 0.1545216 (2974)\ttotal: 34.2s\tremaining: 3h 9m 14s\n",
      "3500:\tlearn: 0.1303967\ttest: 0.1544196\tbest: 0.1544077 (3372)\ttotal: 40.2s\tremaining: 3h 10m 44s\n",
      "4000:\tlearn: 0.1279823\ttest: 0.1544723\tbest: 0.1543860 (3567)\ttotal: 45.9s\tremaining: 3h 10m 38s\n",
      "bestTest = 0.1543860327\n",
      "bestIteration = 3567\n",
      "Shrink model to first 3568 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_kaplan\n",
      "0:\tlearn: 0.1762187\ttest: 0.1767416\tbest: 0.1767416 (0)\ttotal: 10.4ms\tremaining: 2h 53m 56s\n",
      "500:\tlearn: 0.1510612\ttest: 0.1592127\tbest: 0.1592127 (500)\ttotal: 5.09s\tremaining: 2h 49m 18s\n",
      "1000:\tlearn: 0.1456284\ttest: 0.1580020\tbest: 0.1580019 (999)\ttotal: 11.1s\tremaining: 3h 4m 54s\n",
      "1500:\tlearn: 0.1415228\ttest: 0.1575235\tbest: 0.1575182 (1495)\ttotal: 17.2s\tremaining: 3h 11m 1s\n",
      "2000:\tlearn: 0.1380420\ttest: 0.1571488\tbest: 0.1571372 (1990)\ttotal: 23.2s\tremaining: 3h 12m 35s\n",
      "2500:\tlearn: 0.1349577\ttest: 0.1570083\tbest: 0.1570048 (2499)\ttotal: 28.8s\tremaining: 3h 11m 15s\n",
      "3000:\tlearn: 0.1320214\ttest: 0.1570074\tbest: 0.1569894 (2516)\ttotal: 34.9s\tremaining: 3h 13m 10s\n",
      "bestTest = 0.1569893785\n",
      "bestIteration = 2516\n",
      "Shrink model to first 2517 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_kaplan\n",
      "0:\tlearn: 0.1759158\ttest: 0.1793696\tbest: 0.1793696 (0)\ttotal: 8.13ms\tremaining: 2h 15m 33s\n",
      "500:\tlearn: 0.1513074\ttest: 0.1585006\tbest: 0.1585006 (500)\ttotal: 5.38s\tremaining: 2h 58m 56s\n",
      "1000:\tlearn: 0.1460723\ttest: 0.1569840\tbest: 0.1569840 (1000)\ttotal: 11.8s\tremaining: 3h 15m 30s\n",
      "1500:\tlearn: 0.1421496\ttest: 0.1563391\tbest: 0.1563391 (1500)\ttotal: 17.4s\tremaining: 3h 13m 28s\n",
      "2000:\tlearn: 0.1387356\ttest: 0.1558900\tbest: 0.1558900 (2000)\ttotal: 23.7s\tremaining: 3h 16m 38s\n",
      "2500:\tlearn: 0.1356358\ttest: 0.1555702\tbest: 0.1555697 (2499)\ttotal: 30s\tremaining: 3h 19m 5s\n",
      "3000:\tlearn: 0.1328105\ttest: 0.1554468\tbest: 0.1554360 (2992)\ttotal: 36.1s\tremaining: 3h 19m 58s\n",
      "3500:\tlearn: 0.1301425\ttest: 0.1553905\tbest: 0.1553793 (3188)\ttotal: 42.4s\tremaining: 3h 21m 5s\n",
      "4000:\tlearn: 0.1276817\ttest: 0.1553878\tbest: 0.1553647 (3717)\ttotal: 48.6s\tremaining: 3h 21m 28s\n",
      "bestTest = 0.1553646862\n",
      "bestIteration = 3717\n",
      "Shrink model to first 3718 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_kaplan\n",
      "0:\tlearn: 0.1760691\ttest: 0.1781318\tbest: 0.1781318 (0)\ttotal: 8.36ms\tremaining: 2h 19m 21s\n",
      "500:\tlearn: 0.1511718\ttest: 0.1590287\tbest: 0.1590276 (499)\ttotal: 5.89s\tremaining: 3h 15m 56s\n",
      "1000:\tlearn: 0.1459592\ttest: 0.1577902\tbest: 0.1577902 (1000)\ttotal: 12s\tremaining: 3h 19m 30s\n",
      "1500:\tlearn: 0.1419584\ttest: 0.1573274\tbest: 0.1573226 (1497)\ttotal: 18.1s\tremaining: 3h 20m 37s\n",
      "2000:\tlearn: 0.1386418\ttest: 0.1570334\tbest: 0.1570292 (1985)\ttotal: 24.6s\tremaining: 3h 24m 18s\n",
      "2500:\tlearn: 0.1355170\ttest: 0.1569398\tbest: 0.1569093 (2334)\ttotal: 30.5s\tremaining: 3h 22m 48s\n",
      "3000:\tlearn: 0.1326920\ttest: 0.1567916\tbest: 0.1567824 (2992)\ttotal: 36.7s\tremaining: 3h 22m 59s\n",
      "3500:\tlearn: 0.1301109\ttest: 0.1568323\tbest: 0.1567655 (3140)\ttotal: 42.5s\tremaining: 3h 21m 45s\n",
      "bestTest = 0.1567654817\n",
      "bestIteration = 3140\n",
      "Shrink model to first 3141 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_kaplan\n",
      "0:\tlearn: 0.1761428\ttest: 0.1774015\tbest: 0.1774015 (0)\ttotal: 8.6ms\tremaining: 2h 23m 15s\n",
      "500:\tlearn: 0.1515392\ttest: 0.1586494\tbest: 0.1586448 (499)\ttotal: 5.07s\tremaining: 2h 48m 40s\n",
      "1000:\tlearn: 0.1464440\ttest: 0.1570582\tbest: 0.1570582 (1000)\ttotal: 11s\tremaining: 3h 3m 22s\n",
      "1500:\tlearn: 0.1426020\ttest: 0.1564577\tbest: 0.1564556 (1499)\ttotal: 16.7s\tremaining: 3h 4m 46s\n",
      "2000:\tlearn: 0.1392022\ttest: 0.1560857\tbest: 0.1560601 (1976)\ttotal: 22.4s\tremaining: 3h 6m 12s\n",
      "2500:\tlearn: 0.1362162\ttest: 0.1557416\tbest: 0.1557416 (2500)\ttotal: 28.3s\tremaining: 3h 8m 4s\n",
      "3000:\tlearn: 0.1333436\ttest: 0.1556452\tbest: 0.1556337 (2957)\ttotal: 34.2s\tremaining: 3h 9m 29s\n",
      "3500:\tlearn: 0.1306369\ttest: 0.1555654\tbest: 0.1555342 (3459)\ttotal: 39.9s\tremaining: 3h 9m 30s\n",
      "4000:\tlearn: 0.1282110\ttest: 0.1554241\tbest: 0.1554189 (3994)\ttotal: 46s\tremaining: 3h 10m 45s\n",
      "4500:\tlearn: 0.1258789\ttest: 0.1553770\tbest: 0.1553550 (4341)\ttotal: 52s\tremaining: 3h 11m 31s\n",
      "5000:\tlearn: 0.1236335\ttest: 0.1553225\tbest: 0.1552990 (4884)\ttotal: 58.3s\tremaining: 3h 13m 13s\n",
      "bestTest = 0.1552990267\n",
      "bestIteration = 4884\n",
      "Shrink model to first 4885 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6742267017669824\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_nelson\n",
      "0:\tlearn: 0.2713487\ttest: 0.2714857\tbest: 0.2714857 (0)\ttotal: 8.65ms\tremaining: 2h 24m 13s\n",
      "500:\tlearn: 0.2340184\ttest: 0.2435293\tbest: 0.2435293 (500)\ttotal: 5.72s\tremaining: 3h 10m 4s\n",
      "1000:\tlearn: 0.2262576\ttest: 0.2415334\tbest: 0.2415211 (995)\ttotal: 12.2s\tremaining: 3h 23m 31s\n",
      "1500:\tlearn: 0.2203628\ttest: 0.2406962\tbest: 0.2406962 (1500)\ttotal: 18.6s\tremaining: 3h 25m 57s\n",
      "2000:\tlearn: 0.2152943\ttest: 0.2403226\tbest: 0.2403102 (1992)\ttotal: 24.2s\tremaining: 3h 21m 4s\n",
      "2500:\tlearn: 0.2105107\ttest: 0.2400574\tbest: 0.2400290 (2458)\ttotal: 29.6s\tremaining: 3h 16m 27s\n",
      "3000:\tlearn: 0.2063320\ttest: 0.2396558\tbest: 0.2396558 (3000)\ttotal: 35.4s\tremaining: 3h 16m 12s\n",
      "3500:\tlearn: 0.2023550\ttest: 0.2396083\tbest: 0.2395756 (3449)\ttotal: 41.7s\tremaining: 3h 17m 43s\n",
      "4000:\tlearn: 0.1986120\ttest: 0.2395040\tbest: 0.2394990 (3991)\ttotal: 47.8s\tremaining: 3h 18m 31s\n",
      "4500:\tlearn: 0.1951420\ttest: 0.2396667\tbest: 0.2394887 (4024)\ttotal: 53.6s\tremaining: 3h 17m 37s\n",
      "bestTest = 0.2394887323\n",
      "bestIteration = 4024\n",
      "Shrink model to first 4025 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_nelson\n",
      "0:\tlearn: 0.2717946\ttest: 0.2673893\tbest: 0.2673893 (0)\ttotal: 11.1ms\tremaining: 3h 5m 30s\n",
      "500:\tlearn: 0.2341099\ttest: 0.2412288\tbest: 0.2412288 (500)\ttotal: 5.27s\tremaining: 2h 55m 10s\n",
      "1000:\tlearn: 0.2265195\ttest: 0.2397681\tbest: 0.2397636 (998)\ttotal: 10.6s\tremaining: 2h 55m 55s\n",
      "1500:\tlearn: 0.2209658\ttest: 0.2392936\tbest: 0.2392855 (1491)\ttotal: 16.2s\tremaining: 2h 59m 57s\n",
      "2000:\tlearn: 0.2161172\ttest: 0.2390119\tbest: 0.2390039 (1995)\ttotal: 22.1s\tremaining: 3h 4m 6s\n",
      "2500:\tlearn: 0.2117796\ttest: 0.2387886\tbest: 0.2387828 (2496)\ttotal: 28.2s\tremaining: 3h 7m 13s\n",
      "3000:\tlearn: 0.2076938\ttest: 0.2387879\tbest: 0.2387345 (2678)\ttotal: 34.1s\tremaining: 3h 8m 41s\n",
      "bestTest = 0.2387345358\n",
      "bestIteration = 2678\n",
      "Shrink model to first 2679 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_nelson\n",
      "0:\tlearn: 0.2715766\ttest: 0.2694078\tbest: 0.2694078 (0)\ttotal: 8.69ms\tremaining: 2h 24m 48s\n",
      "500:\tlearn: 0.2343528\ttest: 0.2403240\tbest: 0.2403240 (500)\ttotal: 5.24s\tremaining: 2h 54m 6s\n",
      "1000:\tlearn: 0.2265892\ttest: 0.2383655\tbest: 0.2383655 (1000)\ttotal: 11s\tremaining: 3h 2m 53s\n",
      "1500:\tlearn: 0.2207348\ttest: 0.2374491\tbest: 0.2374441 (1498)\ttotal: 16.8s\tremaining: 3h 6m 18s\n",
      "2000:\tlearn: 0.2157783\ttest: 0.2370784\tbest: 0.2370781 (1997)\ttotal: 22.4s\tremaining: 3h 6m 14s\n",
      "2500:\tlearn: 0.2114218\ttest: 0.2368390\tbest: 0.2368312 (2468)\ttotal: 27.9s\tremaining: 3h 5m 24s\n",
      "3000:\tlearn: 0.2073442\ttest: 0.2366114\tbest: 0.2366071 (2988)\ttotal: 33.4s\tremaining: 3h 5m 3s\n",
      "3500:\tlearn: 0.2035845\ttest: 0.2364009\tbest: 0.2364009 (3500)\ttotal: 39.5s\tremaining: 3h 7m 29s\n",
      "4000:\tlearn: 0.1999597\ttest: 0.2365446\tbest: 0.2363924 (3504)\ttotal: 45.7s\tremaining: 3h 9m 37s\n",
      "bestTest = 0.2363924358\n",
      "bestIteration = 3504\n",
      "Shrink model to first 3505 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_nelson\n",
      "0:\tlearn: 0.2711050\ttest: 0.2736921\tbest: 0.2736921 (0)\ttotal: 9.01ms\tremaining: 2h 30m 10s\n",
      "500:\tlearn: 0.2335276\ttest: 0.2448228\tbest: 0.2448228 (500)\ttotal: 5.81s\tremaining: 3h 13m 16s\n",
      "1000:\tlearn: 0.2258755\ttest: 0.2427379\tbest: 0.2427377 (999)\ttotal: 12.1s\tremaining: 3h 20m 31s\n",
      "1500:\tlearn: 0.2199901\ttest: 0.2418589\tbest: 0.2418493 (1489)\ttotal: 18.2s\tremaining: 3h 21m 31s\n",
      "2000:\tlearn: 0.2147362\ttest: 0.2414593\tbest: 0.2414542 (1999)\ttotal: 24.3s\tremaining: 3h 21m 56s\n",
      "2500:\tlearn: 0.2100641\ttest: 0.2411846\tbest: 0.2411846 (2500)\ttotal: 30.1s\tremaining: 3h 20m 16s\n",
      "3000:\tlearn: 0.2057298\ttest: 0.2409473\tbest: 0.2409358 (2966)\ttotal: 36s\tremaining: 3h 19m 28s\n",
      "3500:\tlearn: 0.2017277\ttest: 0.2409437\tbest: 0.2409328 (3164)\ttotal: 42s\tremaining: 3h 19m 27s\n",
      "4000:\tlearn: 0.1979023\ttest: 0.2408335\tbest: 0.2408335 (4000)\ttotal: 47.9s\tremaining: 3h 18m 33s\n",
      "4500:\tlearn: 0.1942267\ttest: 0.2408041\tbest: 0.2407281 (4130)\ttotal: 54.1s\tremaining: 3h 19m 18s\n",
      "bestTest = 0.2407281256\n",
      "bestIteration = 4130\n",
      "Shrink model to first 4131 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_nelson\n",
      "0:\tlearn: 0.2716334\ttest: 0.2686787\tbest: 0.2686787 (0)\ttotal: 10.5ms\tremaining: 2h 54m 24s\n",
      "500:\tlearn: 0.2336582\ttest: 0.2434816\tbest: 0.2434816 (500)\ttotal: 5.61s\tremaining: 3h 6m 36s\n",
      "1000:\tlearn: 0.2256962\ttest: 0.2424027\tbest: 0.2424012 (999)\ttotal: 11.8s\tremaining: 3h 15m 52s\n",
      "1500:\tlearn: 0.2197777\ttest: 0.2420750\tbest: 0.2420750 (1500)\ttotal: 17.9s\tremaining: 3h 18m 3s\n",
      "2000:\tlearn: 0.2146000\ttest: 0.2419776\tbest: 0.2419730 (1999)\ttotal: 24.4s\tremaining: 3h 22m 45s\n",
      "2500:\tlearn: 0.2099221\ttest: 0.2418824\tbest: 0.2418597 (2316)\ttotal: 31s\tremaining: 3h 26m 8s\n",
      "bestTest = 0.2418597448\n",
      "bestIteration = 2316\n",
      "Shrink model to first 2317 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_nelson\n",
      "0:\tlearn: 0.2716045\ttest: 0.2689873\tbest: 0.2689873 (0)\ttotal: 12.5ms\tremaining: 3h 27m 38s\n",
      "500:\tlearn: 0.2341960\ttest: 0.2418000\tbest: 0.2418000 (500)\ttotal: 5.9s\tremaining: 3h 16m 7s\n",
      "1000:\tlearn: 0.2265112\ttest: 0.2402892\tbest: 0.2402737 (989)\ttotal: 12.2s\tremaining: 3h 22m 26s\n",
      "1500:\tlearn: 0.2205555\ttest: 0.2395634\tbest: 0.2395634 (1500)\ttotal: 18.2s\tremaining: 3h 22m 16s\n",
      "2000:\tlearn: 0.2156033\ttest: 0.2394243\tbest: 0.2394208 (1999)\ttotal: 24.5s\tremaining: 3h 23m 21s\n",
      "2500:\tlearn: 0.2110413\ttest: 0.2392453\tbest: 0.2392350 (2477)\ttotal: 30.9s\tremaining: 3h 25m 19s\n",
      "3000:\tlearn: 0.2068051\ttest: 0.2391325\tbest: 0.2391109 (2984)\ttotal: 36.9s\tremaining: 3h 24m 27s\n",
      "3500:\tlearn: 0.2028032\ttest: 0.2390647\tbest: 0.2390434 (3273)\ttotal: 42.8s\tremaining: 3h 23m 2s\n",
      "4000:\tlearn: 0.1990468\ttest: 0.2390937\tbest: 0.2390304 (3524)\ttotal: 48.8s\tremaining: 3h 22m 17s\n",
      "bestTest = 0.2390304489\n",
      "bestIteration = 3524\n",
      "Shrink model to first 3525 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_nelson\n",
      "0:\tlearn: 0.2712665\ttest: 0.2720674\tbest: 0.2720674 (0)\ttotal: 9.01ms\tremaining: 2h 30m 13s\n",
      "500:\tlearn: 0.2334289\ttest: 0.2457773\tbest: 0.2457773 (500)\ttotal: 5.2s\tremaining: 2h 52m 49s\n",
      "1000:\tlearn: 0.2252933\ttest: 0.2441755\tbest: 0.2441505 (970)\ttotal: 10.8s\tremaining: 3h 23s\n",
      "1500:\tlearn: 0.2191860\ttest: 0.2434122\tbest: 0.2434120 (1499)\ttotal: 16.7s\tremaining: 3h 5m 12s\n",
      "2000:\tlearn: 0.2139700\ttest: 0.2428831\tbest: 0.2428645 (1983)\ttotal: 22.5s\tremaining: 3h 7m 17s\n",
      "2500:\tlearn: 0.2093723\ttest: 0.2427365\tbest: 0.2427230 (2493)\ttotal: 28.3s\tremaining: 3h 8m 7s\n",
      "3000:\tlearn: 0.2049540\ttest: 0.2425510\tbest: 0.2425275 (2926)\ttotal: 34.1s\tremaining: 3h 8m 50s\n",
      "3500:\tlearn: 0.2008205\ttest: 0.2426583\tbest: 0.2424973 (3162)\ttotal: 40.1s\tremaining: 3h 10m 1s\n",
      "bestTest = 0.2424972686\n",
      "bestIteration = 3162\n",
      "Shrink model to first 3163 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_nelson\n",
      "0:\tlearn: 0.2709002\ttest: 0.2752810\tbest: 0.2752810 (0)\ttotal: 7.85ms\tremaining: 2h 10m 49s\n",
      "500:\tlearn: 0.2339572\ttest: 0.2441433\tbest: 0.2441433 (500)\ttotal: 5.02s\tremaining: 2h 46m 50s\n",
      "1000:\tlearn: 0.2261909\ttest: 0.2420089\tbest: 0.2420089 (1000)\ttotal: 10.4s\tremaining: 2h 52m 46s\n",
      "1500:\tlearn: 0.2203641\ttest: 0.2413763\tbest: 0.2413751 (1497)\ttotal: 15.6s\tremaining: 2h 52m 25s\n",
      "2000:\tlearn: 0.2152358\ttest: 0.2407760\tbest: 0.2407702 (1996)\ttotal: 21.3s\tremaining: 2h 56m 48s\n",
      "2500:\tlearn: 0.2105752\ttest: 0.2404788\tbest: 0.2404751 (2499)\ttotal: 26.9s\tremaining: 2h 59m 3s\n",
      "3000:\tlearn: 0.2063498\ttest: 0.2403724\tbest: 0.2403158 (2715)\ttotal: 32.7s\tremaining: 3h 1m 17s\n",
      "3500:\tlearn: 0.2023364\ttest: 0.2402650\tbest: 0.2402332 (3383)\ttotal: 38.4s\tremaining: 3h 2m 3s\n",
      "bestTest = 0.2402332061\n",
      "bestIteration = 3383\n",
      "Shrink model to first 3384 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_nelson\n",
      "0:\tlearn: 0.2710991\ttest: 0.2737969\tbest: 0.2737969 (0)\ttotal: 8.76ms\tremaining: 2h 25m 58s\n",
      "500:\tlearn: 0.2338514\ttest: 0.2447743\tbest: 0.2447743 (500)\ttotal: 5.49s\tremaining: 3h 2m 34s\n",
      "1000:\tlearn: 0.2261193\ttest: 0.2430502\tbest: 0.2430493 (999)\ttotal: 11.5s\tremaining: 3h 11m 30s\n",
      "1500:\tlearn: 0.2201591\ttest: 0.2422520\tbest: 0.2422508 (1492)\ttotal: 17.6s\tremaining: 3h 15m 15s\n",
      "2000:\tlearn: 0.2151664\ttest: 0.2419803\tbest: 0.2419408 (1960)\ttotal: 23.5s\tremaining: 3h 15m 22s\n",
      "2500:\tlearn: 0.2104766\ttest: 0.2418433\tbest: 0.2417468 (2342)\ttotal: 29.1s\tremaining: 3h 13m 37s\n",
      "3000:\tlearn: 0.2061378\ttest: 0.2415860\tbest: 0.2415729 (2984)\ttotal: 34.9s\tremaining: 3h 13m 27s\n",
      "bestTest = 0.2415729341\n",
      "bestIteration = 2984\n",
      "Shrink model to first 2985 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_nelson\n",
      "0:\tlearn: 0.2711859\ttest: 0.2729184\tbest: 0.2729184 (0)\ttotal: 8.15ms\tremaining: 2h 15m 46s\n",
      "500:\tlearn: 0.2342701\ttest: 0.2444689\tbest: 0.2444603 (499)\ttotal: 5.33s\tremaining: 2h 57m 21s\n",
      "1000:\tlearn: 0.2265723\ttest: 0.2422216\tbest: 0.2422197 (999)\ttotal: 11.3s\tremaining: 3h 8m 32s\n",
      "1500:\tlearn: 0.2208410\ttest: 0.2411252\tbest: 0.2411197 (1498)\ttotal: 17.4s\tremaining: 3h 12m 56s\n",
      "2000:\tlearn: 0.2158327\ttest: 0.2405692\tbest: 0.2405416 (1962)\ttotal: 23.8s\tremaining: 3h 17m 40s\n",
      "2500:\tlearn: 0.2113936\ttest: 0.2401005\tbest: 0.2400992 (2499)\ttotal: 30s\tremaining: 3h 19m 31s\n",
      "3000:\tlearn: 0.2071757\ttest: 0.2398297\tbest: 0.2398297 (3000)\ttotal: 36.1s\tremaining: 3h 19m 53s\n",
      "3500:\tlearn: 0.2032842\ttest: 0.2398168\tbest: 0.2397698 (3445)\ttotal: 42.1s\tremaining: 3h 19m 55s\n",
      "4000:\tlearn: 0.1996447\ttest: 0.2397460\tbest: 0.2397295 (3990)\ttotal: 48.3s\tremaining: 3h 20m 19s\n",
      "4500:\tlearn: 0.1961573\ttest: 0.2396862\tbest: 0.2396293 (4332)\ttotal: 54.5s\tremaining: 3h 20m 52s\n",
      "bestTest = 0.2396292601\n",
      "bestIteration = 4332\n",
      "Shrink model to first 4333 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6774129626365785\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 1 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61913\n",
      "[500]\tvalidation_0-cox-nloglik:7.43048\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41847\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41487\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41374\n",
      "[2500]\tvalidation_0-cox-nloglik:7.41319\n",
      "[2689]\tvalidation_0-cox-nloglik:7.41322\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 2 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61516\n",
      "[500]\tvalidation_0-cox-nloglik:7.42103\n",
      "[1000]\tvalidation_0-cox-nloglik:7.40891\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40448\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40481\n",
      "[2083]\tvalidation_0-cox-nloglik:7.40498\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 3 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61993\n",
      "[500]\tvalidation_0-cox-nloglik:7.40996\n",
      "[1000]\tvalidation_0-cox-nloglik:7.39541\n",
      "[1500]\tvalidation_0-cox-nloglik:7.39046\n",
      "[2000]\tvalidation_0-cox-nloglik:7.38825\n",
      "[2500]\tvalidation_0-cox-nloglik:7.38728\n",
      "[2961]\tvalidation_0-cox-nloglik:7.38794\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 4 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.62001\n",
      "[500]\tvalidation_0-cox-nloglik:7.42765\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41295\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40712\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40434\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40317\n",
      "[3000]\tvalidation_0-cox-nloglik:7.40299\n",
      "[3346]\tvalidation_0-cox-nloglik:7.40270\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 5 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.62019\n",
      "[500]\tvalidation_0-cox-nloglik:7.43255\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41955\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41571\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41528\n",
      "[2135]\tvalidation_0-cox-nloglik:7.41503\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 6 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61935\n",
      "[500]\tvalidation_0-cox-nloglik:7.43071\n",
      "[1000]\tvalidation_0-cox-nloglik:7.42255\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41853\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41778\n",
      "[2429]\tvalidation_0-cox-nloglik:7.41851\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 7 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61899\n",
      "[500]\tvalidation_0-cox-nloglik:7.44408\n",
      "[1000]\tvalidation_0-cox-nloglik:7.43173\n",
      "[1500]\tvalidation_0-cox-nloglik:7.42757\n",
      "[2000]\tvalidation_0-cox-nloglik:7.42744\n",
      "[2098]\tvalidation_0-cox-nloglik:7.42741\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 8 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61997\n",
      "[500]\tvalidation_0-cox-nloglik:7.39626\n",
      "[1000]\tvalidation_0-cox-nloglik:7.37956\n",
      "[1500]\tvalidation_0-cox-nloglik:7.37325\n",
      "[2000]\tvalidation_0-cox-nloglik:7.36957\n",
      "[2500]\tvalidation_0-cox-nloglik:7.36876\n",
      "[3000]\tvalidation_0-cox-nloglik:7.36830\n",
      "[3500]\tvalidation_0-cox-nloglik:7.36853\n",
      "[3784]\tvalidation_0-cox-nloglik:7.36909\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 9 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61958\n",
      "[500]\tvalidation_0-cox-nloglik:7.42300\n",
      "[1000]\tvalidation_0-cox-nloglik:7.40921\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40498\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40331\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40270\n",
      "[2865]\tvalidation_0-cox-nloglik:7.40382\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 10 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61915\n",
      "[500]\tvalidation_0-cox-nloglik:7.42752\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41066\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40279\n",
      "[2000]\tvalidation_0-cox-nloglik:7.39812\n",
      "[2500]\tvalidation_0-cox-nloglik:7.39609\n",
      "[3000]\tvalidation_0-cox-nloglik:7.39483\n",
      "[3466]\tvalidation_0-cox-nloglik:7.39514\n",
      "==================================================\n",
      "xgboost_cox our out of folds CV score is 0.6736017081375693\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 1 efs_time2\n",
      "0:\tlearn: -137189.3077450\ttest: -11837.7556858\tbest: -11837.7556858 (0)\ttotal: 22.5ms\tremaining: 6h 14m 34s\n",
      "500:\tlearn: -133896.7672740\ttest: -11540.1423508\tbest: -11540.1423508 (500)\ttotal: 8.21s\tremaining: 4h 33m 5s\n",
      "1000:\tlearn: -133191.8440304\ttest: -11517.6463710\tbest: -11517.5032427 (997)\ttotal: 16.1s\tremaining: 4h 27m 37s\n",
      "1500:\tlearn: -132741.8840634\ttest: -11512.5230886\tbest: -11512.3106746 (1471)\ttotal: 23.9s\tremaining: 4h 24m 30s\n",
      "2000:\tlearn: -132366.8459087\ttest: -11509.6629021\tbest: -11509.6507967 (1999)\ttotal: 31.6s\tremaining: 4h 22m 26s\n",
      "2500:\tlearn: -132057.6790453\ttest: -11510.5011325\tbest: -11509.3168143 (2154)\ttotal: 39.3s\tremaining: 4h 21m 12s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11509.31681\n",
      "bestIteration = 2154\n",
      "\n",
      "Shrink model to first 2155 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 2 efs_time2\n",
      "0:\tlearn: -137202.5995846\ttest: -11832.3506454\tbest: -11832.3506454 (0)\ttotal: 18.9ms\tremaining: 5h 14m 16s\n",
      "500:\tlearn: -133940.3587167\ttest: -11529.7069581\tbest: -11529.6968438 (499)\ttotal: 8.15s\tremaining: 4h 30m 58s\n",
      "1000:\tlearn: -133205.3190910\ttest: -11513.3032567\tbest: -11513.2352011 (984)\ttotal: 16s\tremaining: 4h 26m 39s\n",
      "1500:\tlearn: -132750.8584403\ttest: -11509.3071835\tbest: -11509.1168594 (1490)\ttotal: 23.9s\tremaining: 4h 24m 53s\n",
      "2000:\tlearn: -132412.4546087\ttest: -11509.3600520\tbest: -11508.5153803 (1744)\ttotal: 31.6s\tremaining: 4h 22m 59s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11508.51538\n",
      "bestIteration = 1744\n",
      "\n",
      "Shrink model to first 1745 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 3 efs_time2\n",
      "0:\tlearn: -137188.0147447\ttest: -11839.0302130\tbest: -11839.0302130 (0)\ttotal: 22.5ms\tremaining: 6h 15m 17s\n",
      "500:\tlearn: -133899.7483814\ttest: -11507.2798216\tbest: -11507.2798216 (500)\ttotal: 8.45s\tremaining: 4h 40m 59s\n",
      "1000:\tlearn: -133135.5614495\ttest: -11490.4942199\tbest: -11490.4768909 (998)\ttotal: 16.5s\tremaining: 4h 34m 28s\n",
      "1500:\tlearn: -132672.9874902\ttest: -11486.7770410\tbest: -11486.6467015 (1476)\ttotal: 24.5s\tremaining: 4h 31m 10s\n",
      "2000:\tlearn: -132326.4175503\ttest: -11486.9959317\tbest: -11485.6496902 (1722)\ttotal: 32.3s\tremaining: 4h 28m 19s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11485.64969\n",
      "bestIteration = 1722\n",
      "\n",
      "Shrink model to first 1723 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 4 efs_time2\n",
      "0:\tlearn: -137198.5745979\ttest: -11832.4474258\tbest: -11832.4474258 (0)\ttotal: 26.6ms\tremaining: 7h 23m 57s\n",
      "500:\tlearn: -133923.4990259\ttest: -11533.8620694\tbest: -11533.8620694 (500)\ttotal: 8.24s\tremaining: 4h 34m 3s\n",
      "1000:\tlearn: -133217.2632634\ttest: -11517.6340456\tbest: -11517.6340456 (1000)\ttotal: 16.3s\tremaining: 4h 30m 59s\n",
      "1500:\tlearn: -132743.8501029\ttest: -11514.0613720\tbest: -11514.0544405 (1424)\ttotal: 24.2s\tremaining: 4h 28m 31s\n",
      "2000:\tlearn: -132423.1067254\ttest: -11514.6928720\tbest: -11513.5953222 (1528)\ttotal: 31.9s\tremaining: 4h 24m 59s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11513.59532\n",
      "bestIteration = 1528\n",
      "\n",
      "Shrink model to first 1529 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 5 efs_time2\n",
      "0:\tlearn: -137203.5696104\ttest: -11833.0166421\tbest: -11833.0166421 (0)\ttotal: 18.5ms\tremaining: 5h 7m 52s\n",
      "500:\tlearn: -133910.1424455\ttest: -11545.1267568\tbest: -11545.1267568 (500)\ttotal: 8.22s\tremaining: 4h 33m 29s\n",
      "1000:\tlearn: -133128.9915965\ttest: -11535.9627049\tbest: -11535.9350293 (999)\ttotal: 16.2s\tremaining: 4h 30m 12s\n",
      "1500:\tlearn: -132664.8583587\ttest: -11533.8370944\tbest: -11533.7920994 (1496)\ttotal: 24.1s\tremaining: 4h 27m 37s\n",
      "2000:\tlearn: -132329.5297288\ttest: -11534.8844650\tbest: -11532.9128559 (1727)\ttotal: 31.9s\tremaining: 4h 25m 32s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11532.91286\n",
      "bestIteration = 1727\n",
      "\n",
      "Shrink model to first 1728 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 6 efs_time2\n",
      "0:\tlearn: -137215.7949355\ttest: -11823.6207905\tbest: -11823.6207905 (0)\ttotal: 22.9ms\tremaining: 6h 21m 8s\n",
      "500:\tlearn: -133911.9780328\ttest: -11528.5667436\tbest: -11528.5485366 (499)\ttotal: 8.19s\tremaining: 4h 32m 23s\n",
      "1000:\tlearn: -133187.1551511\ttest: -11520.8805309\tbest: -11520.5931721 (965)\ttotal: 16.2s\tremaining: 4h 29m 8s\n",
      "1500:\tlearn: -132795.6323969\ttest: -11520.8100593\tbest: -11520.3434723 (1166)\ttotal: 24.2s\tremaining: 4h 28m 31s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11520.34347\n",
      "bestIteration = 1166\n",
      "\n",
      "Shrink model to first 1167 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 7 efs_time2\n",
      "0:\tlearn: -137210.5079381\ttest: -11823.4044431\tbest: -11823.4044431 (0)\ttotal: 20.7ms\tremaining: 5h 45m 35s\n",
      "500:\tlearn: -133902.4897901\ttest: -11553.2026922\tbest: -11553.1776458 (499)\ttotal: 8.41s\tremaining: 4h 39m 48s\n",
      "1000:\tlearn: -133142.8724027\ttest: -11538.1355336\tbest: -11538.0609056 (999)\ttotal: 16.5s\tremaining: 4h 35m 16s\n",
      "1500:\tlearn: -132701.7939306\ttest: -11536.8775464\tbest: -11536.6421771 (1437)\ttotal: 24.5s\tremaining: 4h 31m 37s\n",
      "2000:\tlearn: -132346.6109545\ttest: -11535.4494672\tbest: -11535.4074718 (1995)\ttotal: 32.2s\tremaining: 4h 27m 49s\n",
      "2500:\tlearn: -132027.9590527\ttest: -11537.4307718\tbest: -11535.2367362 (2026)\ttotal: 39.8s\tremaining: 4h 24m 41s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11535.23674\n",
      "bestIteration = 2026\n",
      "\n",
      "Shrink model to first 2027 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 8 efs_time2\n",
      "0:\tlearn: -137206.5982321\ttest: -11823.7739184\tbest: -11823.7739184 (0)\ttotal: 18.9ms\tremaining: 5h 14m 48s\n",
      "500:\tlearn: -133984.1446242\ttest: -11480.0750298\tbest: -11480.0577321 (498)\ttotal: 8.4s\tremaining: 4h 39m 16s\n",
      "1000:\tlearn: -133216.4401803\ttest: -11462.1081496\tbest: -11462.0405028 (992)\ttotal: 16.5s\tremaining: 4h 34m 16s\n",
      "1500:\tlearn: -132805.0523778\ttest: -11459.0410072\tbest: -11458.8298935 (1447)\ttotal: 24.4s\tremaining: 4h 30m 41s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11458.82989\n",
      "bestIteration = 1447\n",
      "\n",
      "Shrink model to first 1448 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 9 efs_time2\n",
      "0:\tlearn: -137190.0338615\ttest: -11838.3203785\tbest: -11838.3203785 (0)\ttotal: 22ms\tremaining: 6h 6m 28s\n",
      "500:\tlearn: -133931.2039681\ttest: -11534.9498983\tbest: -11534.9498983 (500)\ttotal: 8.27s\tremaining: 4h 34m 49s\n",
      "1000:\tlearn: -133194.1818632\ttest: -11522.4211842\tbest: -11522.4211842 (1000)\ttotal: 16.3s\tremaining: 4h 31m 16s\n",
      "1500:\tlearn: -132763.2264564\ttest: -11520.7876286\tbest: -11520.3224800 (1480)\ttotal: 24.2s\tremaining: 4h 28m 32s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11520.32248\n",
      "bestIteration = 1480\n",
      "\n",
      "Shrink model to first 1481 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 10 efs_time2\n",
      "0:\tlearn: -137188.5978555\ttest: -11837.4850753\tbest: -11837.4850753 (0)\ttotal: 23.1ms\tremaining: 6h 25m 4s\n",
      "500:\tlearn: -133915.6389266\ttest: -11541.3203682\tbest: -11541.3203682 (500)\ttotal: 8.31s\tremaining: 4h 36m 22s\n",
      "1000:\tlearn: -133182.3812277\ttest: -11521.7482195\tbest: -11521.6902021 (998)\ttotal: 16.5s\tremaining: 4h 34m 43s\n",
      "1500:\tlearn: -132698.5544284\ttest: -11514.7232231\tbest: -11514.7032691 (1499)\ttotal: 24.6s\tremaining: 4h 32m 31s\n",
      "2000:\tlearn: -132339.4706809\ttest: -11512.7460420\tbest: -11512.1765160 (1905)\ttotal: 32.4s\tremaining: 4h 29m 27s\n",
      "2500:\tlearn: -132040.2051440\ttest: -11513.0673654\tbest: -11511.9372478 (2276)\ttotal: 40.2s\tremaining: 4h 27m 7s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11511.93725\n",
      "bestIteration = 2276\n",
      "\n",
      "Shrink model to first 2277 iterations.\n",
      "==================================================\n",
      "catboost_cox our out of folds CV score is 0.6717630574738233\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Training\n",
    "# ====================================================\n",
    "# for method in CFG.METHOD_LIST:\n",
    "#     gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n",
    "\n",
    "# kaplan-meier & nelson-aalen models\n",
    "for method in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n",
    "    gradient_boosting_model_cv_training(method, train.to_pandas(), CFG.target_col_list, FEATURES, CATS)\n",
    "# Cox models\n",
    "for method in [\"xgboost_cox\", \"catboost_cox\"]:\n",
    "    gradient_boosting_model_cv_training(method, train.to_pandas(), CFG.cox_target_col_list, FEATURES, CATS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for Ensemble = 0.6811375200448716\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Overall CV\n",
    "# ====================================================\n",
    "# kaplan-meier models\n",
    "oof_lgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# nelson-aalen models\n",
    "oof_lgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "# Cox models\n",
    "oof_cox_xgb = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cox_cat = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].clone()\n",
    "y_pred = train[[\"ID\"]].clone()\n",
    "ensamble_prediction = (\n",
    "    rankdata(oof_xgb_kaplan)\n",
    "    + rankdata(oof_cat_kaplan)\n",
    "    + rankdata(oof_lgb_kaplan)\n",
    "    + rankdata(oof_lgb_nelson)\n",
    "    + rankdata(oof_xgb_nelson)\n",
    "    + rankdata(oof_cat_nelson)\n",
    "    + rankdata(oof_cox_xgb)\n",
    "    + rankdata(oof_cox_cat)\n",
    ")\n",
    "y_pred = y_pred.with_columns(pl.Series(ensamble_prediction).alias(\"prediction\"))\n",
    "m = score(y_true.to_pandas().copy(), y_pred.to_pandas().copy(), \"ID\")\n",
    "print(\"\\nOverall CV for Ensemble =\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な重み: [ 0.18533906 -0.0458442   0.09341751  0.08316677  0.04417384  0.22222723\n",
      "  0.34281995  0.10879585]\n",
      "最適化後のスコア: -0.6822967088847154\n"
     ]
    }
   ],
   "source": [
    "def ensemble_score(weights):\n",
    "    # 重み付けした予測値を計算\n",
    "    weighted_pred = (\n",
    "        weights[0] * rankdata(oof_lgb_kaplan)\n",
    "        + weights[1] * rankdata(oof_xgb_kaplan)\n",
    "        + weights[2] * rankdata(oof_cat_kaplan)\n",
    "        + weights[3] * rankdata(oof_lgb_nelson)\n",
    "        + weights[4] * rankdata(oof_xgb_nelson)\n",
    "        + weights[5] * rankdata(oof_cat_nelson)\n",
    "        + weights[6] * rankdata(oof_cox_xgb)\n",
    "        + weights[7] * rankdata(oof_cox_cat)\n",
    "    )\n",
    "\n",
    "    y_pred = pd.DataFrame({\"ID\": train[\"ID\"], \"prediction\": weighted_pred})\n",
    "    y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].clone().to_pandas()\n",
    "\n",
    "    return -score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "\n",
    "\n",
    "# 8つのモデルの初期重みを均等に設定\n",
    "initial_weights = [1 / 8] * 8\n",
    "\n",
    "# 最適化実行\n",
    "result = minimize(ensemble_score, initial_weights, method=\"Nelder-Mead\")\n",
    "\n",
    "print(\"最適な重み:\", result.x)\n",
    "print(\"最適化後のスコア:\", result.fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Inference functions\n",
    "# ====================================================\n",
    "def lightgbm_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"lightgbm_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def xgboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        # pred = model.predict(xgb.DMatrix(x_test, enable_categorical=True))\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list, target_col: str):\n",
    "    x_test = test_df[features]\n",
    "    if method == \"lightgbm\":\n",
    "        test_pred = lightgbm_inference(x_test, target_col)\n",
    "    if method == \"xgboost\":\n",
    "        test_pred = xgboost_inference(x_test, target_col)\n",
    "    if method == \"catboost\":\n",
    "        test_pred = catboost_inference(x_test, target_col)\n",
    "    # Cox models\n",
    "    elif method == \"xgboost_cox\":\n",
    "        test_pred = xgboost_cox_inference(x_test, target_col)\n",
    "    elif method == \"catboost_cox\":\n",
    "        test_pred = catboost_cox_inference(x_test, target_col)\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "def predicting(method_list: list, input_df: pd.DataFrame, target_col_list: list, features: list):\n",
    "    output_df = input_df.copy()\n",
    "    for target_col in target_col_list:\n",
    "        # output_df[target_col] = 0\n",
    "        for method in method_list:\n",
    "            output_df[f\"{method}_pred_{target_col}\"] = gradient_boosting_model_inference(\n",
    "                method, input_df, features, target_col\n",
    "            )\n",
    "            # output_df[target_col] += CFG.model_weight_dict[method] * output_df[f\"{method}_pred_{target_col}\"]\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28801</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28802</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  prediction\n",
       "0  28800        10.0\n",
       "1  28801        15.0\n",
       "2  28802         5.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Inference\n",
    "# ====================================================\n",
    "output_df = predicting([\"lightgbm\", \"xgboost\", \"catboost\"], test.to_pandas(), CFG.target_col_list, FEATURES)\n",
    "pred_lgb = output_df[\"lightgbm_pred_y_kaplan\"]\n",
    "pred_xgb = output_df[\"xgboost_pred_y_kaplan\"]\n",
    "pred_cat = output_df[\"catboost_pred_y_kaplan\"]\n",
    "# Cox models\n",
    "cox_output_df = predicting([\"xgboost_cox\", \"catboost_cox\"], test.to_pandas(), CFG.cox_target_col_list, FEATURES)\n",
    "pred_cox_xgb = cox_output_df[\"xgboost_cox_pred_efs_time2\"]\n",
    "pred_cox_cat = cox_output_df[\"catboost_cox_pred_efs_time2\"]\n",
    "\n",
    "submission = pd.read_csv(CFG.DATA_PATH / \"sample_submission.csv\")\n",
    "submission[\"prediction\"] = (\n",
    "    rankdata(pred_lgb) + rankdata(pred_xgb) + rankdata(pred_cat) + rankdata(pred_cox_xgb) + rankdata(pred_cox_cat)\n",
    ")\n",
    "submission.to_csv(CFG.OUTPUT_DIR / \"submission.csv\", index=False)\n",
    "print(\"Sub shape:\", submission.shape)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
