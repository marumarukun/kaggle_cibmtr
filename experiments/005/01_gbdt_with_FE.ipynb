{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import config  # edit config.py as needed\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter, NelsonAalenFitter\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from metric import score  # edit metric.py as needed\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import rankdata\n",
    "from seed import seed_everything  # edit seed.py as needed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    DRY_RUN = False\n",
    "    EXP_NAME = config.EXP_NAME\n",
    "    AUTHOR = \"marumarukun\"\n",
    "    COMPETITION = config.KAGGLE_COMPETITION_NAME\n",
    "    DATA_PATH = config.COMP_DATASET_DIR\n",
    "    OUTPUT_DIR = config.OUTPUT_DIR\n",
    "    MODEL_PATH = config.OUTPUT_DIR / \"models\"  # モデル作成・実験時はこちらを使用\n",
    "    # MODEL_PATH = config.ARTIFACT_EXP_DIR(config.EXP_NAME) / \"models\"  # 提出時はこちらを使用\n",
    "    METHOD_LIST = [\"xgboost_cox\", \"catboost_cox\", \"lightgbm\", \"xgboost\", \"catboost\"]\n",
    "    SEED = 42\n",
    "    n_folds = 2 if DRY_RUN else 10\n",
    "    target_col_list = [\"y_kaplan\", \"y_nelson\"]\n",
    "    cox_target_col_list = [\"efs_time2\"]\n",
    "    # group_col = \"race_group\"  # Required for GroupKFold (edit as needed)\n",
    "    stratified_col = \"race_group_efs\"  # Required for StratifiedKFold (edit as needed)\n",
    "    num_boost_round = 100 if DRY_RUN else 1000000\n",
    "    early_stopping_round = 10 if DRY_RUN else 500  # 10÷lrで設定\n",
    "    verbose = 500\n",
    "\n",
    "    # https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
    "    regression_lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        # \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"seed\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
    "    regression_xgb_params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # \"eval_metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    regression_xgb_cox_params = {\n",
    "        \"objective\": \"survival:cox\",\n",
    "        \"eval_metric\": \"cox-nloglik\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 3,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 80,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://catboost.ai/docs/en/references/training-parameters/\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostregressor\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostclassifier\n",
    "    regression_cat_params = {\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"GPU\",  # CPU/GPU\n",
    "    }\n",
    "    regression_cat_cox_params = {\n",
    "        \"loss_function\": \"Cox\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"CPU\",  # CPU/GPU\n",
    "    }\n",
    "\n",
    "    model_weight_dict = {\"lightgbm\": 0.40, \"xgboost\": 0.30, \"catboost\": 0.30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "seed_everything(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "train = pl.read_csv(CFG.DATA_PATH / \"train.csv\", try_parse_dates=True)\n",
    "test = pl.read_csv(CFG.DATA_PATH / \"test.csv\", try_parse_dates=True)\n",
    "# make index column\n",
    "# train = train.with_row_index()\n",
    "# test = test.with_row_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Preprocess(ここに前処理や特徴量エンジニアリングを記述)\n",
    "# ====================================================\n",
    "def transform_survival_probability(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "\n",
    "\n",
    "def transform_cumulative_hazard(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    naf = NelsonAalenFitter()\n",
    "    naf.fit(durations=df[time_col], event_observed=df[event_col])\n",
    "    y = naf.cumulative_hazard_at_times(df[time_col]).to_numpy()\n",
    "    return -y\n",
    "\n",
    "\n",
    "def preprocess(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    output = df.clone()\n",
    "    # 欠損値のカウント（最初に行う）\n",
    "    output = output.with_columns(pl.sum_horizontal(pl.all().is_null()).alias(\"null_count\"))\n",
    "\n",
    "    # ドナーと患者の性別マッチング\n",
    "    output = output.with_columns(\n",
    "        pl.when(pl.col(\"sex_match\").str.contains_any([\"M-M\", \"F-F\"]))\n",
    "        .then(1)\n",
    "        .when(pl.col(\"sex_match\").is_null())\n",
    "        .then(None)\n",
    "        .otherwise(0)\n",
    "        .alias(\"is_sex_match\"),\n",
    "    )\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "\n",
    "# apply Kaplan-Meier\n",
    "y_kaplan = transform_survival_probability(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_kaplan).alias(\"y_kaplan\"))\n",
    "\n",
    "# apply Nelson-Aalen\n",
    "y_nelson = transform_cumulative_hazard(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_nelson).alias(\"y_nelson\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Make fold column\n",
    "# ====================================================\n",
    "# race_group_efs列を作成\n",
    "train = train.with_columns((pl.col(\"race_group\").cast(str) + \"_\" + pl.col(\"efs\").cast(str)).alias(\"race_group_efs\"))\n",
    "\n",
    "fold_array = np.zeros(train.height)\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train, train[CFG.stratified_col]), start=1):\n",
    "    fold_array[val_idx] = fold\n",
    "train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n",
    "\n",
    "# fold_array = np.zeros(train.height)\n",
    "# kf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "# for fold, (_, val_idx) in enumerate(kf.split(train), start=1):\n",
    "#     fold_array[val_idx] = fold\n",
    "# train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pandas\n",
    "\n",
    "train = train.to_pandas()\n",
    "test = test.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10', 'null_count', 'is_sex_match']\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Column selection\n",
    "# ====================================================\n",
    "# Feature columns\n",
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"y_kaplan\", \"y_nelson\", \"fold\", \"race_group_efs\", \"efs_time2\"]\n",
    "FEATURES = [c for c in train.columns if c not in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In these features, there are 35 CATEGORICAL FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n"
     ]
    }
   ],
   "source": [
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype == \"object\":\n",
    "        CATS.append(c)\n",
    "        train[c] = train[c].fillna(\"NAN\")\n",
    "        test[c] = test[c].fillna(\"NAN\")\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We LABEL ENCODE the CATEGORICAL FEATURES: dri_score, psych_disturb, cyto_score, diabetes, tbi_status, arrhythmia, graft_type, vent_hist, renal_issue, pulm_severe, prim_disease_hct, cmv_status, tce_imm_match, rituximab, prod_type, cyto_score_detail, conditioning_intensity, ethnicity, obesity, mrd_hct, in_vivo_tcd, tce_match, hepatic_severe, prior_tumor, peptic_ulcer, gvhd_proph, rheum_issue, sex_match, race_group, hepatic_mild, tce_div_match, donor_related, melphalan_dose, cardiac, pulm_moderate, "
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "# print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \", end=\"\")\n",
    "for c in FEATURES:\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \", end=\"\")\n",
    "        combined[c], _ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype == \"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype == \"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "\n",
    "train = combined.iloc[: len(train)].copy()\n",
    "test = combined.iloc[len(train) :].reset_index(drop=True).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical features\n",
    "# CATS = []\n",
    "# cat_count = 0\n",
    "# for c in FEATURES:\n",
    "#     if train[c].dtype == pl.String:\n",
    "#         cat_count += 1\n",
    "#         CATS.append(c)\n",
    "# print(f\"There are {cat_count} CATEGORICAL FEATURES: {CATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Label encode categorical features\n",
    "\n",
    "# # train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "# # 250109追記)カテゴリ型に変換するだけで充分かも\n",
    "# train = train.with_columns(pl.col(CATS).fill_null(\"NaN\").cast(pl.Categorical))\n",
    "# test = test.with_columns(pl.col(CATS).fill_null(\"NaN\").cast(pl.Categorical))\n",
    "\n",
    "# for c in CATS:\n",
    "#     pass\n",
    "#     # train, testで分けているのはkaggle対策（本来のtestにアクセスできないため）\n",
    "#     # OrdinalEncoderを使用しているのはtestに未知の値あっても指定の値(-1)に変換できるため\n",
    "#     # 250109追記）これだと未知の値を全て同じ値として扱ってしまうので、改善が必要かも\n",
    "#     # oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "#     # train = train.with_columns(\n",
    "#     #     pl.Series(oe.fit_transform(train[c].fill_null(\"NaN\").to_numpy().reshape(-1, 1)).reshape(-1))\n",
    "#     #     .cast(pl.String)\n",
    "#     #     .cast(pl.Categorical)\n",
    "#     #     .alias(c)\n",
    "#     # )\n",
    "#     # test = test.with_columns(\n",
    "#     #     pl.Series(oe.transform(test[c].fill_null(\"NaN\").to_numpy().reshape(-1, 1)).reshape(-1))\n",
    "#     #     .cast(pl.String)\n",
    "#     #     .cast(pl.Categorical)\n",
    "#     #     .alias(c)\n",
    "#     # )\n",
    "#     # # 本来のtestにアクセスできるコンペではtrain, testを結合してLabelEncodeすればよい\n",
    "#     # le = LabelEncoder()\n",
    "#     # train_test = train_test.with_columns(\n",
    "#     #     pl.Series(le.fit_transform(train_test[c].fill_null(\"NaN\")))\n",
    "#     #     .cast(pl.String)\n",
    "#     #     .cast(pl.Categorical)\n",
    "#     #     .alias(c)\n",
    "#     # )\n",
    "# # train = train_test.filter(pl.col(\"fold\").is_not_null())\n",
    "# # test = train_test.filter(pl.col(\"fold\").is_null())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Survival Cox model用のターゲット作成\n",
    "# ====================================================\n",
    "# create cox model's target\n",
    "\n",
    "# polars\n",
    "# train = train.with_columns(\n",
    "#     pl.when(pl.col(\"efs\") == 0).then(pl.col(\"efs_time\") * -1).otherwise(pl.col(\"efs_time\")).alias(\"efs_time2\")\n",
    "# )\n",
    "\n",
    "# pandas\n",
    "train[\"efs_time2\"] = train.efs_time.copy()\n",
    "train.loc[train.efs == 0, \"efs_time2\"] *= -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Training functions\n",
    "# ====================================================\n",
    "def lightgbm_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    model = LGBMRegressor(\n",
    "        **CFG.regression_lgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=CFG.early_stopping_round),\n",
    "            lgb.log_evaluation(CFG.verbose),\n",
    "        ],\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def xgboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_cox_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_cox_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, features, method, target_col, fold):\n",
    "    \"\"\"特徴量の重要度をプロットする関数\"\"\"\n",
    "    # 各モデルタイプに応じた特徴量重要度の取得方法\n",
    "    if method == \"lightgbm\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.feature_importances_})\n",
    "    elif method == \"xgboost\" or method == \"xgboost_cox\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.feature_importances_})\n",
    "    elif method == \"catboost\" or method == \"catboost_cox\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.get_feature_importance()})\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance.sort_values(\"importance\", ascending=False).head(20), x=\"importance\", y=\"feature\")\n",
    "    plt.title(f\"{method} Feature Importance\\nTarget: {target_col}, Fold: {fold}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存先のディレクトリを作成\n",
    "    save_dir = CFG.OUTPUT_DIR / \"feature_importance\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_dir / f\"feature_importance_{method}_{target_col}_fold{fold}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gradient_boosting_model_cv_training(\n",
    "    method: str, train_df: pd.DataFrame, target_col_list: list, features: list, categorical_features: list\n",
    "):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    for target_col in target_col_list:\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{method} training fold {fold+1} {target_col}\")\n",
    "            x_train = train_df[train_df[\"fold\"] != fold + 1][features]\n",
    "            y_train = train_df[train_df[\"fold\"] != fold + 1][target_col]\n",
    "            x_valid = train_df[train_df[\"fold\"] == fold + 1][features]\n",
    "            y_valid = train_df[train_df[\"fold\"] == fold + 1][target_col]\n",
    "\n",
    "            if method == \"lightgbm\":\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            elif method == \"xgboost\":\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost\":\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            # Cox models\n",
    "            elif method == \"xgboost_cox\":\n",
    "                model, valid_pred = xgboost_cox_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost_cox\":\n",
    "                model, valid_pred = catboost_cox_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "            # Feature Importanceの可視化(最後のfoldのみ)\n",
    "            if fold == CFG.n_folds - 1:\n",
    "                plot_feature_importance(model, features, method, target_col, fold + 1)\n",
    "\n",
    "            # Save best model\n",
    "            save_model_path = (\n",
    "                CFG.MODEL_PATH / f\"{method}_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\"\n",
    "            )\n",
    "            save_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            pickle.dump(\n",
    "                model,\n",
    "                open(\n",
    "                    save_model_path,\n",
    "                    \"wb\",\n",
    "                ),\n",
    "            )\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[train_df[\"fold\"] == fold + 1] = valid_pred\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_predictions_df = pd.DataFrame()\n",
    "        oof_predictions_df[\"ID\"] = train_df[\"ID\"].values\n",
    "        oof_predictions_df[\"prediction\"] = oof_predictions\n",
    "        oof_predictions_df.to_csv(\n",
    "            CFG.OUTPUT_DIR / f\"oof_{method}_{target_col}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\", index=False\n",
    "        )\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        y_true = train_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        m = score(y_true.copy(), oof_predictions_df.copy(), \"ID\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{method} our out of folds CV score is {m}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 882\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606188\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0245494\n",
      "[1000]\tvalid_0's l2: 0.0243226\n",
      "[1500]\tvalid_0's l2: 0.0242774\n",
      "Early stopping, best iteration is:\n",
      "[1377]\tvalid_0's l2: 0.0242553\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606660\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0238314\n",
      "[1000]\tvalid_0's l2: 0.0236817\n",
      "Early stopping, best iteration is:\n",
      "[978]\tvalid_0's l2: 0.0236652\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606506\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0237189\n",
      "[1000]\tvalid_0's l2: 0.0233601\n",
      "[1500]\tvalid_0's l2: 0.0232872\n",
      "[2000]\tvalid_0's l2: 0.0233161\n",
      "Early stopping, best iteration is:\n",
      "[1505]\tvalid_0's l2: 0.0232826\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606093\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.024673\n",
      "[1000]\tvalid_0's l2: 0.0243713\n",
      "[1500]\tvalid_0's l2: 0.0242525\n",
      "[2000]\tvalid_0's l2: 0.0242909\n",
      "Early stopping, best iteration is:\n",
      "[1517]\tvalid_0's l2: 0.024247\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606420\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246091\n",
      "[1000]\tvalid_0's l2: 0.0244706\n",
      "[1500]\tvalid_0's l2: 0.0244582\n",
      "Early stopping, best iteration is:\n",
      "[1208]\tvalid_0's l2: 0.024427\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606450\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0240071\n",
      "[1000]\tvalid_0's l2: 0.0239259\n",
      "[1500]\tvalid_0's l2: 0.0239403\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's l2: 0.0239153\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605974\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246738\n",
      "[1000]\tvalid_0's l2: 0.0244629\n",
      "Early stopping, best iteration is:\n",
      "[990]\tvalid_0's l2: 0.0244553\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605772\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0245292\n",
      "[1000]\tvalid_0's l2: 0.0241842\n",
      "[1500]\tvalid_0's l2: 0.0241253\n",
      "[2000]\tvalid_0's l2: 0.0241179\n",
      "Early stopping, best iteration is:\n",
      "[1917]\tvalid_0's l2: 0.0240926\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606044\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0248657\n",
      "[1000]\tvalid_0's l2: 0.0246941\n",
      "Early stopping, best iteration is:\n",
      "[926]\tvalid_0's l2: 0.0246711\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605780\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0243829\n",
      "[1000]\tvalid_0's l2: 0.0239453\n",
      "[1500]\tvalid_0's l2: 0.0238098\n",
      "[2000]\tvalid_0's l2: 0.0237764\n",
      "[2500]\tvalid_0's l2: 0.023726\n",
      "[3000]\tvalid_0's l2: 0.0237729\n",
      "Early stopping, best iteration is:\n",
      "[2601]\tvalid_0's l2: 0.0237187\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.6737897916156838\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 882\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539329\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0587048\n",
      "[1000]\tvalid_0's l2: 0.0580997\n",
      "Early stopping, best iteration is:\n",
      "[954]\tvalid_0's l2: 0.0580347\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538669\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0571666\n",
      "[1000]\tvalid_0's l2: 0.056893\n",
      "[1500]\tvalid_0's l2: 0.0568829\n",
      "Early stopping, best iteration is:\n",
      "[1388]\tvalid_0's l2: 0.0568573\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538856\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0564674\n",
      "[1000]\tvalid_0's l2: 0.0557599\n",
      "[1500]\tvalid_0's l2: 0.0556759\n",
      "Early stopping, best iteration is:\n",
      "[1279]\tvalid_0's l2: 0.0556437\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539416\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0587213\n",
      "[1000]\tvalid_0's l2: 0.0581664\n",
      "[1500]\tvalid_0's l2: 0.0579988\n",
      "[2000]\tvalid_0's l2: 0.0580246\n",
      "Early stopping, best iteration is:\n",
      "[1623]\tvalid_0's l2: 0.0579407\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539032\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0584274\n",
      "[1000]\tvalid_0's l2: 0.0579897\n",
      "[1500]\tvalid_0's l2: 0.0580464\n",
      "Early stopping, best iteration is:\n",
      "[1107]\tvalid_0's l2: 0.0579326\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538973\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0575327\n",
      "[1000]\tvalid_0's l2: 0.0574467\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's l2: 0.0573383\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539673\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0590042\n",
      "[1000]\tvalid_0's l2: 0.0585408\n",
      "[1500]\tvalid_0's l2: 0.0584986\n",
      "[2000]\tvalid_0's l2: 0.0586006\n",
      "Early stopping, best iteration is:\n",
      "[1517]\tvalid_0's l2: 0.0584881\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539898\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0585111\n",
      "[1000]\tvalid_0's l2: 0.0578484\n",
      "[1500]\tvalid_0's l2: 0.0577029\n",
      "Early stopping, best iteration is:\n",
      "[1420]\tvalid_0's l2: 0.0576708\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539493\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0587625\n",
      "[1000]\tvalid_0's l2: 0.0583574\n",
      "Early stopping, best iteration is:\n",
      "[947]\tvalid_0's l2: 0.0583422\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539971\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.058104\n",
      "[1000]\tvalid_0's l2: 0.0572832\n",
      "[1500]\tvalid_0's l2: 0.0571105\n",
      "[2000]\tvalid_0's l2: 0.0569904\n",
      "[2500]\tvalid_0's l2: 0.0569511\n",
      "[3000]\tvalid_0's l2: 0.0569554\n",
      "Early stopping, best iteration is:\n",
      "[2799]\tvalid_0's l2: 0.0568851\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.6759902784398699\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17639\n",
      "[500]\tvalidation_0-rmse:0.15693\n",
      "[1000]\tvalidation_0-rmse:0.15583\n",
      "[1500]\tvalidation_0-rmse:0.15553\n",
      "[2000]\tvalidation_0-rmse:0.15574\n",
      "[2307]\tvalidation_0-rmse:0.15581\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17322\n",
      "[500]\tvalidation_0-rmse:0.15474\n",
      "[1000]\tvalidation_0-rmse:0.15382\n",
      "[1500]\tvalidation_0-rmse:0.15369\n",
      "[1923]\tvalidation_0-rmse:0.15382\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17501\n",
      "[500]\tvalidation_0-rmse:0.15454\n",
      "[1000]\tvalidation_0-rmse:0.15327\n",
      "[1500]\tvalidation_0-rmse:0.15309\n",
      "[2000]\tvalidation_0-rmse:0.15328\n",
      "[2167]\tvalidation_0-rmse:0.15335\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17815\n",
      "[500]\tvalidation_0-rmse:0.15732\n",
      "[1000]\tvalidation_0-rmse:0.15585\n",
      "[1500]\tvalidation_0-rmse:0.15550\n",
      "[2000]\tvalidation_0-rmse:0.15554\n",
      "[2078]\tvalidation_0-rmse:0.15558\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17425\n",
      "[500]\tvalidation_0-rmse:0.15726\n",
      "[1000]\tvalidation_0-rmse:0.15662\n",
      "[1500]\tvalidation_0-rmse:0.15643\n",
      "[2000]\tvalidation_0-rmse:0.15657\n",
      "[2100]\tvalidation_0-rmse:0.15653\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17421\n",
      "[500]\tvalidation_0-rmse:0.15519\n",
      "[1000]\tvalidation_0-rmse:0.15454\n",
      "[1456]\tvalidation_0-rmse:0.15456\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17675\n",
      "[500]\tvalidation_0-rmse:0.15820\n",
      "[1000]\tvalidation_0-rmse:0.15721\n",
      "[1500]\tvalidation_0-rmse:0.15728\n",
      "[1767]\tvalidation_0-rmse:0.15734\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17938\n",
      "[500]\tvalidation_0-rmse:0.15691\n",
      "[1000]\tvalidation_0-rmse:0.15561\n",
      "[1500]\tvalidation_0-rmse:0.15536\n",
      "[2000]\tvalidation_0-rmse:0.15536\n",
      "[2183]\tvalidation_0-rmse:0.15546\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17816\n",
      "[500]\tvalidation_0-rmse:0.15799\n",
      "[1000]\tvalidation_0-rmse:0.15698\n",
      "[1500]\tvalidation_0-rmse:0.15704\n",
      "[1715]\tvalidation_0-rmse:0.15713\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17741\n",
      "[500]\tvalidation_0-rmse:0.15707\n",
      "[1000]\tvalidation_0-rmse:0.15530\n",
      "[1500]\tvalidation_0-rmse:0.15482\n",
      "[2000]\tvalidation_0-rmse:0.15453\n",
      "[2500]\tvalidation_0-rmse:0.15446\n",
      "[3000]\tvalidation_0-rmse:0.15462\n",
      "[3005]\tvalidation_0-rmse:0.15462\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6722085282189766\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27150\n",
      "[500]\tvalidation_0-rmse:0.24251\n",
      "[1000]\tvalidation_0-rmse:0.24094\n",
      "[1500]\tvalidation_0-rmse:0.24061\n",
      "[1762]\tvalidation_0-rmse:0.24067\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26740\n",
      "[500]\tvalidation_0-rmse:0.23968\n",
      "[1000]\tvalidation_0-rmse:0.23829\n",
      "[1500]\tvalidation_0-rmse:0.23812\n",
      "[1817]\tvalidation_0-rmse:0.23807\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26940\n",
      "[500]\tvalidation_0-rmse:0.23823\n",
      "[1000]\tvalidation_0-rmse:0.23657\n",
      "[1500]\tvalidation_0-rmse:0.23616\n",
      "[2000]\tvalidation_0-rmse:0.23644\n",
      "[2166]\tvalidation_0-rmse:0.23659\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27373\n",
      "[500]\tvalidation_0-rmse:0.24305\n",
      "[1000]\tvalidation_0-rmse:0.24095\n",
      "[1500]\tvalidation_0-rmse:0.24044\n",
      "[2000]\tvalidation_0-rmse:0.24049\n",
      "[2364]\tvalidation_0-rmse:0.24043\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26868\n",
      "[500]\tvalidation_0-rmse:0.24242\n",
      "[1000]\tvalidation_0-rmse:0.24148\n",
      "[1500]\tvalidation_0-rmse:0.24148\n",
      "[1733]\tvalidation_0-rmse:0.24164\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26903\n",
      "[500]\tvalidation_0-rmse:0.24037\n",
      "[1000]\tvalidation_0-rmse:0.23965\n",
      "[1436]\tvalidation_0-rmse:0.23993\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27211\n",
      "[500]\tvalidation_0-rmse:0.24409\n",
      "[1000]\tvalidation_0-rmse:0.24270\n",
      "[1500]\tvalidation_0-rmse:0.24263\n",
      "[1788]\tvalidation_0-rmse:0.24283\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27535\n",
      "[500]\tvalidation_0-rmse:0.24213\n",
      "[1000]\tvalidation_0-rmse:0.24013\n",
      "[1500]\tvalidation_0-rmse:0.23963\n",
      "[2000]\tvalidation_0-rmse:0.23975\n",
      "[2351]\tvalidation_0-rmse:0.23995\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27381\n",
      "[500]\tvalidation_0-rmse:0.24323\n",
      "[1000]\tvalidation_0-rmse:0.24200\n",
      "[1500]\tvalidation_0-rmse:0.24216\n",
      "[1722]\tvalidation_0-rmse:0.24210\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27295\n",
      "[500]\tvalidation_0-rmse:0.24217\n",
      "[1000]\tvalidation_0-rmse:0.23960\n",
      "[1500]\tvalidation_0-rmse:0.23881\n",
      "[2000]\tvalidation_0-rmse:0.23893\n",
      "[2136]\tvalidation_0-rmse:0.23897\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6755384377204804\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_kaplan\n",
      "0:\tlearn: 0.1762565\ttest: 0.1763624\tbest: 0.1763624 (0)\ttotal: 15.1ms\tremaining: 4h 12m 19s\n",
      "500:\tlearn: 0.1514103\ttest: 0.1581634\tbest: 0.1581623 (498)\ttotal: 5.03s\tremaining: 2h 47m 9s\n",
      "1000:\tlearn: 0.1461068\ttest: 0.1567917\tbest: 0.1567738 (995)\ttotal: 10.7s\tremaining: 2h 58m 35s\n",
      "1500:\tlearn: 0.1422055\ttest: 0.1561321\tbest: 0.1561321 (1500)\ttotal: 16.8s\tremaining: 3h 6m 11s\n",
      "2000:\tlearn: 0.1388246\ttest: 0.1558884\tbest: 0.1558854 (1999)\ttotal: 22.4s\tremaining: 3h 6m\n",
      "2500:\tlearn: 0.1357041\ttest: 0.1557765\tbest: 0.1557597 (2449)\ttotal: 28.2s\tremaining: 3h 7m 8s\n",
      "3000:\tlearn: 0.1328935\ttest: 0.1555809\tbest: 0.1555809 (3000)\ttotal: 34.1s\tremaining: 3h 8m 55s\n",
      "3500:\tlearn: 0.1302182\ttest: 0.1555196\tbest: 0.1554918 (3411)\ttotal: 40.3s\tremaining: 3h 11m 24s\n",
      "4000:\tlearn: 0.1277265\ttest: 0.1554446\tbest: 0.1554423 (3996)\ttotal: 46.6s\tremaining: 3h 13m 8s\n",
      "4500:\tlearn: 0.1254581\ttest: 0.1554566\tbest: 0.1554169 (4139)\ttotal: 52.3s\tremaining: 3h 12m 56s\n",
      "bestTest = 0.15541687\n",
      "bestIteration = 4139\n",
      "Shrink model to first 4140 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_kaplan\n",
      "0:\tlearn: 0.1766079\ttest: 0.1731950\tbest: 0.1731950 (0)\ttotal: 8.99ms\tremaining: 2h 29m 49s\n",
      "500:\tlearn: 0.1515565\ttest: 0.1556624\tbest: 0.1556624 (500)\ttotal: 4.81s\tremaining: 2h 40m 4s\n",
      "1000:\tlearn: 0.1464292\ttest: 0.1546357\tbest: 0.1546323 (993)\ttotal: 10.5s\tremaining: 2h 54m 14s\n",
      "1500:\tlearn: 0.1426612\ttest: 0.1543616\tbest: 0.1543616 (1500)\ttotal: 16s\tremaining: 2h 57m 32s\n",
      "2000:\tlearn: 0.1393706\ttest: 0.1541546\tbest: 0.1541453 (1989)\ttotal: 21.4s\tremaining: 2h 58m 14s\n",
      "2500:\tlearn: 0.1364470\ttest: 0.1540038\tbest: 0.1540038 (2500)\ttotal: 27s\tremaining: 2h 59m 29s\n",
      "3000:\tlearn: 0.1336636\ttest: 0.1539094\tbest: 0.1539034 (2921)\ttotal: 33s\tremaining: 3h 2m 28s\n",
      "3500:\tlearn: 0.1311178\ttest: 0.1539314\tbest: 0.1538808 (3338)\ttotal: 38.8s\tremaining: 3h 4m 3s\n",
      "bestTest = 0.1538808458\n",
      "bestIteration = 3338\n",
      "Shrink model to first 3339 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_kaplan\n",
      "0:\tlearn: 0.1764088\ttest: 0.1750198\tbest: 0.1750198 (0)\ttotal: 10.5ms\tremaining: 2h 55m 22s\n",
      "500:\tlearn: 0.1515006\ttest: 0.1558685\tbest: 0.1558685 (500)\ttotal: 5.2s\tremaining: 2h 53m 2s\n",
      "1000:\tlearn: 0.1462658\ttest: 0.1546287\tbest: 0.1546249 (998)\ttotal: 11s\tremaining: 3h 3m 35s\n",
      "1500:\tlearn: 0.1423376\ttest: 0.1540829\tbest: 0.1540829 (1500)\ttotal: 17s\tremaining: 3h 8m 27s\n",
      "2000:\tlearn: 0.1390090\ttest: 0.1538447\tbest: 0.1538412 (1999)\ttotal: 22.8s\tremaining: 3h 9m 10s\n",
      "2500:\tlearn: 0.1360430\ttest: 0.1537257\tbest: 0.1537257 (2500)\ttotal: 28.5s\tremaining: 3h 9m 24s\n",
      "3000:\tlearn: 0.1333308\ttest: 0.1535226\tbest: 0.1535226 (3000)\ttotal: 34.3s\tremaining: 3h 9m 47s\n",
      "3500:\tlearn: 0.1307920\ttest: 0.1534190\tbest: 0.1533979 (3244)\ttotal: 40s\tremaining: 3h 9m 42s\n",
      "bestTest = 0.1533979496\n",
      "bestIteration = 3244\n",
      "Shrink model to first 3245 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_kaplan\n",
      "0:\tlearn: 0.1760690\ttest: 0.1781163\tbest: 0.1781163 (0)\ttotal: 10.4ms\tremaining: 2h 52m 45s\n",
      "500:\tlearn: 0.1510184\ttest: 0.1586642\tbest: 0.1586642 (500)\ttotal: 4.87s\tremaining: 2h 41m 59s\n",
      "1000:\tlearn: 0.1458109\ttest: 0.1571779\tbest: 0.1571777 (999)\ttotal: 10.1s\tremaining: 2h 47m 52s\n",
      "1500:\tlearn: 0.1419099\ttest: 0.1565074\tbest: 0.1565009 (1466)\ttotal: 15.3s\tremaining: 2h 50m 5s\n",
      "2000:\tlearn: 0.1383160\ttest: 0.1561359\tbest: 0.1561335 (1997)\ttotal: 21.2s\tremaining: 2h 56m 30s\n",
      "2500:\tlearn: 0.1352391\ttest: 0.1559989\tbest: 0.1559895 (2421)\ttotal: 27.2s\tremaining: 3h 38s\n",
      "3000:\tlearn: 0.1323474\ttest: 0.1557624\tbest: 0.1557565 (2999)\ttotal: 33.1s\tremaining: 3h 3m 18s\n",
      "3500:\tlearn: 0.1296850\ttest: 0.1556946\tbest: 0.1556814 (3395)\ttotal: 39.1s\tremaining: 3h 5m 17s\n",
      "4000:\tlearn: 0.1271259\ttest: 0.1556355\tbest: 0.1556348 (3994)\ttotal: 45.1s\tremaining: 3h 6m 55s\n",
      "4500:\tlearn: 0.1246915\ttest: 0.1556003\tbest: 0.1556003 (4500)\ttotal: 50.5s\tremaining: 3h 6m 4s\n",
      "5000:\tlearn: 0.1224058\ttest: 0.1556399\tbest: 0.1555408 (4744)\ttotal: 56.1s\tremaining: 3h 6m 10s\n",
      "bestTest = 0.1555407896\n",
      "bestIteration = 4744\n",
      "Shrink model to first 4745 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_kaplan\n",
      "0:\tlearn: 0.1764712\ttest: 0.1742453\tbest: 0.1742453 (0)\ttotal: 13.2ms\tremaining: 3h 40m 25s\n",
      "500:\tlearn: 0.1511676\ttest: 0.1577959\tbest: 0.1577864 (498)\ttotal: 5.22s\tremaining: 2h 53m 26s\n",
      "1000:\tlearn: 0.1458766\ttest: 0.1570994\tbest: 0.1570994 (1000)\ttotal: 11s\tremaining: 3h 2m 40s\n",
      "1500:\tlearn: 0.1417969\ttest: 0.1568707\tbest: 0.1568344 (1465)\ttotal: 17.1s\tremaining: 3h 9m 43s\n",
      "2000:\tlearn: 0.1383165\ttest: 0.1568106\tbest: 0.1568087 (1999)\ttotal: 23.5s\tremaining: 3h 15m 29s\n",
      "2500:\tlearn: 0.1351951\ttest: 0.1567114\tbest: 0.1567068 (2433)\ttotal: 29.7s\tremaining: 3h 17m 8s\n",
      "3000:\tlearn: 0.1322104\ttest: 0.1567436\tbest: 0.1567053 (2503)\ttotal: 35.7s\tremaining: 3h 17m 27s\n",
      "bestTest = 0.1567052778\n",
      "bestIteration = 2503\n",
      "Shrink model to first 2504 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_kaplan\n",
      "0:\tlearn: 0.1764928\ttest: 0.1741790\tbest: 0.1741790 (0)\ttotal: 8.65ms\tremaining: 2h 24m 12s\n",
      "500:\tlearn: 0.1515514\ttest: 0.1563570\tbest: 0.1563570 (500)\ttotal: 5.07s\tremaining: 2h 48m 37s\n",
      "1000:\tlearn: 0.1463578\ttest: 0.1552994\tbest: 0.1552852 (996)\ttotal: 11s\tremaining: 3h 2m 31s\n",
      "1500:\tlearn: 0.1424088\ttest: 0.1548863\tbest: 0.1548863 (1500)\ttotal: 17s\tremaining: 3h 8m 27s\n",
      "2000:\tlearn: 0.1389584\ttest: 0.1546072\tbest: 0.1546036 (1996)\ttotal: 22.9s\tremaining: 3h 10m 34s\n",
      "2500:\tlearn: 0.1358795\ttest: 0.1545715\tbest: 0.1545576 (2489)\ttotal: 29s\tremaining: 3h 12m 54s\n",
      "3000:\tlearn: 0.1330614\ttest: 0.1545323\tbest: 0.1545216 (2974)\ttotal: 35.2s\tremaining: 3h 14m 43s\n",
      "3500:\tlearn: 0.1303967\ttest: 0.1544196\tbest: 0.1544077 (3372)\ttotal: 41.2s\tremaining: 3h 15m 21s\n",
      "4000:\tlearn: 0.1279823\ttest: 0.1544723\tbest: 0.1543860 (3567)\ttotal: 47.2s\tremaining: 3h 16m\n",
      "bestTest = 0.1543860327\n",
      "bestIteration = 3567\n",
      "Shrink model to first 3568 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_kaplan\n",
      "0:\tlearn: 0.1762187\ttest: 0.1767416\tbest: 0.1767416 (0)\ttotal: 8.62ms\tremaining: 2h 23m 38s\n",
      "500:\tlearn: 0.1510718\ttest: 0.1592101\tbest: 0.1592101 (500)\ttotal: 5.28s\tremaining: 2h 55m 31s\n",
      "1000:\tlearn: 0.1456179\ttest: 0.1579656\tbest: 0.1579656 (1000)\ttotal: 11.5s\tremaining: 3h 11m 16s\n",
      "1500:\tlearn: 0.1414672\ttest: 0.1574997\tbest: 0.1574948 (1495)\ttotal: 17.9s\tremaining: 3h 18m 37s\n",
      "2000:\tlearn: 0.1380023\ttest: 0.1571705\tbest: 0.1571555 (1978)\ttotal: 24.1s\tremaining: 3h 20m 12s\n",
      "2500:\tlearn: 0.1348991\ttest: 0.1570705\tbest: 0.1570684 (2476)\ttotal: 30.4s\tremaining: 3h 22m 19s\n",
      "3000:\tlearn: 0.1319206\ttest: 0.1570732\tbest: 0.1570522 (2860)\ttotal: 36.3s\tremaining: 3h 21m 3s\n",
      "3500:\tlearn: 0.1291996\ttest: 0.1571476\tbest: 0.1570517 (3202)\ttotal: 42.4s\tremaining: 3h 21m 4s\n",
      "bestTest = 0.1570517084\n",
      "bestIteration = 3202\n",
      "Shrink model to first 3203 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_kaplan\n",
      "0:\tlearn: 0.1759158\ttest: 0.1793696\tbest: 0.1793696 (0)\ttotal: 7.62ms\tremaining: 2h 7m 2s\n",
      "500:\tlearn: 0.1513074\ttest: 0.1585006\tbest: 0.1585006 (500)\ttotal: 5.11s\tremaining: 2h 50m 3s\n",
      "1000:\tlearn: 0.1460723\ttest: 0.1569840\tbest: 0.1569840 (1000)\ttotal: 10.8s\tremaining: 2h 59m 23s\n",
      "1500:\tlearn: 0.1421521\ttest: 0.1563407\tbest: 0.1563407 (1500)\ttotal: 16.5s\tremaining: 3h 3m 26s\n",
      "2000:\tlearn: 0.1387246\ttest: 0.1558945\tbest: 0.1558927 (1996)\ttotal: 22.3s\tremaining: 3h 5m 33s\n",
      "2500:\tlearn: 0.1356349\ttest: 0.1556674\tbest: 0.1556670 (2499)\ttotal: 28.3s\tremaining: 3h 7m 56s\n",
      "3000:\tlearn: 0.1328445\ttest: 0.1555705\tbest: 0.1555465 (2732)\ttotal: 34.3s\tremaining: 3h 10m 10s\n",
      "3500:\tlearn: 0.1301799\ttest: 0.1554394\tbest: 0.1554331 (3451)\ttotal: 40.5s\tremaining: 3h 12m 2s\n",
      "4000:\tlearn: 0.1276886\ttest: 0.1553994\tbest: 0.1553724 (3979)\ttotal: 46.3s\tremaining: 3h 12m 4s\n",
      "4500:\tlearn: 0.1253596\ttest: 0.1553241\tbest: 0.1553199 (4499)\ttotal: 51.8s\tremaining: 3h 10m 48s\n",
      "bestTest = 0.1553198702\n",
      "bestIteration = 4499\n",
      "Shrink model to first 4500 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_kaplan\n",
      "0:\tlearn: 0.1760691\ttest: 0.1781318\tbest: 0.1781318 (0)\ttotal: 8.1ms\tremaining: 2h 15m 1s\n",
      "500:\tlearn: 0.1512088\ttest: 0.1590164\tbest: 0.1590153 (499)\ttotal: 4.7s\tremaining: 2h 36m 13s\n",
      "1000:\tlearn: 0.1460010\ttest: 0.1577442\tbest: 0.1577442 (1000)\ttotal: 9.61s\tremaining: 2h 39m 55s\n",
      "1500:\tlearn: 0.1419611\ttest: 0.1573106\tbest: 0.1573093 (1497)\ttotal: 14.9s\tremaining: 2h 45m 32s\n",
      "2000:\tlearn: 0.1386566\ttest: 0.1570318\tbest: 0.1570199 (1981)\ttotal: 21s\tremaining: 2h 54m 42s\n",
      "2500:\tlearn: 0.1355754\ttest: 0.1568819\tbest: 0.1568536 (2341)\ttotal: 27.2s\tremaining: 3h 1m 5s\n",
      "3000:\tlearn: 0.1327138\ttest: 0.1566742\tbest: 0.1566611 (2991)\ttotal: 33.4s\tremaining: 3h 5m 1s\n",
      "3500:\tlearn: 0.1301126\ttest: 0.1566476\tbest: 0.1566268 (3263)\ttotal: 39.6s\tremaining: 3h 7m 45s\n",
      "4000:\tlearn: 0.1275610\ttest: 0.1566240\tbest: 0.1566240 (4000)\ttotal: 45.6s\tremaining: 3h 9m 21s\n",
      "4500:\tlearn: 0.1251760\ttest: 0.1567741\tbest: 0.1566067 (4049)\ttotal: 51.9s\tremaining: 3h 11m 11s\n",
      "bestTest = 0.1566066909\n",
      "bestIteration = 4049\n",
      "Shrink model to first 4050 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_kaplan\n",
      "0:\tlearn: 0.1761428\ttest: 0.1774015\tbest: 0.1774015 (0)\ttotal: 11.4ms\tremaining: 3h 10m 25s\n",
      "500:\tlearn: 0.1515392\ttest: 0.1586494\tbest: 0.1586448 (499)\ttotal: 5.03s\tremaining: 2h 47m 8s\n",
      "1000:\tlearn: 0.1464440\ttest: 0.1570582\tbest: 0.1570582 (1000)\ttotal: 10.8s\tremaining: 2h 58m 54s\n",
      "1500:\tlearn: 0.1426020\ttest: 0.1564577\tbest: 0.1564556 (1499)\ttotal: 16.2s\tremaining: 2h 59m 24s\n",
      "2000:\tlearn: 0.1392022\ttest: 0.1560857\tbest: 0.1560601 (1976)\ttotal: 21.6s\tremaining: 2h 59m 33s\n",
      "2500:\tlearn: 0.1362162\ttest: 0.1557416\tbest: 0.1557416 (2500)\ttotal: 27.2s\tremaining: 3h 30s\n",
      "3000:\tlearn: 0.1333436\ttest: 0.1556452\tbest: 0.1556337 (2957)\ttotal: 33.2s\tremaining: 3h 4m 5s\n",
      "3500:\tlearn: 0.1306369\ttest: 0.1555654\tbest: 0.1555342 (3459)\ttotal: 39s\tremaining: 3h 5m 7s\n",
      "4000:\tlearn: 0.1282110\ttest: 0.1554241\tbest: 0.1554189 (3994)\ttotal: 44.9s\tremaining: 3h 6m 29s\n",
      "4500:\tlearn: 0.1258789\ttest: 0.1553770\tbest: 0.1553550 (4341)\ttotal: 50.7s\tremaining: 3h 6m 58s\n",
      "5000:\tlearn: 0.1236334\ttest: 0.1553277\tbest: 0.1552987 (4884)\ttotal: 56.3s\tremaining: 3h 6m 46s\n",
      "bestTest = 0.1552987025\n",
      "bestIteration = 4884\n",
      "Shrink model to first 4885 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6743928115710978\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_nelson\n",
      "0:\tlearn: 0.2713487\ttest: 0.2714857\tbest: 0.2714857 (0)\ttotal: 10.6ms\tremaining: 2h 57m 1s\n",
      "500:\tlearn: 0.2340217\ttest: 0.2435480\tbest: 0.2435480 (500)\ttotal: 4.94s\tremaining: 2h 44m 17s\n",
      "1000:\tlearn: 0.2262522\ttest: 0.2416228\tbest: 0.2416091 (995)\ttotal: 10.7s\tremaining: 2h 57m 30s\n",
      "1500:\tlearn: 0.2204007\ttest: 0.2406871\tbest: 0.2406871 (1500)\ttotal: 16.3s\tremaining: 3h 28s\n",
      "2000:\tlearn: 0.2153150\ttest: 0.2404439\tbest: 0.2404284 (1986)\ttotal: 21.6s\tremaining: 2h 59m 51s\n",
      "2500:\tlearn: 0.2105767\ttest: 0.2401147\tbest: 0.2400880 (2449)\ttotal: 27.3s\tremaining: 3h 1m 26s\n",
      "3000:\tlearn: 0.2064148\ttest: 0.2397149\tbest: 0.2397149 (3000)\ttotal: 33.1s\tremaining: 3h 3m 28s\n",
      "3500:\tlearn: 0.2024915\ttest: 0.2396496\tbest: 0.2396268 (3425)\ttotal: 38.9s\tremaining: 3h 4m 35s\n",
      "4000:\tlearn: 0.1987118\ttest: 0.2395941\tbest: 0.2395927 (3999)\ttotal: 45.1s\tremaining: 3h 7m 1s\n",
      "4500:\tlearn: 0.1951969\ttest: 0.2396425\tbest: 0.2395894 (4006)\ttotal: 50.6s\tremaining: 3h 6m 23s\n",
      "bestTest = 0.2395894147\n",
      "bestIteration = 4006\n",
      "Shrink model to first 4007 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_nelson\n",
      "0:\tlearn: 0.2717946\ttest: 0.2673893\tbest: 0.2673893 (0)\ttotal: 11.1ms\tremaining: 3h 5m 24s\n",
      "500:\tlearn: 0.2341072\ttest: 0.2412178\tbest: 0.2412178 (500)\ttotal: 4.94s\tremaining: 2h 44m 8s\n",
      "1000:\tlearn: 0.2264892\ttest: 0.2397781\tbest: 0.2397750 (999)\ttotal: 10.4s\tremaining: 2h 53m 48s\n",
      "1500:\tlearn: 0.2209531\ttest: 0.2392849\tbest: 0.2392746 (1491)\ttotal: 16.1s\tremaining: 2h 58m 5s\n",
      "2000:\tlearn: 0.2160761\ttest: 0.2390118\tbest: 0.2389954 (1919)\ttotal: 22.1s\tremaining: 3h 4m 4s\n",
      "2500:\tlearn: 0.2116381\ttest: 0.2387996\tbest: 0.2387891 (2496)\ttotal: 28.3s\tremaining: 3h 8m 15s\n",
      "3000:\tlearn: 0.2075015\ttest: 0.2388264\tbest: 0.2387426 (2591)\ttotal: 34.4s\tremaining: 3h 10m 15s\n",
      "bestTest = 0.2387425583\n",
      "bestIteration = 2591\n",
      "Shrink model to first 2592 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_nelson\n",
      "0:\tlearn: 0.2715766\ttest: 0.2694078\tbest: 0.2694078 (0)\ttotal: 10.4ms\tremaining: 2h 53m\n",
      "500:\tlearn: 0.2343536\ttest: 0.2402965\tbest: 0.2402965 (500)\ttotal: 5.16s\tremaining: 2h 51m 36s\n",
      "1000:\tlearn: 0.2265626\ttest: 0.2382445\tbest: 0.2382437 (996)\ttotal: 11s\tremaining: 3h 3m 22s\n",
      "1500:\tlearn: 0.2207254\ttest: 0.2373434\tbest: 0.2373363 (1498)\ttotal: 17.1s\tremaining: 3h 10m\n",
      "2000:\tlearn: 0.2157865\ttest: 0.2370092\tbest: 0.2370080 (1999)\ttotal: 23.1s\tremaining: 3h 12m 12s\n",
      "2500:\tlearn: 0.2113872\ttest: 0.2368191\tbest: 0.2368013 (2416)\ttotal: 29.2s\tremaining: 3h 13m 50s\n",
      "3000:\tlearn: 0.2073230\ttest: 0.2365703\tbest: 0.2365518 (2964)\ttotal: 35s\tremaining: 3h 13m 43s\n",
      "3500:\tlearn: 0.2035381\ttest: 0.2363429\tbest: 0.2363429 (3500)\ttotal: 40.7s\tremaining: 3h 13m 12s\n",
      "4000:\tlearn: 0.1999412\ttest: 0.2365061\tbest: 0.2363321 (3504)\ttotal: 46.6s\tremaining: 3h 13m 19s\n",
      "bestTest = 0.2363320595\n",
      "bestIteration = 3504\n",
      "Shrink model to first 3505 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_nelson\n",
      "0:\tlearn: 0.2711050\ttest: 0.2736921\tbest: 0.2736921 (0)\ttotal: 9.73ms\tremaining: 2h 42m 14s\n",
      "500:\tlearn: 0.2335276\ttest: 0.2448228\tbest: 0.2448228 (500)\ttotal: 5.27s\tremaining: 2h 55m 7s\n",
      "1000:\tlearn: 0.2258755\ttest: 0.2427379\tbest: 0.2427377 (999)\ttotal: 11.1s\tremaining: 3h 5m 2s\n",
      "1500:\tlearn: 0.2199901\ttest: 0.2418589\tbest: 0.2418493 (1489)\ttotal: 17.1s\tremaining: 3h 9m 40s\n",
      "2000:\tlearn: 0.2147362\ttest: 0.2414593\tbest: 0.2414542 (1999)\ttotal: 22.9s\tremaining: 3h 10m 23s\n",
      "2500:\tlearn: 0.2100641\ttest: 0.2411846\tbest: 0.2411846 (2500)\ttotal: 28.7s\tremaining: 3h 10m 57s\n",
      "3000:\tlearn: 0.2057298\ttest: 0.2409473\tbest: 0.2409358 (2966)\ttotal: 34.5s\tremaining: 3h 11m\n",
      "3500:\tlearn: 0.2017277\ttest: 0.2409437\tbest: 0.2409328 (3164)\ttotal: 40.5s\tremaining: 3h 12m 11s\n",
      "4000:\tlearn: 0.1979013\ttest: 0.2408379\tbest: 0.2408379 (4000)\ttotal: 46.5s\tremaining: 3h 13m 3s\n",
      "4500:\tlearn: 0.1942541\ttest: 0.2407951\tbest: 0.2407306 (4130)\ttotal: 52.5s\tremaining: 3h 13m 42s\n",
      "bestTest = 0.2407306016\n",
      "bestIteration = 4130\n",
      "Shrink model to first 4131 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_nelson\n",
      "0:\tlearn: 0.2716334\ttest: 0.2686787\tbest: 0.2686787 (0)\ttotal: 10.4ms\tremaining: 2h 52m 56s\n",
      "500:\tlearn: 0.2336582\ttest: 0.2434816\tbest: 0.2434816 (500)\ttotal: 4.85s\tremaining: 2h 41m 22s\n",
      "1000:\tlearn: 0.2256962\ttest: 0.2424027\tbest: 0.2424012 (999)\ttotal: 10.4s\tremaining: 2h 52m 43s\n",
      "1500:\tlearn: 0.2197793\ttest: 0.2420766\tbest: 0.2420766 (1500)\ttotal: 16.2s\tremaining: 2h 59m 49s\n",
      "2000:\tlearn: 0.2145572\ttest: 0.2420259\tbest: 0.2419829 (1919)\ttotal: 22.1s\tremaining: 3h 3m 37s\n",
      "2500:\tlearn: 0.2098248\ttest: 0.2419109\tbest: 0.2419069 (2488)\ttotal: 27.4s\tremaining: 3h 2m 5s\n",
      "3000:\tlearn: 0.2054231\ttest: 0.2419638\tbest: 0.2419046 (2503)\ttotal: 33.1s\tremaining: 3h 3m 17s\n",
      "bestTest = 0.2419046151\n",
      "bestIteration = 2503\n",
      "Shrink model to first 2504 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_nelson\n",
      "0:\tlearn: 0.2716045\ttest: 0.2689873\tbest: 0.2689873 (0)\ttotal: 8.04ms\tremaining: 2h 14m\n",
      "500:\tlearn: 0.2341960\ttest: 0.2418000\tbest: 0.2418000 (500)\ttotal: 5.05s\tremaining: 2h 47m 57s\n",
      "1000:\tlearn: 0.2265112\ttest: 0.2402892\tbest: 0.2402737 (989)\ttotal: 11s\tremaining: 3h 2m 21s\n",
      "1500:\tlearn: 0.2205555\ttest: 0.2395634\tbest: 0.2395634 (1500)\ttotal: 16.5s\tremaining: 3h 2m 35s\n",
      "2000:\tlearn: 0.2156033\ttest: 0.2394243\tbest: 0.2394208 (1999)\ttotal: 22.3s\tremaining: 3h 5m 19s\n",
      "2500:\tlearn: 0.2110413\ttest: 0.2392453\tbest: 0.2392350 (2477)\ttotal: 28.3s\tremaining: 3h 7m 51s\n",
      "3000:\tlearn: 0.2068025\ttest: 0.2391338\tbest: 0.2391316 (2984)\ttotal: 33.6s\tremaining: 3h 5m 57s\n",
      "3500:\tlearn: 0.2028033\ttest: 0.2390759\tbest: 0.2390353 (3290)\ttotal: 39.1s\tremaining: 3h 5m 29s\n",
      "bestTest = 0.2390352808\n",
      "bestIteration = 3290\n",
      "Shrink model to first 3291 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_nelson\n",
      "0:\tlearn: 0.2712665\ttest: 0.2720674\tbest: 0.2720674 (0)\ttotal: 10.1ms\tremaining: 2h 48m 10s\n",
      "500:\tlearn: 0.2333975\ttest: 0.2457910\tbest: 0.2457910 (500)\ttotal: 5.06s\tremaining: 2h 48m 13s\n",
      "1000:\tlearn: 0.2253117\ttest: 0.2439559\tbest: 0.2439559 (1000)\ttotal: 10.9s\tremaining: 3h 1m 45s\n",
      "1500:\tlearn: 0.2192599\ttest: 0.2432886\tbest: 0.2432886 (1500)\ttotal: 16.9s\tremaining: 3h 7m 31s\n",
      "2000:\tlearn: 0.2140262\ttest: 0.2428403\tbest: 0.2428403 (2000)\ttotal: 22.9s\tremaining: 3h 10m 37s\n",
      "2500:\tlearn: 0.2093760\ttest: 0.2427590\tbest: 0.2427437 (2489)\ttotal: 28.7s\tremaining: 3h 10m 57s\n",
      "3000:\tlearn: 0.2048701\ttest: 0.2425864\tbest: 0.2425605 (2862)\ttotal: 34.1s\tremaining: 3h 8m 34s\n",
      "bestTest = 0.242560468\n",
      "bestIteration = 2862\n",
      "Shrink model to first 2863 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_nelson\n",
      "0:\tlearn: 0.2709002\ttest: 0.2752810\tbest: 0.2752810 (0)\ttotal: 8.57ms\tremaining: 2h 22m 55s\n",
      "500:\tlearn: 0.2339572\ttest: 0.2441433\tbest: 0.2441433 (500)\ttotal: 5.11s\tremaining: 2h 49m 53s\n",
      "1000:\tlearn: 0.2261909\ttest: 0.2420089\tbest: 0.2420089 (1000)\ttotal: 10.8s\tremaining: 2h 59m 29s\n",
      "1500:\tlearn: 0.2203641\ttest: 0.2413763\tbest: 0.2413751 (1497)\ttotal: 16.3s\tremaining: 3h 57s\n",
      "2000:\tlearn: 0.2152358\ttest: 0.2407760\tbest: 0.2407702 (1996)\ttotal: 21.9s\tremaining: 3h 2m 16s\n",
      "2500:\tlearn: 0.2105752\ttest: 0.2404788\tbest: 0.2404751 (2499)\ttotal: 27.6s\tremaining: 3h 3m 34s\n",
      "3000:\tlearn: 0.2063498\ttest: 0.2403724\tbest: 0.2403158 (2715)\ttotal: 33s\tremaining: 3h 2m 38s\n",
      "3500:\tlearn: 0.2023364\ttest: 0.2402650\tbest: 0.2402332 (3383)\ttotal: 38.6s\tremaining: 3h 3m 5s\n",
      "bestTest = 0.2402332061\n",
      "bestIteration = 3383\n",
      "Shrink model to first 3384 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_nelson\n",
      "0:\tlearn: 0.2710991\ttest: 0.2737969\tbest: 0.2737969 (0)\ttotal: 13.2ms\tremaining: 3h 39m 37s\n",
      "500:\tlearn: 0.2339194\ttest: 0.2447266\tbest: 0.2447266 (500)\ttotal: 4.92s\tremaining: 2h 43m 33s\n",
      "1000:\tlearn: 0.2261406\ttest: 0.2430942\tbest: 0.2430942 (1000)\ttotal: 10.3s\tremaining: 2h 52m 8s\n",
      "1500:\tlearn: 0.2201825\ttest: 0.2422809\tbest: 0.2422809 (1500)\ttotal: 16.4s\tremaining: 3h 1m 33s\n",
      "2000:\tlearn: 0.2152197\ttest: 0.2418663\tbest: 0.2418598 (1936)\ttotal: 22.2s\tremaining: 3h 4m 50s\n",
      "2500:\tlearn: 0.2105320\ttest: 0.2416759\tbest: 0.2416187 (2365)\ttotal: 27.9s\tremaining: 3h 5m 41s\n",
      "3000:\tlearn: 0.2062184\ttest: 0.2414144\tbest: 0.2414109 (2998)\ttotal: 33.7s\tremaining: 3h 6m 45s\n",
      "bestTest = 0.2414109225\n",
      "bestIteration = 2998\n",
      "Shrink model to first 2999 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_nelson\n",
      "0:\tlearn: 0.2711859\ttest: 0.2729184\tbest: 0.2729184 (0)\ttotal: 9.06ms\tremaining: 2h 31m 3s\n",
      "500:\tlearn: 0.2342701\ttest: 0.2444689\tbest: 0.2444603 (499)\ttotal: 4.86s\tremaining: 2h 41m 41s\n",
      "1000:\tlearn: 0.2265723\ttest: 0.2422216\tbest: 0.2422197 (999)\ttotal: 10.5s\tremaining: 2h 55m 2s\n",
      "1500:\tlearn: 0.2208410\ttest: 0.2411252\tbest: 0.2411197 (1498)\ttotal: 16.2s\tremaining: 3h 8s\n",
      "2000:\tlearn: 0.2158259\ttest: 0.2405686\tbest: 0.2405524 (1962)\ttotal: 21.7s\tremaining: 3h 45s\n",
      "2500:\tlearn: 0.2114112\ttest: 0.2401272\tbest: 0.2401248 (2460)\ttotal: 27.5s\tremaining: 3h 2m 58s\n",
      "3000:\tlearn: 0.2071375\ttest: 0.2398079\tbest: 0.2398061 (2997)\ttotal: 33.6s\tremaining: 3h 5m 50s\n",
      "3500:\tlearn: 0.2032417\ttest: 0.2397862\tbest: 0.2397329 (3444)\ttotal: 39.7s\tremaining: 3h 8m 15s\n",
      "4000:\tlearn: 0.1995797\ttest: 0.2396562\tbest: 0.2396381 (3990)\ttotal: 45.7s\tremaining: 3h 9m 25s\n",
      "4500:\tlearn: 0.1960984\ttest: 0.2395703\tbest: 0.2395079 (4367)\ttotal: 51.7s\tremaining: 3h 10m 37s\n",
      "5000:\tlearn: 0.1927885\ttest: 0.2394397\tbest: 0.2394312 (4978)\ttotal: 57.8s\tremaining: 3h 11m 33s\n",
      "5500:\tlearn: 0.1895903\ttest: 0.2394116\tbest: 0.2394065 (5438)\ttotal: 1m 3s\tremaining: 3h 11m 47s\n",
      "6000:\tlearn: 0.1866065\ttest: 0.2394031\tbest: 0.2392885 (5837)\ttotal: 1m 9s\tremaining: 3h 12m 29s\n",
      "bestTest = 0.2392884582\n",
      "bestIteration = 5837\n",
      "Shrink model to first 5838 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6776245343291826\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 1 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61913\n",
      "[500]\tvalidation_0-cox-nloglik:7.43048\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41847\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41487\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41374\n",
      "[2500]\tvalidation_0-cox-nloglik:7.41319\n",
      "[2688]\tvalidation_0-cox-nloglik:7.41317\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 2 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61516\n",
      "[500]\tvalidation_0-cox-nloglik:7.42103\n",
      "[1000]\tvalidation_0-cox-nloglik:7.40891\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40448\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40481\n",
      "[2082]\tvalidation_0-cox-nloglik:7.40500\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 3 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61993\n",
      "[500]\tvalidation_0-cox-nloglik:7.40996\n",
      "[1000]\tvalidation_0-cox-nloglik:7.39541\n",
      "[1500]\tvalidation_0-cox-nloglik:7.39046\n",
      "[2000]\tvalidation_0-cox-nloglik:7.38825\n",
      "[2500]\tvalidation_0-cox-nloglik:7.38728\n",
      "[2961]\tvalidation_0-cox-nloglik:7.38794\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 4 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.62001\n",
      "[500]\tvalidation_0-cox-nloglik:7.42765\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41295\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40712\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40434\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40317\n",
      "[3000]\tvalidation_0-cox-nloglik:7.40299\n",
      "[3346]\tvalidation_0-cox-nloglik:7.40270\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 5 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.62019\n",
      "[500]\tvalidation_0-cox-nloglik:7.43255\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41955\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41571\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41528\n",
      "[2134]\tvalidation_0-cox-nloglik:7.41502\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 6 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61935\n",
      "[500]\tvalidation_0-cox-nloglik:7.43071\n",
      "[1000]\tvalidation_0-cox-nloglik:7.42255\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41853\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41778\n",
      "[2429]\tvalidation_0-cox-nloglik:7.41851\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 7 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61899\n",
      "[500]\tvalidation_0-cox-nloglik:7.44408\n",
      "[1000]\tvalidation_0-cox-nloglik:7.43173\n",
      "[1500]\tvalidation_0-cox-nloglik:7.42757\n",
      "[2000]\tvalidation_0-cox-nloglik:7.42744\n",
      "[2098]\tvalidation_0-cox-nloglik:7.42741\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 8 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61997\n",
      "[500]\tvalidation_0-cox-nloglik:7.39626\n",
      "[1000]\tvalidation_0-cox-nloglik:7.37956\n",
      "[1500]\tvalidation_0-cox-nloglik:7.37325\n",
      "[2000]\tvalidation_0-cox-nloglik:7.36957\n",
      "[2500]\tvalidation_0-cox-nloglik:7.36876\n",
      "[3000]\tvalidation_0-cox-nloglik:7.36830\n",
      "[3500]\tvalidation_0-cox-nloglik:7.36853\n",
      "[3784]\tvalidation_0-cox-nloglik:7.36909\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 9 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61958\n",
      "[500]\tvalidation_0-cox-nloglik:7.42300\n",
      "[1000]\tvalidation_0-cox-nloglik:7.40921\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40498\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40331\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40270\n",
      "[2865]\tvalidation_0-cox-nloglik:7.40382\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 10 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61915\n",
      "[500]\tvalidation_0-cox-nloglik:7.42752\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41066\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40279\n",
      "[2000]\tvalidation_0-cox-nloglik:7.39812\n",
      "[2500]\tvalidation_0-cox-nloglik:7.39609\n",
      "[3000]\tvalidation_0-cox-nloglik:7.39483\n",
      "[3465]\tvalidation_0-cox-nloglik:7.39511\n",
      "==================================================\n",
      "xgboost_cox our out of folds CV score is 0.6736017081375693\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 1 efs_time2\n",
      "0:\tlearn: -137189.3077450\ttest: -11837.7556858\tbest: -11837.7556858 (0)\ttotal: 27.8ms\tremaining: 7h 43m 30s\n",
      "500:\tlearn: -133896.7672740\ttest: -11540.1423508\tbest: -11540.1423508 (500)\ttotal: 7.81s\tremaining: 4h 19m 43s\n",
      "1000:\tlearn: -133191.8440304\ttest: -11517.6463710\tbest: -11517.5032427 (997)\ttotal: 15.4s\tremaining: 4h 15m 45s\n",
      "1500:\tlearn: -132741.8840634\ttest: -11512.5230886\tbest: -11512.3106746 (1471)\ttotal: 22.9s\tremaining: 4h 13m 43s\n",
      "2000:\tlearn: -132366.8459087\ttest: -11509.6629021\tbest: -11509.6507967 (1999)\ttotal: 30.4s\tremaining: 4h 12m 57s\n",
      "2500:\tlearn: -132057.6790453\ttest: -11510.5011325\tbest: -11509.3168143 (2154)\ttotal: 37.9s\tremaining: 4h 12m 7s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11509.31681\n",
      "bestIteration = 2154\n",
      "\n",
      "Shrink model to first 2155 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 2 efs_time2\n",
      "0:\tlearn: -137202.5995846\ttest: -11832.3506454\tbest: -11832.3506454 (0)\ttotal: 30.5ms\tremaining: 8h 28m 22s\n",
      "500:\tlearn: -133940.3587167\ttest: -11529.7069581\tbest: -11529.6968438 (499)\ttotal: 7.92s\tremaining: 4h 23m 19s\n",
      "1000:\tlearn: -133205.3190910\ttest: -11513.3032567\tbest: -11513.2352011 (984)\ttotal: 15.6s\tremaining: 4h 18m 40s\n",
      "1500:\tlearn: -132750.8584403\ttest: -11509.3071835\tbest: -11509.1168594 (1490)\ttotal: 23.2s\tremaining: 4h 16m 50s\n",
      "2000:\tlearn: -132412.4546087\ttest: -11509.3600520\tbest: -11508.5153803 (1744)\ttotal: 30.6s\tremaining: 4h 14m 41s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11508.51538\n",
      "bestIteration = 1744\n",
      "\n",
      "Shrink model to first 1745 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 3 efs_time2\n",
      "0:\tlearn: -137188.0147447\ttest: -11839.0302130\tbest: -11839.0302130 (0)\ttotal: 26.8ms\tremaining: 7h 26m\n",
      "500:\tlearn: -133899.7483814\ttest: -11507.2798216\tbest: -11507.2798216 (500)\ttotal: 7.93s\tremaining: 4h 23m 44s\n",
      "1000:\tlearn: -133135.5614495\ttest: -11490.4942199\tbest: -11490.4768909 (998)\ttotal: 15.7s\tremaining: 4h 21m 5s\n",
      "1500:\tlearn: -132672.9874902\ttest: -11486.7770410\tbest: -11486.6467015 (1476)\ttotal: 23.4s\tremaining: 4h 19m 8s\n",
      "2000:\tlearn: -132326.4175503\ttest: -11486.9959317\tbest: -11485.6496902 (1722)\ttotal: 31.2s\tremaining: 4h 19m 14s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11485.64969\n",
      "bestIteration = 1722\n",
      "\n",
      "Shrink model to first 1723 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 4 efs_time2\n",
      "0:\tlearn: -137198.5745979\ttest: -11832.4474258\tbest: -11832.4474258 (0)\ttotal: 19.3ms\tremaining: 5h 22m 17s\n",
      "500:\tlearn: -133923.4990259\ttest: -11533.8620694\tbest: -11533.8620694 (500)\ttotal: 7.95s\tremaining: 4h 24m 13s\n",
      "1000:\tlearn: -133217.2632634\ttest: -11517.6340456\tbest: -11517.6340456 (1000)\ttotal: 15.5s\tremaining: 4h 18m 31s\n",
      "1500:\tlearn: -132743.8501029\ttest: -11514.0613720\tbest: -11514.0544405 (1424)\ttotal: 23.4s\tremaining: 4h 19m 56s\n",
      "2000:\tlearn: -132423.1067254\ttest: -11514.6928720\tbest: -11513.5953222 (1528)\ttotal: 31s\tremaining: 4h 17m 31s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11513.59532\n",
      "bestIteration = 1528\n",
      "\n",
      "Shrink model to first 1529 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 5 efs_time2\n",
      "0:\tlearn: -137203.5696104\ttest: -11833.0166421\tbest: -11833.0166421 (0)\ttotal: 19.2ms\tremaining: 5h 19m 15s\n",
      "500:\tlearn: -133910.1424455\ttest: -11545.1267568\tbest: -11545.1267568 (500)\ttotal: 7.99s\tremaining: 4h 25m 33s\n",
      "1000:\tlearn: -133128.9915965\ttest: -11535.9627049\tbest: -11535.9350293 (999)\ttotal: 15.7s\tremaining: 4h 20m 39s\n",
      "1500:\tlearn: -132664.8583587\ttest: -11533.8370944\tbest: -11533.7920994 (1496)\ttotal: 23.3s\tremaining: 4h 18m 31s\n",
      "2000:\tlearn: -132329.5297288\ttest: -11534.8844650\tbest: -11532.9128559 (1727)\ttotal: 30.8s\tremaining: 4h 16m 25s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11532.91286\n",
      "bestIteration = 1727\n",
      "\n",
      "Shrink model to first 1728 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 6 efs_time2\n",
      "0:\tlearn: -137215.7949355\ttest: -11823.6207905\tbest: -11823.6207905 (0)\ttotal: 23.3ms\tremaining: 6h 28m 17s\n",
      "500:\tlearn: -133911.9780328\ttest: -11528.5667436\tbest: -11528.5485366 (499)\ttotal: 7.94s\tremaining: 4h 23m 58s\n",
      "1000:\tlearn: -133187.1551511\ttest: -11520.8805309\tbest: -11520.5931721 (965)\ttotal: 15.7s\tremaining: 4h 20m 22s\n",
      "1500:\tlearn: -132795.6323969\ttest: -11520.8100593\tbest: -11520.3434723 (1166)\ttotal: 23.3s\tremaining: 4h 17m 56s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11520.34347\n",
      "bestIteration = 1166\n",
      "\n",
      "Shrink model to first 1167 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 7 efs_time2\n",
      "0:\tlearn: -137210.5079381\ttest: -11823.4044431\tbest: -11823.4044431 (0)\ttotal: 24.2ms\tremaining: 6h 43m 7s\n",
      "500:\tlearn: -133902.4897901\ttest: -11553.2026922\tbest: -11553.1776458 (499)\ttotal: 7.91s\tremaining: 4h 22m 50s\n",
      "1000:\tlearn: -133142.8724027\ttest: -11538.1355336\tbest: -11538.0609056 (999)\ttotal: 15.6s\tremaining: 4h 18m 56s\n",
      "1500:\tlearn: -132701.7939306\ttest: -11536.8775464\tbest: -11536.6421771 (1437)\ttotal: 23.2s\tremaining: 4h 17m 7s\n",
      "2000:\tlearn: -132346.6109545\ttest: -11535.4494672\tbest: -11535.4074718 (1995)\ttotal: 30.6s\tremaining: 4h 14m 23s\n",
      "2500:\tlearn: -132027.9590527\ttest: -11537.4307718\tbest: -11535.2367362 (2026)\ttotal: 37.9s\tremaining: 4h 12m 11s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11535.23674\n",
      "bestIteration = 2026\n",
      "\n",
      "Shrink model to first 2027 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 8 efs_time2\n",
      "0:\tlearn: -137206.5982321\ttest: -11823.7739184\tbest: -11823.7739184 (0)\ttotal: 19.9ms\tremaining: 5h 31m 11s\n",
      "500:\tlearn: -133984.1446242\ttest: -11480.0750298\tbest: -11480.0577321 (498)\ttotal: 7.95s\tremaining: 4h 24m 24s\n",
      "1000:\tlearn: -133216.4401803\ttest: -11462.1081496\tbest: -11462.0405028 (992)\ttotal: 15.7s\tremaining: 4h 21m 26s\n",
      "1500:\tlearn: -132805.0523778\ttest: -11459.0410072\tbest: -11458.8298935 (1447)\ttotal: 23.5s\tremaining: 4h 20m 45s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11458.82989\n",
      "bestIteration = 1447\n",
      "\n",
      "Shrink model to first 1448 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 9 efs_time2\n",
      "0:\tlearn: -137190.0338615\ttest: -11838.3203785\tbest: -11838.3203785 (0)\ttotal: 22.6ms\tremaining: 6h 16m 40s\n",
      "500:\tlearn: -133931.2039681\ttest: -11534.9498983\tbest: -11534.9498983 (500)\ttotal: 8.09s\tremaining: 4h 28m 53s\n",
      "1000:\tlearn: -133194.1818632\ttest: -11522.4211842\tbest: -11522.4211842 (1000)\ttotal: 15.8s\tremaining: 4h 22m 46s\n",
      "1500:\tlearn: -132763.2264564\ttest: -11520.7876286\tbest: -11520.3224800 (1480)\ttotal: 23.3s\tremaining: 4h 18m 50s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11520.32248\n",
      "bestIteration = 1480\n",
      "\n",
      "Shrink model to first 1481 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 10 efs_time2\n",
      "0:\tlearn: -137188.5978555\ttest: -11837.4850753\tbest: -11837.4850753 (0)\ttotal: 20ms\tremaining: 5h 33m 17s\n",
      "500:\tlearn: -133915.6389266\ttest: -11541.3203682\tbest: -11541.3203682 (500)\ttotal: 7.98s\tremaining: 4h 25m 26s\n",
      "1000:\tlearn: -133182.3812277\ttest: -11521.7482195\tbest: -11521.6902021 (998)\ttotal: 15.8s\tremaining: 4h 22m 27s\n",
      "1500:\tlearn: -132698.5544284\ttest: -11514.7232231\tbest: -11514.7032691 (1499)\ttotal: 23.5s\tremaining: 4h 20m 56s\n",
      "2000:\tlearn: -132339.4706809\ttest: -11512.7460420\tbest: -11512.1765160 (1905)\ttotal: 31.1s\tremaining: 4h 18m 18s\n",
      "2500:\tlearn: -132040.2051440\ttest: -11513.0673654\tbest: -11511.9372478 (2276)\ttotal: 38.4s\tremaining: 4h 15m 12s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11511.93725\n",
      "bestIteration = 2276\n",
      "\n",
      "Shrink model to first 2277 iterations.\n",
      "==================================================\n",
      "catboost_cox our out of folds CV score is 0.6717630574738233\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Training\n",
    "# ====================================================\n",
    "# for method in CFG.METHOD_LIST:\n",
    "#     gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n",
    "\n",
    "# kaplan-meier & nelson-aalen models\n",
    "for method in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n",
    "    gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n",
    "# Cox models\n",
    "for method in [\"xgboost_cox\", \"catboost_cox\"]:\n",
    "    gradient_boosting_model_cv_training(method, train, CFG.cox_target_col_list, FEATURES, CATS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for Ensemble = 0.6813499611954243\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Overall CV\n",
    "# ====================================================\n",
    "# kaplan-meier models\n",
    "oof_lgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# nelson-aalen models\n",
    "oof_lgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "# Cox models\n",
    "oof_cox_xgb = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cox_cat = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# # polars\n",
    "# y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].clone()\n",
    "# y_pred = train[[\"ID\"]].clone()\n",
    "# ensamble_prediction = (\n",
    "#     rankdata(oof_xgb_kaplan)\n",
    "#     + rankdata(oof_cat_kaplan)\n",
    "#     + rankdata(oof_lgb_kaplan)\n",
    "#     + rankdata(oof_lgb_nelson)\n",
    "#     + rankdata(oof_xgb_nelson)\n",
    "#     + rankdata(oof_cat_nelson)\n",
    "#     + rankdata(oof_cox_xgb)\n",
    "#     + rankdata(oof_cox_cat)\n",
    "# )\n",
    "# y_pred = y_pred.with_columns(pl.Series(ensamble_prediction).alias(\"prediction\"))\n",
    "# m = score(y_true.to_pandas().copy(), y_pred.to_pandas().copy(), \"ID\")\n",
    "# print(\"\\nOverall CV for Ensemble =\", m)\n",
    "\n",
    "# pandas\n",
    "y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "ensamble_prediction = (\n",
    "    rankdata(oof_xgb_kaplan)\n",
    "    + rankdata(oof_cat_kaplan)\n",
    "    + rankdata(oof_lgb_kaplan)\n",
    "    + rankdata(oof_lgb_nelson)\n",
    "    + rankdata(oof_xgb_nelson)\n",
    "    + rankdata(oof_cat_nelson)\n",
    "    + rankdata(oof_cox_xgb)\n",
    "    + rankdata(oof_cox_cat)\n",
    ")\n",
    "y_pred[\"prediction\"] = ensamble_prediction\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(\"\\nOverall CV for Ensemble =\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な重み: [ 0.12081769  0.10278806  0.04831965  0.12423459 -0.06554445  0.2514433\n",
      "  0.36799918  0.1182188 ]\n",
      "最適化後のスコア: -0.6825346392733045\n"
     ]
    }
   ],
   "source": [
    "def ensemble_score(weights):\n",
    "    # 重み付けした予測値を計算\n",
    "    weighted_pred = (\n",
    "        weights[0] * rankdata(oof_lgb_kaplan)\n",
    "        + weights[1] * rankdata(oof_xgb_kaplan)\n",
    "        + weights[2] * rankdata(oof_cat_kaplan)\n",
    "        + weights[3] * rankdata(oof_lgb_nelson)\n",
    "        + weights[4] * rankdata(oof_xgb_nelson)\n",
    "        + weights[5] * rankdata(oof_cat_nelson)\n",
    "        + weights[6] * rankdata(oof_cox_xgb)\n",
    "        + weights[7] * rankdata(oof_cox_cat)\n",
    "    )\n",
    "\n",
    "    y_pred = pd.DataFrame({\"ID\": train[\"ID\"], \"prediction\": weighted_pred})\n",
    "    y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "\n",
    "    return -score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "\n",
    "\n",
    "# 8つのモデルの初期重みを均等に設定\n",
    "initial_weights = [1 / 8] * 8\n",
    "\n",
    "# 最適化実行\n",
    "result = minimize(ensemble_score, initial_weights, method=\"Nelder-Mead\")\n",
    "\n",
    "print(\"最適な重み:\", result.x)\n",
    "print(\"最適化後のスコア:\", result.fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Inference functions\n",
    "# ====================================================\n",
    "def lightgbm_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"lightgbm_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def xgboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        # pred = model.predict(xgb.DMatrix(x_test, enable_categorical=True))\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list, target_col: str):\n",
    "    x_test = test_df[features]\n",
    "    if method == \"lightgbm\":\n",
    "        test_pred = lightgbm_inference(x_test, target_col)\n",
    "    if method == \"xgboost\":\n",
    "        test_pred = xgboost_inference(x_test, target_col)\n",
    "    if method == \"catboost\":\n",
    "        test_pred = catboost_inference(x_test, target_col)\n",
    "    # Cox models\n",
    "    elif method == \"xgboost_cox\":\n",
    "        test_pred = xgboost_cox_inference(x_test, target_col)\n",
    "    elif method == \"catboost_cox\":\n",
    "        test_pred = catboost_cox_inference(x_test, target_col)\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "def predicting(method_list: list, input_df: pd.DataFrame, target_col_list: list, features: list):\n",
    "    output_df = input_df.copy()\n",
    "    for target_col in target_col_list:\n",
    "        # output_df[target_col] = 0\n",
    "        for method in method_list:\n",
    "            output_df[f\"{method}_pred_{target_col}\"] = gradient_boosting_model_inference(\n",
    "                method, input_df, features, target_col\n",
    "            )\n",
    "            # output_df[target_col] += CFG.model_weight_dict[method] * output_df[f\"{method}_pred_{target_col}\"]\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub shape: (3, 2)\n",
      "      ID  prediction\n",
      "0  28800        16.0\n",
      "1  28801        24.0\n",
      "2  28802         8.0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Inference\n",
    "# ====================================================\n",
    "# kaplan-meier & nelson-aalen models\n",
    "output_df = predicting([\"lightgbm\", \"xgboost\", \"catboost\"], test, CFG.target_col_list, FEATURES)\n",
    "pred_lgb_kaplan = output_df[\"lightgbm_pred_y_kaplan\"]\n",
    "pred_xgb_kaplan = output_df[\"xgboost_pred_y_kaplan\"]\n",
    "pred_cat_kaplan = output_df[\"catboost_pred_y_kaplan\"]\n",
    "pred_lgb_nelson = output_df[\"lightgbm_pred_y_nelson\"]\n",
    "pred_xgb_nelson = output_df[\"xgboost_pred_y_nelson\"]\n",
    "pred_cat_nelson = output_df[\"catboost_pred_y_nelson\"]\n",
    "# Cox models\n",
    "cox_output_df = predicting([\"xgboost_cox\", \"catboost_cox\"], test, CFG.cox_target_col_list, FEATURES)\n",
    "pred_cox_xgb = cox_output_df[\"xgboost_cox_pred_efs_time2\"]\n",
    "pred_cox_cat = cox_output_df[\"catboost_cox_pred_efs_time2\"]\n",
    "\n",
    "submission = pd.read_csv(CFG.DATA_PATH / \"sample_submission.csv\")\n",
    "submission[\"prediction\"] = (\n",
    "    rankdata(pred_lgb_kaplan)\n",
    "    + rankdata(pred_xgb_kaplan)\n",
    "    + rankdata(pred_cat_kaplan)\n",
    "    + rankdata(pred_lgb_nelson)\n",
    "    + rankdata(pred_xgb_nelson)\n",
    "    + rankdata(pred_cat_nelson)\n",
    "    + rankdata(pred_cox_xgb)\n",
    "    + rankdata(pred_cox_cat)\n",
    ")\n",
    "submission.to_csv(CFG.OUTPUT_DIR / \"submission.csv\", index=False)\n",
    "print(\"Sub shape:\", submission.shape)\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
