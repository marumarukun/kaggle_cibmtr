{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import config  # edit config.py as needed\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter, NelsonAalenFitter\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from metric import score  # edit metric.py as needed\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import rankdata\n",
    "from seed import seed_everything  # edit seed.py as needed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    DRY_RUN = False\n",
    "    EXP_NAME = config.EXP_NAME\n",
    "    AUTHOR = \"marumarukun\"\n",
    "    COMPETITION = config.KAGGLE_COMPETITION_NAME\n",
    "    DATA_PATH = config.COMP_DATASET_DIR\n",
    "    OUTPUT_DIR = config.OUTPUT_DIR\n",
    "    MODEL_PATH = config.OUTPUT_DIR / \"models\"  # モデル作成・実験時はこちらを使用\n",
    "    # MODEL_PATH = config.ARTIFACT_EXP_DIR(config.EXP_NAME) / \"models\"  # 提出時はこちらを使用\n",
    "    METHOD_LIST = [\"xgboost_cox\", \"catboost_cox\", \"lightgbm\", \"xgboost\", \"catboost\"]\n",
    "    SEED = 42\n",
    "    n_folds = 2 if DRY_RUN else 10\n",
    "    target_col_list = [\"y_kaplan\", \"y_nelson\"]\n",
    "    cox_target_col_list = [\"efs_time2\"]\n",
    "    # group_col = \"race_group\"  # Required for GroupKFold (edit as needed)\n",
    "    stratified_col = \"race_group_efs\"  # Required for StratifiedKFold (edit as needed)\n",
    "    num_boost_round = 100 if DRY_RUN else 1000000\n",
    "    early_stopping_round = 10 if DRY_RUN else 500  # 10÷lrで設定\n",
    "    verbose = 500\n",
    "\n",
    "    # https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
    "    regression_lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        # \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"seed\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
    "    regression_xgb_params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # \"eval_metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    regression_xgb_cox_params = {\n",
    "        \"objective\": \"survival:cox\",\n",
    "        \"eval_metric\": \"cox-nloglik\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 3,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 80,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://catboost.ai/docs/en/references/training-parameters/\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostregressor\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostclassifier\n",
    "    regression_cat_params = {\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"GPU\",  # CPU/GPU\n",
    "    }\n",
    "    regression_cat_cox_params = {\n",
    "        \"loss_function\": \"Cox\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"CPU\",  # CPU/GPU\n",
    "    }\n",
    "\n",
    "    model_weight_dict = {\"lightgbm\": 0.40, \"xgboost\": 0.30, \"catboost\": 0.30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "seed_everything(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "train = pl.read_csv(CFG.DATA_PATH / \"train.csv\", try_parse_dates=True)\n",
    "test = pl.read_csv(CFG.DATA_PATH / \"test.csv\", try_parse_dates=True)\n",
    "# make index column\n",
    "# train = train.with_row_index()\n",
    "# test = test.with_row_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Preprocess(ここに前処理や特徴量エンジニアリングを記述)\n",
    "# ====================================================\n",
    "def transform_kaplan_target(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "\n",
    "\n",
    "def transform_nelson_target(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    naf = NelsonAalenFitter()\n",
    "    naf.fit(durations=df[time_col], event_observed=df[event_col])\n",
    "    y = naf.cumulative_hazard_at_times(df[time_col]).to_numpy()\n",
    "    return -y\n",
    "\n",
    "\n",
    "def preprocess(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    output = df.clone()\n",
    "    # 欠損値のカウント（最初に行う）\n",
    "    output = output.with_columns(pl.sum_horizontal(pl.all().is_null()).alias(\"null_count\"))\n",
    "\n",
    "    # ドナーと患者の性別マッチング\n",
    "    output = output.with_columns(\n",
    "        pl.when(pl.col(\"sex_match\").str.contains_any([\"M-M\", \"F-F\"]))\n",
    "        .then(1)\n",
    "        .when(pl.col(\"sex_match\").is_null())\n",
    "        .then(None)\n",
    "        .otherwise(0)\n",
    "        .alias(\"is_sex_match\"),\n",
    "    )\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "\n",
    "# apply Kaplan-Meier\n",
    "y_kaplan = transform_kaplan_target(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_kaplan).alias(\"y_kaplan\"))\n",
    "\n",
    "# apply Nelson-Aalen\n",
    "y_nelson = transform_nelson_target(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_nelson).alias(\"y_nelson\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Make fold column\n",
    "# ====================================================\n",
    "# race_group_efs列を作成\n",
    "train = train.with_columns((pl.col(\"race_group\").cast(str) + \"_\" + pl.col(\"efs\").cast(str)).alias(\"race_group_efs\"))\n",
    "\n",
    "fold_array = np.zeros(train.height)\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train, train[CFG.stratified_col]), start=1):\n",
    "    fold_array[val_idx] = fold\n",
    "train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n",
    "\n",
    "# fold_array = np.zeros(train.height)\n",
    "# kf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "# for fold, (_, val_idx) in enumerate(kf.split(train), start=1):\n",
    "#     fold_array[val_idx] = fold\n",
    "# train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# To pandas\n",
    "# ====================================================\n",
    "\n",
    "train = train.to_pandas()\n",
    "test = test.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Survival Cox model用のターゲット作成\n",
    "# ====================================================\n",
    "# create cox model's target\n",
    "\n",
    "# polars\n",
    "# train = train.with_columns(\n",
    "#     pl.when(pl.col(\"efs\") == 0).then(pl.col(\"efs_time\") * -1).otherwise(pl.col(\"efs_time\")).alias(\"efs_time2\")\n",
    "# )\n",
    "\n",
    "# pandas\n",
    "train[\"efs_time2\"] = train.efs_time.copy()\n",
    "train.loc[train.efs == 0, \"efs_time2\"] *= -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10', 'null_count', 'is_sex_match']\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Column selection\n",
    "# ====================================================\n",
    "# Feature columns\n",
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"y_kaplan\", \"y_nelson\", \"fold\", \"race_group_efs\", \"efs_time2\"]\n",
    "FEATURES = [c for c in train.columns if c not in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In these features, there are 35 CATEGORICAL FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n"
     ]
    }
   ],
   "source": [
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype == \"object\":\n",
    "        CATS.append(c)\n",
    "        train[c] = train[c].fillna(\"NAN\")\n",
    "        test[c] = test[c].fillna(\"NAN\")\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 105 target encoded features\n"
     ]
    }
   ],
   "source": [
    "# target encoding\n",
    "# target encoding\n",
    "def target_encoding(train_df, test_df, cat_features, target_cols):\n",
    "    \"\"\"\n",
    "    カテゴリカル変数に対してターゲットエンコーディングを行う関数\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    train_df : pd.DataFrame\n",
    "        学習データ\n",
    "    test_df : pd.DataFrame\n",
    "        テストデータ\n",
    "    cat_features : list\n",
    "        カテゴリカル変数のリスト\n",
    "    target_cols : list\n",
    "        ターゲット変数のリスト\n",
    "    \"\"\"\n",
    "    encoded_train = train_df.copy()\n",
    "    encoded_test = test_df.copy()\n",
    "\n",
    "    # 各ターゲットに対してエンコーディングを実行\n",
    "    for target in target_cols:\n",
    "        for cat in cat_features:\n",
    "            # 学習データの各foldでエンコーディング\n",
    "            target_mean_per_fold = {}\n",
    "            for fold in range(1, CFG.n_folds + 1):\n",
    "                # 現在のfold以外のデータでターゲットの平均を計算\n",
    "                train_mean = encoded_train[encoded_train[\"fold\"] != fold].groupby(cat)[target].mean()\n",
    "\n",
    "                # 現在のfoldのデータにエンコーディングを適用\n",
    "                target_mean_per_fold[fold] = train_mean\n",
    "                encoded_train.loc[encoded_train[\"fold\"] == fold, f\"{cat}_{target}_enc\"] = encoded_train.loc[\n",
    "                    encoded_train[\"fold\"] == fold, cat\n",
    "                ].map(train_mean)\n",
    "\n",
    "            # テストデータのエンコーディング（全学習データの平均を使用）\n",
    "            global_mean = encoded_train.groupby(cat)[target].mean()\n",
    "            encoded_test[f\"{cat}_{target}_enc\"] = encoded_test[cat].map(global_mean)\n",
    "\n",
    "            # 欠損値を全体の平均で埋める\n",
    "            global_target_mean = encoded_train[target].mean()\n",
    "            encoded_train[f\"{cat}_{target}_enc\"].fillna(global_target_mean, inplace=True)\n",
    "            encoded_test[f\"{cat}_{target}_enc\"].fillna(global_target_mean, inplace=True)\n",
    "\n",
    "    return encoded_train, encoded_test\n",
    "\n",
    "\n",
    "# ターゲットエンコーディングの実行\n",
    "train, test = target_encoding(\n",
    "    train,\n",
    "    test,\n",
    "    CATS,\n",
    "    [\"y_kaplan\", \"y_nelson\", \"efs_time2\"]\n",
    ")\n",
    "\n",
    "# 特徴量リストの更新\n",
    "encoded_features = [col for col in train.columns if col.endswith(\"_enc\")]\n",
    "FEATURES.extend(encoded_features)\n",
    "print(f\"Added {len(encoded_features)} target encoded features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We LABEL ENCODE the CATEGORICAL FEATURES: dri_score, psych_disturb, cyto_score, diabetes, tbi_status, arrhythmia, graft_type, vent_hist, renal_issue, pulm_severe, prim_disease_hct, cmv_status, tce_imm_match, rituximab, prod_type, cyto_score_detail, conditioning_intensity, ethnicity, obesity, mrd_hct, in_vivo_tcd, tce_match, hepatic_severe, prior_tumor, peptic_ulcer, gvhd_proph, rheum_issue, sex_match, race_group, hepatic_mild, tce_div_match, donor_related, melphalan_dose, cardiac, pulm_moderate, "
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "# print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \", end=\"\")\n",
    "for c in FEATURES:\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \", end=\"\")\n",
    "        combined[c], _ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype == \"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype == \"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "\n",
    "train = combined.iloc[: len(train)].copy()\n",
    "test = combined.iloc[len(train) :].reset_index(drop=True).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Training functions\n",
    "# ====================================================\n",
    "def lightgbm_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    model = LGBMRegressor(\n",
    "        **CFG.regression_lgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=CFG.early_stopping_round),\n",
    "            lgb.log_evaluation(CFG.verbose),\n",
    "        ],\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def xgboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_cox_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_cox_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, features, method, target_col, fold):\n",
    "    \"\"\"特徴量の重要度をプロットする関数\"\"\"\n",
    "    # 各モデルタイプに応じた特徴量重要度の取得方法\n",
    "    if method == \"lightgbm\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.feature_importances_})\n",
    "    elif method == \"xgboost\" or method == \"xgboost_cox\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.feature_importances_})\n",
    "    elif method == \"catboost\" or method == \"catboost_cox\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.get_feature_importance()})\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance.sort_values(\"importance\", ascending=False).head(20), x=\"importance\", y=\"feature\")\n",
    "    plt.title(f\"{method} Feature Importance\\nTarget: {target_col}, Fold: {fold}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存先のディレクトリを作成\n",
    "    save_dir = CFG.OUTPUT_DIR / \"feature_importance\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_dir / f\"feature_importance_{method}_{target_col}_fold{fold}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gradient_boosting_model_cv_training(\n",
    "    method: str, train_df: pd.DataFrame, target_col_list: list, features: list, categorical_features: list\n",
    "):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    for target_col in target_col_list:\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{method} training fold {fold+1} {target_col}\")\n",
    "            x_train = train_df[train_df[\"fold\"] != fold + 1][features]\n",
    "            y_train = train_df[train_df[\"fold\"] != fold + 1][target_col]\n",
    "            x_valid = train_df[train_df[\"fold\"] == fold + 1][features]\n",
    "            y_valid = train_df[train_df[\"fold\"] == fold + 1][target_col]\n",
    "\n",
    "            if method == \"lightgbm\":\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            elif method == \"xgboost\":\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost\":\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            # Cox models\n",
    "            elif method == \"xgboost_cox\":\n",
    "                model, valid_pred = xgboost_cox_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost_cox\":\n",
    "                model, valid_pred = catboost_cox_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "            # Feature Importanceの可視化(最後のfoldのみ)\n",
    "            if fold == CFG.n_folds - 1:\n",
    "                plot_feature_importance(model, features, method, target_col, fold + 1)\n",
    "\n",
    "            # Save best model\n",
    "            save_model_path = (\n",
    "                CFG.MODEL_PATH / f\"{method}_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\"\n",
    "            )\n",
    "            save_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            pickle.dump(\n",
    "                model,\n",
    "                open(\n",
    "                    save_model_path,\n",
    "                    \"wb\",\n",
    "                ),\n",
    "            )\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[train_df[\"fold\"] == fold + 1] = valid_pred\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_predictions_df = pd.DataFrame()\n",
    "        oof_predictions_df[\"ID\"] = train_df[\"ID\"].values\n",
    "        oof_predictions_df[\"prediction\"] = oof_predictions\n",
    "        oof_predictions_df.to_csv(\n",
    "            CFG.OUTPUT_DIR / f\"oof_{method}_{target_col}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\", index=False\n",
    "        )\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        y_true = train_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        m = score(y_true.copy(), oof_predictions_df.copy(), \"ID\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{method} our out of folds CV score is {m}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_kaplan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6041\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.606188\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.024454\n",
      "[1000]\tvalid_0's l2: 0.0242283\n",
      "[1500]\tvalid_0's l2: 0.0242\n",
      "[2000]\tvalid_0's l2: 0.0242091\n",
      "Early stopping, best iteration is:\n",
      "[1625]\tvalid_0's l2: 0.0241565\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6047\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.606660\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.024005\n",
      "[1000]\tvalid_0's l2: 0.0238209\n",
      "[1500]\tvalid_0's l2: 0.0238028\n",
      "[2000]\tvalid_0's l2: 0.0237626\n",
      "[2500]\tvalid_0's l2: 0.0237949\n",
      "Early stopping, best iteration is:\n",
      "[2002]\tvalid_0's l2: 0.0237619\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6045\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.606506\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0238524\n",
      "[1000]\tvalid_0's l2: 0.0236114\n",
      "[1500]\tvalid_0's l2: 0.023578\n",
      "[2000]\tvalid_0's l2: 0.0235538\n",
      "Early stopping, best iteration is:\n",
      "[1947]\tvalid_0's l2: 0.0235316\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6050\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.606093\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246323\n",
      "[1000]\tvalid_0's l2: 0.0243341\n",
      "[1500]\tvalid_0's l2: 0.0242047\n",
      "[2000]\tvalid_0's l2: 0.024176\n",
      "Early stopping, best iteration is:\n",
      "[1851]\tvalid_0's l2: 0.0241665\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6047\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.606420\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0245464\n",
      "[1000]\tvalid_0's l2: 0.0243931\n",
      "[1500]\tvalid_0's l2: 0.0244175\n",
      "[2000]\tvalid_0's l2: 0.024416\n",
      "Early stopping, best iteration is:\n",
      "[1615]\tvalid_0's l2: 0.0243777\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6042\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.606450\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0241424\n",
      "[1000]\tvalid_0's l2: 0.0239727\n",
      "[1500]\tvalid_0's l2: 0.0240012\n",
      "Early stopping, best iteration is:\n",
      "[1161]\tvalid_0's l2: 0.0239499\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6046\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.605974\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0247667\n",
      "[1000]\tvalid_0's l2: 0.0244544\n",
      "[1500]\tvalid_0's l2: 0.0244039\n",
      "Early stopping, best iteration is:\n",
      "[1457]\tvalid_0's l2: 0.0243947\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6043\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.605772\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246456\n",
      "[1000]\tvalid_0's l2: 0.024262\n",
      "[1500]\tvalid_0's l2: 0.0241932\n",
      "[2000]\tvalid_0's l2: 0.0240776\n",
      "[2500]\tvalid_0's l2: 0.0240551\n",
      "Early stopping, best iteration is:\n",
      "[2093]\tvalid_0's l2: 0.0240258\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6041\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.606044\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246666\n",
      "[1000]\tvalid_0's l2: 0.024414\n",
      "[1500]\tvalid_0's l2: 0.0243114\n",
      "Early stopping, best iteration is:\n",
      "[1406]\tvalid_0's l2: 0.0242934\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6039\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score 0.605780\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0245817\n",
      "[1000]\tvalid_0's l2: 0.0242164\n",
      "[1500]\tvalid_0's l2: 0.0240956\n",
      "[2000]\tvalid_0's l2: 0.0240736\n",
      "Early stopping, best iteration is:\n",
      "[1787]\tvalid_0's l2: 0.024059\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.6745052918286317\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6041\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.539329\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0585351\n",
      "[1000]\tvalid_0's l2: 0.0580189\n",
      "[1500]\tvalid_0's l2: 0.0578958\n",
      "[2000]\tvalid_0's l2: 0.0580018\n",
      "Early stopping, best iteration is:\n",
      "[1589]\tvalid_0's l2: 0.0578276\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6047\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.538669\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0576629\n",
      "[1000]\tvalid_0's l2: 0.0573664\n",
      "[1500]\tvalid_0's l2: 0.0572677\n",
      "[2000]\tvalid_0's l2: 0.0571641\n",
      "[2500]\tvalid_0's l2: 0.0573449\n",
      "Early stopping, best iteration is:\n",
      "[2001]\tvalid_0's l2: 0.0571639\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6045\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.538856\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0565643\n",
      "[1000]\tvalid_0's l2: 0.0558398\n",
      "[1500]\tvalid_0's l2: 0.0557632\n",
      "[2000]\tvalid_0's l2: 0.0557705\n",
      "Early stopping, best iteration is:\n",
      "[1874]\tvalid_0's l2: 0.0557165\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6050\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.539416\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0586127\n",
      "[1000]\tvalid_0's l2: 0.0580213\n",
      "[1500]\tvalid_0's l2: 0.0576674\n",
      "[2000]\tvalid_0's l2: 0.0574644\n",
      "[2500]\tvalid_0's l2: 0.0574477\n",
      "Early stopping, best iteration is:\n",
      "[2126]\tvalid_0's l2: 0.0574031\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6047\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.539032\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0584436\n",
      "[1000]\tvalid_0's l2: 0.058028\n",
      "[1500]\tvalid_0's l2: 0.0579919\n",
      "[2000]\tvalid_0's l2: 0.0580162\n",
      "Early stopping, best iteration is:\n",
      "[1614]\tvalid_0's l2: 0.0579099\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6042\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.538973\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0579521\n",
      "[1000]\tvalid_0's l2: 0.0576038\n",
      "[1500]\tvalid_0's l2: 0.0574948\n",
      "Early stopping, best iteration is:\n",
      "[1141]\tvalid_0's l2: 0.0574229\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6046\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.539673\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0592671\n",
      "[1000]\tvalid_0's l2: 0.0584756\n",
      "[1500]\tvalid_0's l2: 0.0582588\n",
      "Early stopping, best iteration is:\n",
      "[1467]\tvalid_0's l2: 0.0582227\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6043\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.539898\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0586258\n",
      "[1000]\tvalid_0's l2: 0.0578255\n",
      "[1500]\tvalid_0's l2: 0.0574411\n",
      "[2000]\tvalid_0's l2: 0.0573947\n",
      "Early stopping, best iteration is:\n",
      "[1931]\tvalid_0's l2: 0.0573547\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6041\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.539493\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0588231\n",
      "[1000]\tvalid_0's l2: 0.0584971\n",
      "[1500]\tvalid_0's l2: 0.058429\n",
      "[2000]\tvalid_0's l2: 0.0584435\n",
      "Early stopping, best iteration is:\n",
      "[1815]\tvalid_0's l2: 0.0583596\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 6039\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 164\n",
      "[LightGBM] [Info] Start training from score -0.539971\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0584435\n",
      "[1000]\tvalid_0's l2: 0.0577273\n",
      "[1500]\tvalid_0's l2: 0.0574589\n",
      "[2000]\tvalid_0's l2: 0.0574335\n",
      "[2500]\tvalid_0's l2: 0.0574232\n",
      "[3000]\tvalid_0's l2: 0.0573138\n",
      "[3500]\tvalid_0's l2: 0.057328\n",
      "Early stopping, best iteration is:\n",
      "[3133]\tvalid_0's l2: 0.0572628\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.6768457334263172\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17640\n",
      "[500]\tvalidation_0-rmse:0.15729\n",
      "[1000]\tvalidation_0-rmse:0.15625\n",
      "[1500]\tvalidation_0-rmse:0.15604\n",
      "[1924]\tvalidation_0-rmse:0.15608\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17319\n",
      "[500]\tvalidation_0-rmse:0.15528\n",
      "[1000]\tvalidation_0-rmse:0.15424\n",
      "[1500]\tvalidation_0-rmse:0.15370\n",
      "[2000]\tvalidation_0-rmse:0.15347\n",
      "[2500]\tvalidation_0-rmse:0.15353\n",
      "[2651]\tvalidation_0-rmse:0.15351\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17500\n",
      "[500]\tvalidation_0-rmse:0.15481\n",
      "[1000]\tvalidation_0-rmse:0.15369\n",
      "[1500]\tvalidation_0-rmse:0.15334\n",
      "[2000]\tvalidation_0-rmse:0.15314\n",
      "[2500]\tvalidation_0-rmse:0.15301\n",
      "[2929]\tvalidation_0-rmse:0.15310\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17812\n",
      "[500]\tvalidation_0-rmse:0.15776\n",
      "[1000]\tvalidation_0-rmse:0.15642\n",
      "[1500]\tvalidation_0-rmse:0.15604\n",
      "[2000]\tvalidation_0-rmse:0.15586\n",
      "[2398]\tvalidation_0-rmse:0.15589\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17428\n",
      "[500]\tvalidation_0-rmse:0.15706\n",
      "[1000]\tvalidation_0-rmse:0.15622\n",
      "[1500]\tvalidation_0-rmse:0.15591\n",
      "[2000]\tvalidation_0-rmse:0.15590\n",
      "[2500]\tvalidation_0-rmse:0.15595\n",
      "[2756]\tvalidation_0-rmse:0.15600\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17420\n",
      "[500]\tvalidation_0-rmse:0.15564\n",
      "[1000]\tvalidation_0-rmse:0.15465\n",
      "[1500]\tvalidation_0-rmse:0.15447\n",
      "[2000]\tvalidation_0-rmse:0.15416\n",
      "[2500]\tvalidation_0-rmse:0.15429\n",
      "[2578]\tvalidation_0-rmse:0.15432\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17675\n",
      "[500]\tvalidation_0-rmse:0.15864\n",
      "[1000]\tvalidation_0-rmse:0.15740\n",
      "[1500]\tvalidation_0-rmse:0.15720\n",
      "[2000]\tvalidation_0-rmse:0.15739\n",
      "[2102]\tvalidation_0-rmse:0.15741\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17939\n",
      "[500]\tvalidation_0-rmse:0.15753\n",
      "[1000]\tvalidation_0-rmse:0.15602\n",
      "[1500]\tvalidation_0-rmse:0.15572\n",
      "[1890]\tvalidation_0-rmse:0.15573\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17829\n",
      "[500]\tvalidation_0-rmse:0.15798\n",
      "[1000]\tvalidation_0-rmse:0.15677\n",
      "[1500]\tvalidation_0-rmse:0.15641\n",
      "[2000]\tvalidation_0-rmse:0.15623\n",
      "[2500]\tvalidation_0-rmse:0.15607\n",
      "[3000]\tvalidation_0-rmse:0.15599\n",
      "[3500]\tvalidation_0-rmse:0.15600\n",
      "[3802]\tvalidation_0-rmse:0.15609\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17744\n",
      "[500]\tvalidation_0-rmse:0.15756\n",
      "[1000]\tvalidation_0-rmse:0.15622\n",
      "[1500]\tvalidation_0-rmse:0.15616\n",
      "[1673]\tvalidation_0-rmse:0.15598\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6737829608594365\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27152\n",
      "[500]\tvalidation_0-rmse:0.24286\n",
      "[1000]\tvalidation_0-rmse:0.24114\n",
      "[1500]\tvalidation_0-rmse:0.24087\n",
      "[2000]\tvalidation_0-rmse:0.24079\n",
      "[2138]\tvalidation_0-rmse:0.24086\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26742\n",
      "[500]\tvalidation_0-rmse:0.24077\n",
      "[1000]\tvalidation_0-rmse:0.23951\n",
      "[1500]\tvalidation_0-rmse:0.23872\n",
      "[2000]\tvalidation_0-rmse:0.23852\n",
      "[2500]\tvalidation_0-rmse:0.23868\n",
      "[2588]\tvalidation_0-rmse:0.23875\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26941\n",
      "[500]\tvalidation_0-rmse:0.23863\n",
      "[1000]\tvalidation_0-rmse:0.23707\n",
      "[1500]\tvalidation_0-rmse:0.23670\n",
      "[2000]\tvalidation_0-rmse:0.23635\n",
      "[2500]\tvalidation_0-rmse:0.23631\n",
      "[3000]\tvalidation_0-rmse:0.23643\n",
      "[3037]\tvalidation_0-rmse:0.23641\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27371\n",
      "[500]\tvalidation_0-rmse:0.24320\n",
      "[1000]\tvalidation_0-rmse:0.24130\n",
      "[1500]\tvalidation_0-rmse:0.24058\n",
      "[2000]\tvalidation_0-rmse:0.24037\n",
      "[2500]\tvalidation_0-rmse:0.24020\n",
      "[2910]\tvalidation_0-rmse:0.24022\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26872\n",
      "[500]\tvalidation_0-rmse:0.24271\n",
      "[1000]\tvalidation_0-rmse:0.24171\n",
      "[1500]\tvalidation_0-rmse:0.24135\n",
      "[2000]\tvalidation_0-rmse:0.24123\n",
      "[2500]\tvalidation_0-rmse:0.24137\n",
      "[2769]\tvalidation_0-rmse:0.24145\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26899\n",
      "[500]\tvalidation_0-rmse:0.24123\n",
      "[1000]\tvalidation_0-rmse:0.23979\n",
      "[1500]\tvalidation_0-rmse:0.23948\n",
      "[2000]\tvalidation_0-rmse:0.23916\n",
      "[2500]\tvalidation_0-rmse:0.23913\n",
      "[2754]\tvalidation_0-rmse:0.23931\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27211\n",
      "[500]\tvalidation_0-rmse:0.24497\n",
      "[1000]\tvalidation_0-rmse:0.24301\n",
      "[1500]\tvalidation_0-rmse:0.24259\n",
      "[2000]\tvalidation_0-rmse:0.24263\n",
      "[2136]\tvalidation_0-rmse:0.24270\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27538\n",
      "[500]\tvalidation_0-rmse:0.24248\n",
      "[1000]\tvalidation_0-rmse:0.24049\n",
      "[1500]\tvalidation_0-rmse:0.23997\n",
      "[2000]\tvalidation_0-rmse:0.24006\n",
      "[2303]\tvalidation_0-rmse:0.24005\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27403\n",
      "[500]\tvalidation_0-rmse:0.24318\n",
      "[1000]\tvalidation_0-rmse:0.24144\n",
      "[1500]\tvalidation_0-rmse:0.24068\n",
      "[2000]\tvalidation_0-rmse:0.24059\n",
      "[2339]\tvalidation_0-rmse:0.24057\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27297\n",
      "[500]\tvalidation_0-rmse:0.24307\n",
      "[1000]\tvalidation_0-rmse:0.24092\n",
      "[1500]\tvalidation_0-rmse:0.24037\n",
      "[1742]\tvalidation_0-rmse:0.24059\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6761928656021032\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_kaplan\n",
      "0:\tlearn: 0.1762454\ttest: 0.1763725\tbest: 0.1763725 (0)\ttotal: 34.3ms\tremaining: 9h 32m 20s\n",
      "500:\tlearn: 0.1485731\ttest: 0.1580807\tbest: 0.1580807 (500)\ttotal: 5.69s\tremaining: 3h 9m 12s\n",
      "1000:\tlearn: 0.1417767\ttest: 0.1568816\tbest: 0.1568750 (988)\ttotal: 11.9s\tremaining: 3h 17m 24s\n",
      "1500:\tlearn: 0.1363419\ttest: 0.1563574\tbest: 0.1563569 (1497)\ttotal: 17.8s\tremaining: 3h 17m 1s\n",
      "2000:\tlearn: 0.1317155\ttest: 0.1562447\tbest: 0.1562402 (1999)\ttotal: 23.3s\tremaining: 3h 13m 33s\n",
      "2500:\tlearn: 0.1273911\ttest: 0.1561356\tbest: 0.1560979 (2359)\ttotal: 28.9s\tremaining: 3h 12m 19s\n",
      "3000:\tlearn: 0.1235069\ttest: 0.1561111\tbest: 0.1560039 (2783)\ttotal: 34.8s\tremaining: 3h 12m 27s\n",
      "bestTest = 0.1560038854\n",
      "bestIteration = 2783\n",
      "Shrink model to first 2784 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_kaplan\n",
      "0:\tlearn: 0.1765895\ttest: 0.1731789\tbest: 0.1731789 (0)\ttotal: 10.3ms\tremaining: 2h 52m 26s\n",
      "500:\tlearn: 0.1486443\ttest: 0.1562001\tbest: 0.1562001 (500)\ttotal: 5.8s\tremaining: 3h 12m 58s\n",
      "1000:\tlearn: 0.1417551\ttest: 0.1553866\tbest: 0.1553546 (975)\ttotal: 12.2s\tremaining: 3h 23m 17s\n",
      "1500:\tlearn: 0.1365561\ttest: 0.1550551\tbest: 0.1550502 (1497)\ttotal: 18.2s\tremaining: 3h 21m 17s\n",
      "2000:\tlearn: 0.1320172\ttest: 0.1548626\tbest: 0.1548495 (1940)\ttotal: 24.3s\tremaining: 3h 21m 37s\n",
      "2500:\tlearn: 0.1279337\ttest: 0.1547840\tbest: 0.1547679 (2080)\ttotal: 30s\tremaining: 3h 19m 44s\n",
      "bestTest = 0.1547678952\n",
      "bestIteration = 2080\n",
      "Shrink model to first 2081 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_kaplan\n",
      "0:\tlearn: 0.1764040\ttest: 0.1749981\tbest: 0.1749981 (0)\ttotal: 9.26ms\tremaining: 2h 34m 23s\n",
      "500:\tlearn: 0.1487988\ttest: 0.1559899\tbest: 0.1559899 (500)\ttotal: 6.21s\tremaining: 3h 26m 20s\n",
      "1000:\tlearn: 0.1417902\ttest: 0.1545983\tbest: 0.1545965 (998)\ttotal: 12.7s\tremaining: 3h 30m 39s\n",
      "1500:\tlearn: 0.1364615\ttest: 0.1540085\tbest: 0.1540052 (1482)\ttotal: 19s\tremaining: 3h 30m 57s\n",
      "2000:\tlearn: 0.1319197\ttest: 0.1537764\tbest: 0.1537729 (1908)\ttotal: 25s\tremaining: 3h 27m 26s\n",
      "2500:\tlearn: 0.1277652\ttest: 0.1536795\tbest: 0.1536399 (2464)\ttotal: 31.2s\tremaining: 3h 27m 11s\n",
      "3000:\tlearn: 0.1239896\ttest: 0.1534406\tbest: 0.1534406 (3000)\ttotal: 36.9s\tremaining: 3h 24m 8s\n",
      "3500:\tlearn: 0.1204222\ttest: 0.1534061\tbest: 0.1533796 (3491)\ttotal: 42.8s\tremaining: 3h 22m 51s\n",
      "4000:\tlearn: 0.1170936\ttest: 0.1533162\tbest: 0.1533114 (3997)\ttotal: 48.8s\tremaining: 3h 22m 31s\n",
      "4500:\tlearn: 0.1140117\ttest: 0.1532831\tbest: 0.1532332 (4341)\ttotal: 55s\tremaining: 3h 22m 35s\n",
      "5000:\tlearn: 0.1110343\ttest: 0.1532688\tbest: 0.1532066 (4918)\ttotal: 1m\tremaining: 3h 22m 13s\n",
      "5500:\tlearn: 0.1082193\ttest: 0.1532119\tbest: 0.1531915 (5350)\ttotal: 1m 7s\tremaining: 3h 23m 19s\n",
      "6000:\tlearn: 0.1056137\ttest: 0.1531712\tbest: 0.1531511 (5780)\ttotal: 1m 13s\tremaining: 3h 22m 54s\n",
      "bestTest = 0.1531511263\n",
      "bestIteration = 5780\n",
      "Shrink model to first 5781 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_kaplan\n",
      "0:\tlearn: 0.1760524\ttest: 0.1781192\tbest: 0.1781192 (0)\ttotal: 11.4ms\tremaining: 3h 10m 11s\n",
      "500:\tlearn: 0.1483971\ttest: 0.1584802\tbest: 0.1584802 (500)\ttotal: 5.6s\tremaining: 3h 6m 15s\n",
      "1000:\tlearn: 0.1413895\ttest: 0.1569787\tbest: 0.1569763 (998)\ttotal: 11.6s\tremaining: 3h 12m 54s\n",
      "1500:\tlearn: 0.1360196\ttest: 0.1562931\tbest: 0.1562930 (1498)\ttotal: 17.5s\tremaining: 3h 13m 58s\n",
      "2000:\tlearn: 0.1313325\ttest: 0.1560040\tbest: 0.1559957 (1974)\ttotal: 23.7s\tremaining: 3h 17m 1s\n",
      "2500:\tlearn: 0.1272704\ttest: 0.1557996\tbest: 0.1557996 (2500)\ttotal: 29.9s\tremaining: 3h 18m 37s\n",
      "3000:\tlearn: 0.1233633\ttest: 0.1557106\tbest: 0.1556920 (2797)\ttotal: 36.5s\tremaining: 3h 22m 15s\n",
      "3500:\tlearn: 0.1196889\ttest: 0.1555730\tbest: 0.1555676 (3433)\ttotal: 43.1s\tremaining: 3h 24m 35s\n",
      "4000:\tlearn: 0.1163444\ttest: 0.1554981\tbest: 0.1554578 (3791)\ttotal: 49.7s\tremaining: 3h 26m 17s\n",
      "bestTest = 0.155457841\n",
      "bestIteration = 3791\n",
      "Shrink model to first 3792 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_kaplan\n",
      "0:\tlearn: 0.1764646\ttest: 0.1742538\tbest: 0.1742538 (0)\ttotal: 11.4ms\tremaining: 3h 10m 42s\n",
      "500:\tlearn: 0.1484121\ttest: 0.1576394\tbest: 0.1576349 (497)\ttotal: 5.69s\tremaining: 3h 9m 5s\n",
      "1000:\tlearn: 0.1416051\ttest: 0.1568056\tbest: 0.1568056 (1000)\ttotal: 12.2s\tremaining: 3h 23m 23s\n",
      "1500:\tlearn: 0.1360381\ttest: 0.1566352\tbest: 0.1566319 (1498)\ttotal: 18.4s\tremaining: 3h 23m 45s\n",
      "2000:\tlearn: 0.1313261\ttest: 0.1565324\tbest: 0.1564951 (1868)\ttotal: 24.7s\tremaining: 3h 25m 13s\n",
      "bestTest = 0.1564951436\n",
      "bestIteration = 1868\n",
      "Shrink model to first 1869 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_kaplan\n",
      "0:\tlearn: 0.1764847\ttest: 0.1741722\tbest: 0.1741722 (0)\ttotal: 11.1ms\tremaining: 3h 4m 57s\n",
      "500:\tlearn: 0.1487479\ttest: 0.1564736\tbest: 0.1564736 (500)\ttotal: 5.94s\tremaining: 3h 17m 36s\n",
      "1000:\tlearn: 0.1417437\ttest: 0.1555572\tbest: 0.1555467 (996)\ttotal: 12.1s\tremaining: 3h 21m 56s\n",
      "1500:\tlearn: 0.1363292\ttest: 0.1551689\tbest: 0.1551670 (1497)\ttotal: 18.5s\tremaining: 3h 24m 54s\n",
      "2000:\tlearn: 0.1316167\ttest: 0.1550649\tbest: 0.1550416 (1830)\ttotal: 25.1s\tremaining: 3h 28m 58s\n",
      "2500:\tlearn: 0.1274903\ttest: 0.1550407\tbest: 0.1550143 (2134)\ttotal: 31.2s\tremaining: 3h 27m 36s\n",
      "bestTest = 0.1550142632\n",
      "bestIteration = 2134\n",
      "Shrink model to first 2135 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_kaplan\n",
      "0:\tlearn: 0.1762022\ttest: 0.1767325\tbest: 0.1767325 (0)\ttotal: 11.4ms\tremaining: 3h 9m 48s\n",
      "500:\tlearn: 0.1484810\ttest: 0.1590727\tbest: 0.1590726 (499)\ttotal: 5.75s\tremaining: 3h 11m 6s\n",
      "1000:\tlearn: 0.1413805\ttest: 0.1578452\tbest: 0.1578360 (991)\ttotal: 11.9s\tremaining: 3h 17m 53s\n",
      "1500:\tlearn: 0.1360320\ttest: 0.1574454\tbest: 0.1574348 (1470)\ttotal: 17.5s\tremaining: 3h 14m 25s\n",
      "2000:\tlearn: 0.1312953\ttest: 0.1571927\tbest: 0.1571783 (1984)\ttotal: 23.5s\tremaining: 3h 14m 57s\n",
      "2500:\tlearn: 0.1270495\ttest: 0.1572025\tbest: 0.1571722 (2298)\ttotal: 29.1s\tremaining: 3h 13m 42s\n",
      "3000:\tlearn: 0.1230497\ttest: 0.1568946\tbest: 0.1568867 (2993)\ttotal: 35.5s\tremaining: 3h 16m 47s\n",
      "3500:\tlearn: 0.1193427\ttest: 0.1569768\tbest: 0.1568591 (3074)\ttotal: 42.1s\tremaining: 3h 19m 31s\n",
      "bestTest = 0.1568591134\n",
      "bestIteration = 3074\n",
      "Shrink model to first 3075 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_kaplan\n",
      "0:\tlearn: 0.1759050\ttest: 0.1793786\tbest: 0.1793786 (0)\ttotal: 8.65ms\tremaining: 2h 24m 14s\n",
      "500:\tlearn: 0.1487460\ttest: 0.1585496\tbest: 0.1585496 (500)\ttotal: 5.62s\tremaining: 3h 6m 59s\n",
      "1000:\tlearn: 0.1417087\ttest: 0.1569796\tbest: 0.1569796 (1000)\ttotal: 12.2s\tremaining: 3h 22m 50s\n",
      "1500:\tlearn: 0.1364392\ttest: 0.1563497\tbest: 0.1563497 (1500)\ttotal: 18.1s\tremaining: 3h 20m 21s\n",
      "2000:\tlearn: 0.1316606\ttest: 0.1558832\tbest: 0.1558832 (2000)\ttotal: 24.8s\tremaining: 3h 26m 32s\n",
      "2500:\tlearn: 0.1274550\ttest: 0.1556507\tbest: 0.1556473 (2487)\ttotal: 31.2s\tremaining: 3h 27m 43s\n",
      "3000:\tlearn: 0.1235193\ttest: 0.1553678\tbest: 0.1553488 (2975)\ttotal: 37.8s\tremaining: 3h 29m 32s\n",
      "3500:\tlearn: 0.1198604\ttest: 0.1553027\tbest: 0.1552892 (3483)\ttotal: 44.4s\tremaining: 3h 30m 41s\n",
      "4000:\tlearn: 0.1164986\ttest: 0.1552438\tbest: 0.1552361 (3940)\ttotal: 51.2s\tremaining: 3h 32m 30s\n",
      "4500:\tlearn: 0.1133108\ttest: 0.1552362\tbest: 0.1551719 (4325)\ttotal: 57.8s\tremaining: 3h 33m\n",
      "bestTest = 0.1551719094\n",
      "bestIteration = 4325\n",
      "Shrink model to first 4326 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_kaplan\n",
      "0:\tlearn: 0.1760565\ttest: 0.1781289\tbest: 0.1781289 (0)\ttotal: 11.5ms\tremaining: 3h 11m 48s\n",
      "500:\tlearn: 0.1485033\ttest: 0.1586361\tbest: 0.1586316 (499)\ttotal: 5.61s\tremaining: 3h 6m 29s\n",
      "1000:\tlearn: 0.1416247\ttest: 0.1573182\tbest: 0.1573177 (973)\ttotal: 10.8s\tremaining: 3h 12s\n",
      "1500:\tlearn: 0.1362109\ttest: 0.1567373\tbest: 0.1567373 (1500)\ttotal: 16.3s\tremaining: 3h 21s\n",
      "2000:\tlearn: 0.1316680\ttest: 0.1564213\tbest: 0.1564038 (1989)\ttotal: 21.8s\tremaining: 3h 55s\n",
      "2500:\tlearn: 0.1275107\ttest: 0.1562647\tbest: 0.1562580 (2495)\ttotal: 27.5s\tremaining: 3h 2m 43s\n",
      "3000:\tlearn: 0.1235357\ttest: 0.1561856\tbest: 0.1561517 (2890)\ttotal: 33s\tremaining: 3h 2m 44s\n",
      "3500:\tlearn: 0.1199574\ttest: 0.1561141\tbest: 0.1560667 (3453)\ttotal: 38.8s\tremaining: 3h 4m 14s\n",
      "4000:\tlearn: 0.1165827\ttest: 0.1560710\tbest: 0.1560640 (3847)\ttotal: 45s\tremaining: 3h 6m 48s\n",
      "4500:\tlearn: 0.1133892\ttest: 0.1560004\tbest: 0.1560004 (4500)\ttotal: 50.8s\tremaining: 3h 7m 5s\n",
      "5000:\tlearn: 0.1103789\ttest: 0.1558985\tbest: 0.1558873 (4919)\ttotal: 57.2s\tremaining: 3h 9m 37s\n",
      "5500:\tlearn: 0.1074953\ttest: 0.1559004\tbest: 0.1558792 (5132)\ttotal: 1m 3s\tremaining: 3h 10m 14s\n",
      "bestTest = 0.1558791613\n",
      "bestIteration = 5132\n",
      "Shrink model to first 5133 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_kaplan\n",
      "0:\tlearn: 0.1761224\ttest: 0.1774221\tbest: 0.1774221 (0)\ttotal: 10.3ms\tremaining: 2h 51m 42s\n",
      "500:\tlearn: 0.1486608\ttest: 0.1587240\tbest: 0.1587220 (499)\ttotal: 5.34s\tremaining: 2h 57m 32s\n",
      "1000:\tlearn: 0.1417773\ttest: 0.1573590\tbest: 0.1573588 (999)\ttotal: 11.1s\tremaining: 3h 3m 48s\n",
      "1500:\tlearn: 0.1366076\ttest: 0.1567309\tbest: 0.1567264 (1486)\ttotal: 16.5s\tremaining: 3h 2m 51s\n",
      "2000:\tlearn: 0.1318636\ttest: 0.1562385\tbest: 0.1562385 (2000)\ttotal: 22.4s\tremaining: 3h 6m 25s\n",
      "2500:\tlearn: 0.1276752\ttest: 0.1558820\tbest: 0.1558820 (2500)\ttotal: 28.3s\tremaining: 3h 8m 16s\n",
      "3000:\tlearn: 0.1238616\ttest: 0.1556203\tbest: 0.1556203 (3000)\ttotal: 34.2s\tremaining: 3h 9m 25s\n",
      "3500:\tlearn: 0.1202868\ttest: 0.1555074\tbest: 0.1555022 (3476)\ttotal: 40s\tremaining: 3h 9m 53s\n",
      "4000:\tlearn: 0.1168682\ttest: 0.1553580\tbest: 0.1553499 (3975)\ttotal: 45.6s\tremaining: 3h 9m 16s\n",
      "4500:\tlearn: 0.1137765\ttest: 0.1552710\tbest: 0.1552551 (4489)\ttotal: 51.1s\tremaining: 3h 8m 30s\n",
      "5000:\tlearn: 0.1108330\ttest: 0.1552332\tbest: 0.1552209 (4974)\ttotal: 56.9s\tremaining: 3h 8m 39s\n",
      "5500:\tlearn: 0.1080108\ttest: 0.1552922\tbest: 0.1551991 (5150)\ttotal: 1m 3s\tremaining: 3h 11m 3s\n",
      "bestTest = 0.1551990515\n",
      "bestIteration = 5150\n",
      "Shrink model to first 5151 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6740959365447844\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_nelson\n",
      "0:\tlearn: 0.2713285\ttest: 0.2715086\tbest: 0.2715086 (0)\ttotal: 12.2ms\tremaining: 3h 24m 3s\n",
      "500:\tlearn: 0.2299652\ttest: 0.2440152\tbest: 0.2440152 (500)\ttotal: 5.92s\tremaining: 3h 16m 46s\n",
      "1000:\tlearn: 0.2197358\ttest: 0.2421652\tbest: 0.2421629 (995)\ttotal: 12.1s\tremaining: 3h 21m 26s\n",
      "1500:\tlearn: 0.2116189\ttest: 0.2412964\tbest: 0.2412964 (1500)\ttotal: 18s\tremaining: 3h 19m 52s\n",
      "2000:\tlearn: 0.2045436\ttest: 0.2410664\tbest: 0.2409988 (1989)\ttotal: 23.8s\tremaining: 3h 17m 25s\n",
      "2500:\tlearn: 0.1980931\ttest: 0.2409364\tbest: 0.2408980 (2362)\ttotal: 29.7s\tremaining: 3h 17m 42s\n",
      "3000:\tlearn: 0.1922836\ttest: 0.2409527\tbest: 0.2408577 (2605)\ttotal: 35.5s\tremaining: 3h 16m 22s\n",
      "bestTest = 0.2408576908\n",
      "bestIteration = 2605\n",
      "Shrink model to first 2606 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_nelson\n",
      "0:\tlearn: 0.2717744\ttest: 0.2673604\tbest: 0.2673604 (0)\ttotal: 8.75ms\tremaining: 2h 25m 46s\n",
      "500:\tlearn: 0.2297886\ttest: 0.2418371\tbest: 0.2418371 (500)\ttotal: 5.53s\tremaining: 3h 3m 59s\n",
      "1000:\tlearn: 0.2195737\ttest: 0.2407075\tbest: 0.2406867 (976)\ttotal: 11.5s\tremaining: 3h 10m 29s\n",
      "1500:\tlearn: 0.2118203\ttest: 0.2401561\tbest: 0.2401561 (1500)\ttotal: 17.4s\tremaining: 3h 12m 44s\n",
      "2000:\tlearn: 0.2050019\ttest: 0.2399607\tbest: 0.2399242 (1972)\ttotal: 23.2s\tremaining: 3h 13m 6s\n",
      "2500:\tlearn: 0.1988618\ttest: 0.2398719\tbest: 0.2398342 (2401)\ttotal: 29s\tremaining: 3h 13m 1s\n",
      "3000:\tlearn: 0.1931650\ttest: 0.2396421\tbest: 0.2396197 (2976)\ttotal: 34.9s\tremaining: 3h 13m 27s\n",
      "3500:\tlearn: 0.1879014\ttest: 0.2394947\tbest: 0.2394947 (3500)\ttotal: 40.9s\tremaining: 3h 14m 4s\n",
      "4000:\tlearn: 0.1830151\ttest: 0.2393788\tbest: 0.2393260 (3909)\ttotal: 46.8s\tremaining: 3h 14m 4s\n",
      "bestTest = 0.2393259739\n",
      "bestIteration = 3909\n",
      "Shrink model to first 3910 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_nelson\n",
      "0:\tlearn: 0.2715679\ttest: 0.2693906\tbest: 0.2693906 (0)\ttotal: 9.85ms\tremaining: 2h 44m 6s\n",
      "500:\tlearn: 0.2301595\ttest: 0.2402516\tbest: 0.2402516 (500)\ttotal: 5.73s\tremaining: 3h 10m 35s\n",
      "1000:\tlearn: 0.2198223\ttest: 0.2381880\tbest: 0.2381854 (998)\ttotal: 11.9s\tremaining: 3h 17m 21s\n",
      "1500:\tlearn: 0.2118894\ttest: 0.2374647\tbest: 0.2374597 (1487)\ttotal: 17.6s\tremaining: 3h 14m 54s\n",
      "2000:\tlearn: 0.2050301\ttest: 0.2370761\tbest: 0.2370689 (1980)\ttotal: 23.3s\tremaining: 3h 13m 38s\n",
      "2500:\tlearn: 0.1987843\ttest: 0.2367593\tbest: 0.2367381 (2492)\ttotal: 29s\tremaining: 3h 12m 30s\n",
      "3000:\tlearn: 0.1930336\ttest: 0.2364339\tbest: 0.2364339 (3000)\ttotal: 34.5s\tremaining: 3h 10m 53s\n",
      "3500:\tlearn: 0.1876498\ttest: 0.2363342\tbest: 0.2362809 (3444)\ttotal: 40.2s\tremaining: 3h 10m 45s\n",
      "4000:\tlearn: 0.1826610\ttest: 0.2361930\tbest: 0.2361797 (3995)\ttotal: 45.9s\tremaining: 3h 10m 36s\n",
      "4500:\tlearn: 0.1779599\ttest: 0.2361051\tbest: 0.2360939 (4471)\ttotal: 51.7s\tremaining: 3h 10m 24s\n",
      "5000:\tlearn: 0.1735763\ttest: 0.2361227\tbest: 0.2360778 (4759)\ttotal: 57.1s\tremaining: 3h 9m 12s\n",
      "5500:\tlearn: 0.1692685\ttest: 0.2360868\tbest: 0.2360618 (5274)\ttotal: 1m 3s\tremaining: 3h 10m 22s\n",
      "6000:\tlearn: 0.1652256\ttest: 0.2360217\tbest: 0.2359973 (5929)\ttotal: 1m 9s\tremaining: 3h 11m 32s\n",
      "6500:\tlearn: 0.1613461\ttest: 0.2360781\tbest: 0.2359422 (6114)\ttotal: 1m 14s\tremaining: 3h 10m 50s\n",
      "bestTest = 0.2359422179\n",
      "bestIteration = 6114\n",
      "Shrink model to first 6115 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_nelson\n",
      "0:\tlearn: 0.2710992\ttest: 0.2736933\tbest: 0.2736933 (0)\ttotal: 12.4ms\tremaining: 3h 26m 30s\n",
      "500:\tlearn: 0.2296943\ttest: 0.2447017\tbest: 0.2447017 (500)\ttotal: 6.1s\tremaining: 3h 22m 42s\n",
      "1000:\tlearn: 0.2194004\ttest: 0.2427022\tbest: 0.2427022 (1000)\ttotal: 11.6s\tremaining: 3h 13m 36s\n",
      "1500:\tlearn: 0.2111920\ttest: 0.2416526\tbest: 0.2416481 (1498)\ttotal: 17s\tremaining: 3h 9m 1s\n",
      "2000:\tlearn: 0.2040955\ttest: 0.2411474\tbest: 0.2411186 (1977)\ttotal: 23.4s\tremaining: 3h 14m 11s\n",
      "2500:\tlearn: 0.1978390\ttest: 0.2408358\tbest: 0.2408358 (2500)\ttotal: 29.4s\tremaining: 3h 15m 13s\n",
      "3000:\tlearn: 0.1919634\ttest: 0.2407423\tbest: 0.2407230 (2994)\ttotal: 35.2s\tremaining: 3h 14m 56s\n",
      "3500:\tlearn: 0.1864595\ttest: 0.2406069\tbest: 0.2405774 (3437)\ttotal: 41.3s\tremaining: 3h 16m 7s\n",
      "4000:\tlearn: 0.1813459\ttest: 0.2405646\tbest: 0.2405398 (3840)\ttotal: 47.2s\tremaining: 3h 16m 1s\n",
      "4500:\tlearn: 0.1764986\ttest: 0.2404995\tbest: 0.2404368 (4289)\ttotal: 53.3s\tremaining: 3h 16m 24s\n",
      "bestTest = 0.240436836\n",
      "bestIteration = 4289\n",
      "Shrink model to first 4290 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_nelson\n",
      "0:\tlearn: 0.2716155\ttest: 0.2686842\tbest: 0.2686842 (0)\ttotal: 9.08ms\tremaining: 2h 31m 17s\n",
      "500:\tlearn: 0.2296641\ttest: 0.2431734\tbest: 0.2431669 (499)\ttotal: 5.41s\tremaining: 2h 59m 46s\n",
      "1000:\tlearn: 0.2193848\ttest: 0.2421264\tbest: 0.2421264 (1000)\ttotal: 11.6s\tremaining: 3h 13m 22s\n",
      "1500:\tlearn: 0.2110938\ttest: 0.2417903\tbest: 0.2417808 (1498)\ttotal: 17.7s\tremaining: 3h 15m 57s\n",
      "2000:\tlearn: 0.2039251\ttest: 0.2416294\tbest: 0.2416063 (1947)\ttotal: 24.1s\tremaining: 3h 20m 11s\n",
      "2500:\tlearn: 0.1973660\ttest: 0.2416278\tbest: 0.2415986 (2104)\ttotal: 30.4s\tremaining: 3h 21m 53s\n",
      "bestTest = 0.2415985932\n",
      "bestIteration = 2104\n",
      "Shrink model to first 2105 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_nelson\n",
      "0:\tlearn: 0.2715893\ttest: 0.2689516\tbest: 0.2689516 (0)\ttotal: 9.72ms\tremaining: 2h 42m 2s\n",
      "500:\tlearn: 0.2298842\ttest: 0.2424186\tbest: 0.2424186 (500)\ttotal: 5.36s\tremaining: 2h 58m 20s\n",
      "1000:\tlearn: 0.2196334\ttest: 0.2412017\tbest: 0.2411949 (996)\ttotal: 11.4s\tremaining: 3h 10m 21s\n",
      "1500:\tlearn: 0.2115996\ttest: 0.2405985\tbest: 0.2405888 (1496)\ttotal: 17.6s\tremaining: 3h 15m 26s\n",
      "2000:\tlearn: 0.2044462\ttest: 0.2403473\tbest: 0.2403473 (2000)\ttotal: 23.4s\tremaining: 3h 14m 11s\n",
      "2500:\tlearn: 0.1981794\ttest: 0.2403125\tbest: 0.2402704 (2208)\ttotal: 29.2s\tremaining: 3h 14m 14s\n",
      "3000:\tlearn: 0.1923237\ttest: 0.2402898\tbest: 0.2402618 (2553)\ttotal: 35.2s\tremaining: 3h 14m 51s\n",
      "3500:\tlearn: 0.1868820\ttest: 0.2402588\tbest: 0.2402231 (3077)\ttotal: 41.3s\tremaining: 3h 15m 44s\n",
      "bestTest = 0.2402231491\n",
      "bestIteration = 3077\n",
      "Shrink model to first 3078 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_nelson\n",
      "0:\tlearn: 0.2712515\ttest: 0.2720873\tbest: 0.2720873 (0)\ttotal: 9.41ms\tremaining: 2h 36m 54s\n",
      "500:\tlearn: 0.2296183\ttest: 0.2460158\tbest: 0.2460158 (500)\ttotal: 5.57s\tremaining: 3h 5m 9s\n",
      "1000:\tlearn: 0.2190638\ttest: 0.2441777\tbest: 0.2441692 (992)\ttotal: 11.5s\tremaining: 3h 11m 58s\n",
      "1500:\tlearn: 0.2110205\ttest: 0.2433788\tbest: 0.2433707 (1485)\ttotal: 17.7s\tremaining: 3h 16m 37s\n",
      "2000:\tlearn: 0.2039665\ttest: 0.2431939\tbest: 0.2431923 (1999)\ttotal: 23.9s\tremaining: 3h 18m 28s\n",
      "2500:\tlearn: 0.1974990\ttest: 0.2433198\tbest: 0.2431366 (2169)\ttotal: 30s\tremaining: 3h 19m 37s\n",
      "bestTest = 0.2431365747\n",
      "bestIteration = 2169\n",
      "Shrink model to first 2170 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_nelson\n",
      "0:\tlearn: 0.2709038\ttest: 0.2753733\tbest: 0.2753733 (0)\ttotal: 9.36ms\tremaining: 2h 36m 3s\n",
      "500:\tlearn: 0.2300159\ttest: 0.2440553\tbest: 0.2440553 (500)\ttotal: 5.87s\tremaining: 3h 15m 2s\n",
      "1000:\tlearn: 0.2196270\ttest: 0.2419232\tbest: 0.2419232 (1000)\ttotal: 12s\tremaining: 3h 19m 17s\n",
      "1500:\tlearn: 0.2115285\ttest: 0.2409042\tbest: 0.2409042 (1500)\ttotal: 17.8s\tremaining: 3h 16m 59s\n",
      "2000:\tlearn: 0.2043889\ttest: 0.2401547\tbest: 0.2401231 (1955)\ttotal: 23.7s\tremaining: 3h 16m 48s\n",
      "2500:\tlearn: 0.1980481\ttest: 0.2397429\tbest: 0.2397429 (2500)\ttotal: 29.6s\tremaining: 3h 16m 45s\n",
      "3000:\tlearn: 0.1921228\ttest: 0.2395687\tbest: 0.2395343 (2981)\ttotal: 35.9s\tremaining: 3h 18m 43s\n",
      "3500:\tlearn: 0.1866079\ttest: 0.2394672\tbest: 0.2394439 (3466)\ttotal: 42.2s\tremaining: 3h 20m 15s\n",
      "4000:\tlearn: 0.1816093\ttest: 0.2394478\tbest: 0.2393972 (3940)\ttotal: 48.5s\tremaining: 3h 21m 21s\n",
      "4500:\tlearn: 0.1767241\ttest: 0.2394731\tbest: 0.2393771 (4284)\ttotal: 54.7s\tremaining: 3h 21m 30s\n",
      "bestTest = 0.2393771404\n",
      "bestIteration = 4284\n",
      "Shrink model to first 4285 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_nelson\n",
      "0:\tlearn: 0.2710736\ttest: 0.2737473\tbest: 0.2737473 (0)\ttotal: 9.93ms\tremaining: 2h 45m 28s\n",
      "500:\tlearn: 0.2297181\ttest: 0.2442984\tbest: 0.2442984 (500)\ttotal: 5.83s\tremaining: 3h 13m 59s\n",
      "1000:\tlearn: 0.2193961\ttest: 0.2425219\tbest: 0.2425010 (972)\ttotal: 12.1s\tremaining: 3h 20m 32s\n",
      "1500:\tlearn: 0.2114473\ttest: 0.2416098\tbest: 0.2416086 (1499)\ttotal: 18.3s\tremaining: 3h 23m 1s\n",
      "2000:\tlearn: 0.2045484\ttest: 0.2412035\tbest: 0.2412028 (1999)\ttotal: 24.8s\tremaining: 3h 26m 3s\n",
      "2500:\tlearn: 0.1982618\ttest: 0.2409635\tbest: 0.2409359 (2470)\ttotal: 31.1s\tremaining: 3h 26m 40s\n",
      "3000:\tlearn: 0.1924005\ttest: 0.2406468\tbest: 0.2406383 (2964)\ttotal: 37.7s\tremaining: 3h 28m 57s\n",
      "3500:\tlearn: 0.1869479\ttest: 0.2404570\tbest: 0.2404430 (3495)\ttotal: 44s\tremaining: 3h 28m 30s\n",
      "4000:\tlearn: 0.1817549\ttest: 0.2403172\tbest: 0.2403102 (3977)\ttotal: 50.4s\tremaining: 3h 28m 56s\n",
      "4500:\tlearn: 0.1768802\ttest: 0.2403689\tbest: 0.2402995 (4007)\ttotal: 57s\tremaining: 3h 29m 58s\n",
      "bestTest = 0.2402994703\n",
      "bestIteration = 4007\n",
      "Shrink model to first 4008 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_nelson\n",
      "0:\tlearn: 0.2711547\ttest: 0.2729373\tbest: 0.2729373 (0)\ttotal: 11.3ms\tremaining: 3h 8m 9s\n",
      "500:\tlearn: 0.2300835\ttest: 0.2446533\tbest: 0.2446533 (500)\ttotal: 6.04s\tremaining: 3h 20m 58s\n",
      "1000:\tlearn: 0.2197662\ttest: 0.2428037\tbest: 0.2427992 (972)\ttotal: 12.2s\tremaining: 3h 23m 17s\n",
      "1500:\tlearn: 0.2118459\ttest: 0.2419745\tbest: 0.2419693 (1483)\ttotal: 18.6s\tremaining: 3h 25m 57s\n",
      "2000:\tlearn: 0.2048471\ttest: 0.2412861\tbest: 0.2412861 (2000)\ttotal: 25.3s\tremaining: 3h 30m 38s\n",
      "2500:\tlearn: 0.1985908\ttest: 0.2408339\tbest: 0.2407834 (2413)\ttotal: 31.6s\tremaining: 3h 29m 58s\n",
      "3000:\tlearn: 0.1928358\ttest: 0.2407105\tbest: 0.2406746 (2975)\ttotal: 37.4s\tremaining: 3h 27m 8s\n",
      "3500:\tlearn: 0.1874935\ttest: 0.2403507\tbest: 0.2403507 (3500)\ttotal: 43.7s\tremaining: 3h 27m 16s\n",
      "4000:\tlearn: 0.1823622\ttest: 0.2402313\tbest: 0.2402146 (3993)\ttotal: 49.8s\tremaining: 3h 26m 27s\n",
      "4500:\tlearn: 0.1776481\ttest: 0.2402174\tbest: 0.2401472 (4233)\ttotal: 56s\tremaining: 3h 26m 19s\n",
      "5000:\tlearn: 0.1731759\ttest: 0.2401465\tbest: 0.2400771 (4780)\ttotal: 1m 2s\tremaining: 3h 26m 12s\n",
      "bestTest = 0.2400770767\n",
      "bestIteration = 4780\n",
      "Shrink model to first 4781 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6768485148989012\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 1 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61759\n",
      "[500]\tvalidation_0-cox-nloglik:7.43015\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41789\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41225\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40855\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40741\n",
      "[3000]\tvalidation_0-cox-nloglik:7.40531\n",
      "[3500]\tvalidation_0-cox-nloglik:7.40565\n",
      "[3509]\tvalidation_0-cox-nloglik:7.40566\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 2 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61400\n",
      "[500]\tvalidation_0-cox-nloglik:7.42389\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41179\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40735\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40549\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40395\n",
      "[2796]\tvalidation_0-cox-nloglik:7.40439\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 3 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61871\n",
      "[500]\tvalidation_0-cox-nloglik:7.41014\n",
      "[1000]\tvalidation_0-cox-nloglik:7.39824\n",
      "[1500]\tvalidation_0-cox-nloglik:7.39263\n",
      "[2000]\tvalidation_0-cox-nloglik:7.38812\n",
      "[2500]\tvalidation_0-cox-nloglik:7.38686\n",
      "[3000]\tvalidation_0-cox-nloglik:7.38658\n",
      "[3500]\tvalidation_0-cox-nloglik:7.38656\n",
      "[3734]\tvalidation_0-cox-nloglik:7.38678\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 4 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61935\n",
      "[500]\tvalidation_0-cox-nloglik:7.42770\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41342\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40753\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40287\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40109\n",
      "[3000]\tvalidation_0-cox-nloglik:7.40028\n",
      "[3500]\tvalidation_0-cox-nloglik:7.40040\n",
      "[3602]\tvalidation_0-cox-nloglik:7.40080\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 5 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61893\n",
      "[500]\tvalidation_0-cox-nloglik:7.43312\n",
      "[1000]\tvalidation_0-cox-nloglik:7.42054\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41444\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41222\n",
      "[2500]\tvalidation_0-cox-nloglik:7.41133\n",
      "[3000]\tvalidation_0-cox-nloglik:7.41120\n",
      "[3500]\tvalidation_0-cox-nloglik:7.41058\n",
      "[3937]\tvalidation_0-cox-nloglik:7.41137\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 6 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61824\n",
      "[500]\tvalidation_0-cox-nloglik:7.43223\n",
      "[1000]\tvalidation_0-cox-nloglik:7.42263\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41812\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41684\n",
      "[2500]\tvalidation_0-cox-nloglik:7.41595\n",
      "[3000]\tvalidation_0-cox-nloglik:7.41564\n",
      "[3500]\tvalidation_0-cox-nloglik:7.41664\n",
      "[3555]\tvalidation_0-cox-nloglik:7.41687\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 7 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61822\n",
      "[500]\tvalidation_0-cox-nloglik:7.44684\n",
      "[1000]\tvalidation_0-cox-nloglik:7.43471\n",
      "[1500]\tvalidation_0-cox-nloglik:7.42979\n",
      "[2000]\tvalidation_0-cox-nloglik:7.42673\n",
      "[2500]\tvalidation_0-cox-nloglik:7.42449\n",
      "[3000]\tvalidation_0-cox-nloglik:7.42501\n",
      "[3125]\tvalidation_0-cox-nloglik:7.42446\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 8 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61985\n",
      "[500]\tvalidation_0-cox-nloglik:7.40491\n",
      "[1000]\tvalidation_0-cox-nloglik:7.38986\n",
      "[1500]\tvalidation_0-cox-nloglik:7.38711\n",
      "[2000]\tvalidation_0-cox-nloglik:7.38536\n",
      "[2500]\tvalidation_0-cox-nloglik:7.38215\n",
      "[3000]\tvalidation_0-cox-nloglik:7.38269\n",
      "[3029]\tvalidation_0-cox-nloglik:7.38298\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 9 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61974\n",
      "[500]\tvalidation_0-cox-nloglik:7.43001\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41550\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41120\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40788\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40694\n",
      "[3000]\tvalidation_0-cox-nloglik:7.40752\n",
      "[3018]\tvalidation_0-cox-nloglik:7.40741\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 10 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61794\n",
      "[500]\tvalidation_0-cox-nloglik:7.43268\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41858\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41112\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40738\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40534\n",
      "[3000]\tvalidation_0-cox-nloglik:7.40474\n",
      "[3500]\tvalidation_0-cox-nloglik:7.40256\n",
      "[3940]\tvalidation_0-cox-nloglik:7.40285\n",
      "==================================================\n",
      "xgboost_cox our out of folds CV score is 0.6718407968430803\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 1 efs_time2\n",
      "0:\tlearn: -137186.7657786\ttest: -11837.5449518\tbest: -11837.5449518 (0)\ttotal: 27.9ms\tremaining: 7h 45m 28s\n",
      "500:\tlearn: -133551.2924164\ttest: -11542.8989296\tbest: -11542.8989296 (500)\ttotal: 10.1s\tremaining: 5h 34m 50s\n",
      "1000:\tlearn: -132465.8649827\ttest: -11523.6841434\tbest: -11523.6144329 (999)\ttotal: 20.2s\tremaining: 5h 36m 2s\n",
      "1500:\tlearn: -131718.0136670\ttest: -11520.2302823\tbest: -11519.9712498 (1486)\ttotal: 30.6s\tremaining: 5h 39m 34s\n",
      "2000:\tlearn: -131086.7561910\ttest: -11520.9426913\tbest: -11519.3562597 (1842)\ttotal: 41s\tremaining: 5h 41m 3s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11519.35626\n",
      "bestIteration = 1842\n",
      "\n",
      "Shrink model to first 1843 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 2 efs_time2\n",
      "0:\tlearn: -137186.8509829\ttest: -11830.6906015\tbest: -11830.6906015 (0)\ttotal: 23.8ms\tremaining: 6h 36m 8s\n",
      "500:\tlearn: -133589.3496723\ttest: -11530.5367632\tbest: -11530.5367632 (500)\ttotal: 10.3s\tremaining: 5h 42m 7s\n",
      "1000:\tlearn: -132468.9234520\ttest: -11514.0927949\tbest: -11514.0927949 (1000)\ttotal: 20.8s\tremaining: 5h 45m 11s\n",
      "1500:\tlearn: -131738.1810197\ttest: -11509.8270878\tbest: -11509.5565563 (1197)\ttotal: 31.3s\tremaining: 5h 47m 24s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11509.55656\n",
      "bestIteration = 1197\n",
      "\n",
      "Shrink model to first 1198 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 3 efs_time2\n",
      "0:\tlearn: -137184.7956455\ttest: -11838.8616366\tbest: -11838.8616366 (0)\ttotal: 21.6ms\tremaining: 5h 59m 20s\n",
      "500:\tlearn: -133575.5771158\ttest: -11510.2654380\tbest: -11510.0970859 (494)\ttotal: 10.4s\tremaining: 5h 45m 17s\n",
      "1000:\tlearn: -132528.1110992\ttest: -11489.9021425\tbest: -11489.9021425 (1000)\ttotal: 20.7s\tremaining: 5h 43m 41s\n",
      "1500:\tlearn: -131786.1801762\ttest: -11486.8572432\tbest: -11486.5838980 (1497)\ttotal: 31.5s\tremaining: 5h 49m 5s\n",
      "2000:\tlearn: -131144.4607863\ttest: -11487.9935767\tbest: -11485.9289635 (1678)\ttotal: 42.5s\tremaining: 5h 53m 40s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11485.92896\n",
      "bestIteration = 1678\n",
      "\n",
      "Shrink model to first 1679 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 4 efs_time2\n",
      "0:\tlearn: -137187.0705622\ttest: -11831.3903810\tbest: -11831.3903810 (0)\ttotal: 22.2ms\tremaining: 6h 10m 46s\n",
      "500:\tlearn: -133596.6104097\ttest: -11533.4613947\tbest: -11533.4613947 (500)\ttotal: 10.6s\tremaining: 5h 52m 15s\n",
      "1000:\tlearn: -132506.8164077\ttest: -11510.6526954\tbest: -11510.3222745 (994)\ttotal: 21.1s\tremaining: 5h 51m 30s\n",
      "1500:\tlearn: -131822.4132560\ttest: -11506.7349210\tbest: -11506.5198201 (1457)\ttotal: 31.6s\tremaining: 5h 50m 52s\n",
      "2000:\tlearn: -131259.4272791\ttest: -11507.1436760\tbest: -11506.3037327 (1684)\ttotal: 42.3s\tremaining: 5h 51m 50s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11506.30373\n",
      "bestIteration = 1684\n",
      "\n",
      "Shrink model to first 1685 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 5 efs_time2\n",
      "0:\tlearn: -137186.1693931\ttest: -11831.1304911\tbest: -11831.1304911 (0)\ttotal: 22.6ms\tremaining: 6h 16m 3s\n",
      "500:\tlearn: -133578.7328518\ttest: -11543.9576433\tbest: -11543.9576433 (500)\ttotal: 10.6s\tremaining: 5h 53m 42s\n",
      "1000:\tlearn: -132462.5772657\ttest: -11525.6328693\tbest: -11525.4394721 (998)\ttotal: 21.3s\tremaining: 5h 53m 55s\n",
      "1500:\tlearn: -131707.5237112\ttest: -11523.4950958\tbest: -11522.9834496 (1438)\ttotal: 32s\tremaining: 5h 54m 50s\n",
      "2000:\tlearn: -131056.9224224\ttest: -11526.8762255\tbest: -11522.8015589 (1532)\ttotal: 42.4s\tremaining: 5h 52m 9s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11522.80156\n",
      "bestIteration = 1532\n",
      "\n",
      "Shrink model to first 1533 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 6 efs_time2\n",
      "0:\tlearn: -137208.6146301\ttest: -11822.8762020\tbest: -11822.8762020 (0)\ttotal: 22.4ms\tremaining: 6h 13m 56s\n",
      "500:\tlearn: -133569.4202965\ttest: -11529.9240484\tbest: -11529.9240484 (500)\ttotal: 10.7s\tremaining: 5h 54m 40s\n",
      "1000:\tlearn: -132446.7324643\ttest: -11519.4800874\tbest: -11519.3729225 (992)\ttotal: 21.4s\tremaining: 5h 55m 52s\n",
      "1500:\tlearn: -131793.8132760\ttest: -11518.7249015\tbest: -11518.2785503 (1451)\ttotal: 32.4s\tremaining: 5h 58m 49s\n",
      "2000:\tlearn: -131279.2830434\ttest: -11519.4196794\tbest: -11517.7233462 (1622)\ttotal: 42.9s\tremaining: 5h 56m 18s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11517.72335\n",
      "bestIteration = 1622\n",
      "\n",
      "Shrink model to first 1623 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 7 efs_time2\n",
      "0:\tlearn: -137201.3624302\ttest: -11822.3621103\tbest: -11822.3621103 (0)\ttotal: 23.1ms\tremaining: 6h 25m 30s\n",
      "500:\tlearn: -133545.7281502\ttest: -11550.3580201\tbest: -11550.3580201 (500)\ttotal: 11.1s\tremaining: 6h 9m 13s\n",
      "1000:\tlearn: -132389.5860592\ttest: -11534.8415547\tbest: -11534.3337800 (967)\ttotal: 21.9s\tremaining: 6h 3m 55s\n",
      "1500:\tlearn: -131669.3531905\ttest: -11531.9472034\tbest: -11531.4786843 (1409)\ttotal: 32.9s\tremaining: 6h 4m 13s\n",
      "2000:\tlearn: -131046.9883132\ttest: -11530.9584922\tbest: -11529.2986382 (1634)\ttotal: 43.7s\tremaining: 6h 2m 59s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11529.29864\n",
      "bestIteration = 1634\n",
      "\n",
      "Shrink model to first 1635 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 8 efs_time2\n",
      "0:\tlearn: -137200.4398944\ttest: -11823.0662223\tbest: -11823.0662223 (0)\ttotal: 22.6ms\tremaining: 6h 16m 55s\n",
      "500:\tlearn: -133657.7554860\ttest: -11478.2797892\tbest: -11478.2797892 (500)\ttotal: 10.8s\tremaining: 6h 10s\n",
      "1000:\tlearn: -132487.2529107\ttest: -11456.3807604\tbest: -11456.3362248 (996)\ttotal: 21.4s\tremaining: 5h 56m 24s\n",
      "1500:\tlearn: -131801.1214846\ttest: -11449.4603087\tbest: -11449.4603087 (1500)\ttotal: 32.1s\tremaining: 5h 56m 11s\n",
      "2000:\tlearn: -131201.6536906\ttest: -11449.1291108\tbest: -11447.7594630 (1746)\ttotal: 42.6s\tremaining: 5h 54m 26s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11447.75946\n",
      "bestIteration = 1746\n",
      "\n",
      "Shrink model to first 1747 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 9 efs_time2\n",
      "0:\tlearn: -137178.2324758\ttest: -11836.8719004\tbest: -11836.8719004 (0)\ttotal: 29.3ms\tremaining: 8h 9m 2s\n",
      "500:\tlearn: -133587.9251376\ttest: -11526.6450908\tbest: -11526.5360371 (495)\ttotal: 11.3s\tremaining: 6h 14m 22s\n",
      "1000:\tlearn: -132464.3665607\ttest: -11508.0317937\tbest: -11508.0317937 (1000)\ttotal: 21.9s\tremaining: 6h 3m 44s\n",
      "1500:\tlearn: -131735.9438460\ttest: -11501.2066867\tbest: -11501.2066867 (1500)\ttotal: 32.5s\tremaining: 6h 36s\n",
      "2000:\tlearn: -131116.6207552\ttest: -11498.4892632\tbest: -11498.2510364 (1929)\ttotal: 43.5s\tremaining: 6h 1m 41s\n",
      "2500:\tlearn: -130557.4944227\ttest: -11497.4264376\tbest: -11497.1005634 (2467)\ttotal: 54.7s\tremaining: 6h 3m 48s\n",
      "3000:\tlearn: -130124.8261980\ttest: -11498.0518425\tbest: -11497.0115735 (2534)\ttotal: 1m 7s\tremaining: 6h 12m 32s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11497.01157\n",
      "bestIteration = 2534\n",
      "\n",
      "Shrink model to first 2535 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 10 efs_time2\n",
      "0:\tlearn: -137188.6019806\ttest: -11837.9914066\tbest: -11837.9914066 (0)\ttotal: 23.5ms\tremaining: 6h 31m 20s\n",
      "500:\tlearn: -133575.6704949\ttest: -11544.6077719\tbest: -11544.6077719 (500)\ttotal: 13.8s\tremaining: 7h 37m 49s\n",
      "1000:\tlearn: -132515.3755818\ttest: -11525.2804004\tbest: -11525.2804004 (1000)\ttotal: 25.8s\tremaining: 7h 9m 57s\n",
      "1500:\tlearn: -131789.6847448\ttest: -11520.4675440\tbest: -11520.3741855 (1476)\ttotal: 37s\tremaining: 6h 50m 43s\n",
      "2000:\tlearn: -131146.1529146\ttest: -11520.7373075\tbest: -11519.3154945 (1852)\ttotal: 49s\tremaining: 6h 47m 24s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11519.31549\n",
      "bestIteration = 1852\n",
      "\n",
      "Shrink model to first 1853 iterations.\n",
      "==================================================\n",
      "catboost_cox our out of folds CV score is 0.672173604835094\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Training\n",
    "# ====================================================\n",
    "# for method in CFG.METHOD_LIST:\n",
    "#     gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n",
    "\n",
    "# kaplan-meier & nelson-aalen models\n",
    "for method in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n",
    "    gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n",
    "# Cox models\n",
    "for method in [\"xgboost_cox\", \"catboost_cox\"]:\n",
    "    gradient_boosting_model_cv_training(method, train, CFG.cox_target_col_list, FEATURES, CATS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for Ensemble = 0.6817420032581314\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Overall CV\n",
    "# ====================================================\n",
    "# kaplan-meier models\n",
    "oof_lgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# nelson-aalen models\n",
    "oof_lgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "# Cox models\n",
    "oof_cox_xgb = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cox_cat = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# # polars\n",
    "# y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].clone()\n",
    "# y_pred = train[[\"ID\"]].clone()\n",
    "# ensamble_prediction = (\n",
    "#     rankdata(oof_xgb_kaplan)\n",
    "#     + rankdata(oof_cat_kaplan)\n",
    "#     + rankdata(oof_lgb_kaplan)\n",
    "#     + rankdata(oof_lgb_nelson)\n",
    "#     + rankdata(oof_xgb_nelson)\n",
    "#     + rankdata(oof_cat_nelson)\n",
    "#     + rankdata(oof_cox_xgb)\n",
    "#     + rankdata(oof_cox_cat)\n",
    "# )\n",
    "# y_pred = y_pred.with_columns(pl.Series(ensamble_prediction).alias(\"prediction\"))\n",
    "# m = score(y_true.to_pandas().copy(), y_pred.to_pandas().copy(), \"ID\")\n",
    "# print(\"\\nOverall CV for Ensemble =\", m)\n",
    "\n",
    "# pandas\n",
    "y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "ensamble_prediction = (\n",
    "    rankdata(oof_xgb_kaplan)\n",
    "    + rankdata(oof_cat_kaplan)\n",
    "    + rankdata(oof_lgb_kaplan)\n",
    "    + rankdata(oof_lgb_nelson)\n",
    "    + rankdata(oof_xgb_nelson)\n",
    "    + rankdata(oof_cat_nelson)\n",
    "    + rankdata(oof_cox_xgb)\n",
    "    + rankdata(oof_cox_cat)\n",
    ")\n",
    "y_pred[\"prediction\"] = ensamble_prediction\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(\"\\nOverall CV for Ensemble =\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な重み: [0.15025943 0.11314827 0.00614957 0.12902333 0.11067298 0.09774591\n",
      " 0.28140135 0.1415951 ]\n",
      "最適化後のスコア: -0.6826559463511102\n"
     ]
    }
   ],
   "source": [
    "def ensemble_score(weights):\n",
    "    # 重み付けした予測値を計算\n",
    "    weighted_pred = (\n",
    "        weights[0] * rankdata(oof_lgb_kaplan)\n",
    "        + weights[1] * rankdata(oof_xgb_kaplan)\n",
    "        + weights[2] * rankdata(oof_cat_kaplan)\n",
    "        + weights[3] * rankdata(oof_lgb_nelson)\n",
    "        + weights[4] * rankdata(oof_xgb_nelson)\n",
    "        + weights[5] * rankdata(oof_cat_nelson)\n",
    "        + weights[6] * rankdata(oof_cox_xgb)\n",
    "        + weights[7] * rankdata(oof_cox_cat)\n",
    "    )\n",
    "\n",
    "    y_pred = pd.DataFrame({\"ID\": train[\"ID\"], \"prediction\": weighted_pred})\n",
    "    y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "\n",
    "    return -score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "\n",
    "\n",
    "# 8つのモデルの初期重みを均等に設定\n",
    "initial_weights = [1 / 8] * 8\n",
    "\n",
    "# 最適化実行\n",
    "result = minimize(ensemble_score, initial_weights, method=\"Nelder-Mead\")\n",
    "\n",
    "print(\"最適な重み:\", result.x)\n",
    "print(\"最適化後のスコア:\", result.fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Inference functions\n",
    "# ====================================================\n",
    "def lightgbm_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"lightgbm_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def xgboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        # pred = model.predict(xgb.DMatrix(x_test, enable_categorical=True))\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list, target_col: str):\n",
    "    x_test = test_df[features]\n",
    "    if method == \"lightgbm\":\n",
    "        test_pred = lightgbm_inference(x_test, target_col)\n",
    "    if method == \"xgboost\":\n",
    "        test_pred = xgboost_inference(x_test, target_col)\n",
    "    if method == \"catboost\":\n",
    "        test_pred = catboost_inference(x_test, target_col)\n",
    "    # Cox models\n",
    "    elif method == \"xgboost_cox\":\n",
    "        test_pred = xgboost_cox_inference(x_test, target_col)\n",
    "    elif method == \"catboost_cox\":\n",
    "        test_pred = catboost_cox_inference(x_test, target_col)\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "def predicting(method_list: list, input_df: pd.DataFrame, target_col_list: list, features: list):\n",
    "    output_df = input_df.copy()\n",
    "    for target_col in target_col_list:\n",
    "        # output_df[target_col] = 0\n",
    "        for method in method_list:\n",
    "            output_df[f\"{method}_pred_{target_col}\"] = gradient_boosting_model_inference(\n",
    "                method, input_df, features, target_col\n",
    "            )\n",
    "            # output_df[target_col] += CFG.model_weight_dict[method] * output_df[f\"{method}_pred_{target_col}\"]\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub shape: (3, 2)\n",
      "      ID  prediction\n",
      "0  28800        16.0\n",
      "1  28801        24.0\n",
      "2  28802         8.0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Inference\n",
    "# ====================================================\n",
    "# kaplan-meier & nelson-aalen models\n",
    "output_df = predicting([\"lightgbm\", \"xgboost\", \"catboost\"], test, CFG.target_col_list, FEATURES)\n",
    "pred_lgb_kaplan = output_df[\"lightgbm_pred_y_kaplan\"]\n",
    "pred_xgb_kaplan = output_df[\"xgboost_pred_y_kaplan\"]\n",
    "pred_cat_kaplan = output_df[\"catboost_pred_y_kaplan\"]\n",
    "pred_lgb_nelson = output_df[\"lightgbm_pred_y_nelson\"]\n",
    "pred_xgb_nelson = output_df[\"xgboost_pred_y_nelson\"]\n",
    "pred_cat_nelson = output_df[\"catboost_pred_y_nelson\"]\n",
    "# Cox models\n",
    "cox_output_df = predicting([\"xgboost_cox\", \"catboost_cox\"], test, CFG.cox_target_col_list, FEATURES)\n",
    "pred_cox_xgb = cox_output_df[\"xgboost_cox_pred_efs_time2\"]\n",
    "pred_cox_cat = cox_output_df[\"catboost_cox_pred_efs_time2\"]\n",
    "\n",
    "submission = pd.read_csv(CFG.DATA_PATH / \"sample_submission.csv\")\n",
    "submission[\"prediction\"] = (\n",
    "    rankdata(pred_lgb_kaplan)\n",
    "    + rankdata(pred_xgb_kaplan)\n",
    "    + rankdata(pred_cat_kaplan)\n",
    "    + rankdata(pred_lgb_nelson)\n",
    "    + rankdata(pred_xgb_nelson)\n",
    "    + rankdata(pred_cat_nelson)\n",
    "    + rankdata(pred_cox_xgb)\n",
    "    + rankdata(pred_cox_cat)\n",
    ")\n",
    "submission.to_csv(CFG.OUTPUT_DIR / \"submission.csv\", index=False)\n",
    "print(\"Sub shape:\", submission.shape)\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
