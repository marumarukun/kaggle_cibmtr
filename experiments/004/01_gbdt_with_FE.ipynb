{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import config  # edit config.py as needed\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter, NelsonAalenFitter\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from metric import score  # edit metric.py as needed\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import rankdata\n",
    "from seed import seed_everything  # edit seed.py as needed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    DRY_RUN = False\n",
    "    EXP_NAME = config.EXP_NAME\n",
    "    AUTHOR = \"marumarukun\"\n",
    "    COMPETITION = config.KAGGLE_COMPETITION_NAME\n",
    "    DATA_PATH = config.COMP_DATASET_DIR\n",
    "    OUTPUT_DIR = config.OUTPUT_DIR\n",
    "    MODEL_PATH = config.OUTPUT_DIR / \"models\"  # モデル作成・実験時はこちらを使用\n",
    "    # MODEL_PATH = config.ARTIFACT_EXP_DIR(config.EXP_NAME) / \"models\"  # 提出時はこちらを使用\n",
    "    METHOD_LIST = [\"xgboost_cox\", \"catboost_cox\", \"lightgbm\", \"xgboost\", \"catboost\"]\n",
    "    SEED = 42\n",
    "    n_folds = 2 if DRY_RUN else 10\n",
    "    target_col_list = [\"y_kaplan\", \"y_nelson\"]\n",
    "    cox_target_col_list = [\"efs_time2\"]\n",
    "    # group_col = \"race_group\"  # Required for GroupKFold (edit as needed)\n",
    "    stratified_col = \"race_group_efs\"  # Required for StratifiedKFold (edit as needed)\n",
    "    num_boost_round = 100 if DRY_RUN else 1000000\n",
    "    early_stopping_round = 10 if DRY_RUN else 500  # 10÷lrで設定\n",
    "    verbose = 500\n",
    "\n",
    "    # https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
    "    regression_lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        # \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"seed\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
    "    regression_xgb_params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # \"eval_metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    regression_xgb_cox_params = {\n",
    "        \"objective\": \"survival:cox\",\n",
    "        \"eval_metric\": \"cox-nloglik\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 3,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 80,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://catboost.ai/docs/en/references/training-parameters/\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostregressor\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostclassifier\n",
    "    regression_cat_params = {\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"GPU\",  # CPU/GPU\n",
    "    }\n",
    "    regression_cat_cox_params = {\n",
    "        \"loss_function\": \"Cox\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"CPU\",  # CPU/GPU\n",
    "    }\n",
    "\n",
    "    model_weight_dict = {\"lightgbm\": 0.40, \"xgboost\": 0.30, \"catboost\": 0.30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "seed_everything(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "train = pl.read_csv(CFG.DATA_PATH / \"train.csv\", try_parse_dates=True)\n",
    "test = pl.read_csv(CFG.DATA_PATH / \"test.csv\", try_parse_dates=True)\n",
    "# make index column\n",
    "# train = train.with_row_index()\n",
    "# test = test.with_row_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Preprocess(ここに前処理や特徴量エンジニアリングを記述)\n",
    "# ====================================================\n",
    "def transform_survival_probability(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "\n",
    "\n",
    "def transform_cumulative_hazard(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    naf = NelsonAalenFitter()\n",
    "    naf.fit(durations=df[time_col], event_observed=df[event_col])\n",
    "    y = naf.cumulative_hazard_at_times(df[time_col]).to_numpy()\n",
    "    return -y\n",
    "\n",
    "\n",
    "def preprocess(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    output = df.clone()\n",
    "    # 欠損値のカウント（最初に行う）\n",
    "    output = output.with_columns(pl.sum_horizontal(pl.all().is_null()).alias(\"null_count\"))\n",
    "\n",
    "    # ドナーと患者の性別マッチング\n",
    "    output = output.with_columns(\n",
    "        pl.when(pl.col(\"sex_match\").str.contains_any([\"M-M\", \"F-F\"]))\n",
    "        .then(1)\n",
    "        .when(pl.col(\"sex_match\").is_null())\n",
    "        .then(None)\n",
    "        .otherwise(0)\n",
    "        .alias(\"is_sex_match\"),\n",
    "    )\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "\n",
    "# apply Kaplan-Meier\n",
    "y_kaplan = transform_survival_probability(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_kaplan).alias(\"y_kaplan\"))\n",
    "\n",
    "# apply Nelson-Aalen\n",
    "y_nelson = transform_cumulative_hazard(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_nelson).alias(\"y_nelson\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Make fold column\n",
    "# ====================================================\n",
    "# race_group_efs列を作成\n",
    "train = train.with_columns((pl.col(\"race_group\").cast(str) + \"_\" + pl.col(\"efs\").cast(str)).alias(\"race_group_efs\"))\n",
    "\n",
    "fold_array = np.zeros(train.height)\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train, train[CFG.stratified_col]), start=1):\n",
    "    fold_array[val_idx] = fold\n",
    "train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n",
    "\n",
    "# fold_array = np.zeros(train.height)\n",
    "# kf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "# for fold, (_, val_idx) in enumerate(kf.split(train), start=1):\n",
    "#     fold_array[val_idx] = fold\n",
    "# train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10', 'null_count', 'is_sex_match']\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Column selection\n",
    "# ====================================================\n",
    "# Feature columns\n",
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"y_kaplan\", \"y_nelson\", \"fold\", \"race_group_efs\", \"efs_time2\"]\n",
    "FEATURES = [c for c in train.columns if c not in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35 CATEGORICAL FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n"
     ]
    }
   ],
   "source": [
    "# Categorical features\n",
    "CATS = []\n",
    "cat_count = 0\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype == pl.String:\n",
    "        cat_count += 1\n",
    "        CATS.append(c)\n",
    "print(f\"There are {cat_count} CATEGORICAL FEATURES: {CATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode categorical features\n",
    "\n",
    "# train_test = pl.concat([train, test], how=\"diagonal\")\n",
    "\n",
    "# 250109追記)カテゴリ型に変換するだけで充分かも\n",
    "train = train.with_columns(pl.col(CATS).fill_null(\"NaN\").cast(pl.Categorical))\n",
    "test = test.with_columns(pl.col(CATS).fill_null(\"NaN\").cast(pl.Categorical))\n",
    "\n",
    "for c in CATS:\n",
    "    pass\n",
    "    # train, testで分けているのはkaggle対策（本来のtestにアクセスできないため）\n",
    "    # OrdinalEncoderを使用しているのはtestに未知の値あっても指定の値(-1)に変換できるため\n",
    "    # 250109追記）これだと未知の値を全て同じ値として扱ってしまうので、改善が必要かも\n",
    "    # oe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "    # train = train.with_columns(\n",
    "    #     pl.Series(oe.fit_transform(train[c].fill_null(\"NaN\").to_numpy().reshape(-1, 1)).reshape(-1))\n",
    "    #     .cast(pl.String)\n",
    "    #     .cast(pl.Categorical)\n",
    "    #     .alias(c)\n",
    "    # )\n",
    "    # test = test.with_columns(\n",
    "    #     pl.Series(oe.transform(test[c].fill_null(\"NaN\").to_numpy().reshape(-1, 1)).reshape(-1))\n",
    "    #     .cast(pl.String)\n",
    "    #     .cast(pl.Categorical)\n",
    "    #     .alias(c)\n",
    "    # )\n",
    "    # # 本来のtestにアクセスできるコンペではtrain, testを結合してLabelEncodeすればよい\n",
    "    # le = LabelEncoder()\n",
    "    # train_test = train_test.with_columns(\n",
    "    #     pl.Series(le.fit_transform(train_test[c].fill_null(\"NaN\")))\n",
    "    #     .cast(pl.String)\n",
    "    #     .cast(pl.Categorical)\n",
    "    #     .alias(c)\n",
    "    # )\n",
    "# train = train_test.filter(pl.col(\"fold\").is_not_null())\n",
    "# test = train_test.filter(pl.col(\"fold\").is_null())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Survival Cox model用のターゲット作成\n",
    "# ====================================================\n",
    "# create cox model's target\n",
    "train = train.with_columns(\n",
    "    pl.when(pl.col(\"efs\") == 0).then(pl.col(\"efs_time\") * -1).otherwise(pl.col(\"efs_time\")).alias(\"efs_time2\")\n",
    ")\n",
    "\n",
    "# train[\"efs_time2\"] = train.efs_time.copy()\n",
    "# train.loc[train.efs == 0, \"efs_time2\"] *= -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Training functions\n",
    "# ====================================================\n",
    "def lightgbm_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    model = LGBMRegressor(\n",
    "        **CFG.regression_lgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=CFG.early_stopping_round),\n",
    "            lgb.log_evaluation(CFG.verbose),\n",
    "        ],\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def xgboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_cox_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_cox_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def plot_feature_importance(model, features, method, target_col, fold):\n",
    "    \"\"\"特徴量の重要度をプロットする関数\"\"\"\n",
    "    # 各モデルタイプに応じた特徴量重要度の取得方法\n",
    "    if method == \"lightgbm\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.feature_importances_})\n",
    "    elif method == \"xgboost\" or method == \"xgboost_cox\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.feature_importances_})\n",
    "    elif method == \"catboost\" or method == \"catboost_cox\":\n",
    "        importance = pd.DataFrame({\"feature\": features, \"importance\": model.get_feature_importance()})\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance.sort_values(\"importance\", ascending=False).head(20), x=\"importance\", y=\"feature\")\n",
    "    plt.title(f\"{method} Feature Importance\\nTarget: {target_col}, Fold: {fold}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 保存先のディレクトリを作成\n",
    "    save_dir = CFG.OUTPUT_DIR / \"feature_importance\"\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(save_dir / f\"feature_importance_{method}_{target_col}_fold{fold}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def gradient_boosting_model_cv_training(\n",
    "    method: str, train_df: pd.DataFrame, target_col_list: list, features: list, categorical_features: list\n",
    "):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    for target_col in target_col_list:\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{method} training fold {fold+1} {target_col}\")\n",
    "            x_train = train_df[train_df[\"fold\"] != fold + 1][features]\n",
    "            y_train = train_df[train_df[\"fold\"] != fold + 1][target_col]\n",
    "            x_valid = train_df[train_df[\"fold\"] == fold + 1][features]\n",
    "            y_valid = train_df[train_df[\"fold\"] == fold + 1][target_col]\n",
    "\n",
    "            if method == \"lightgbm\":\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            elif method == \"xgboost\":\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost\":\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            # Cox models\n",
    "            elif method == \"xgboost_cox\":\n",
    "                model, valid_pred = xgboost_cox_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost_cox\":\n",
    "                model, valid_pred = catboost_cox_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "            # Feature Importanceの可視化(最後のfoldのみ)\n",
    "            if fold == CFG.n_folds - 1:\n",
    "                plot_feature_importance(model, features, method, target_col, fold + 1)\n",
    "\n",
    "            # Save best model\n",
    "            save_model_path = (\n",
    "                CFG.MODEL_PATH / f\"{method}_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\"\n",
    "            )\n",
    "            save_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            pickle.dump(\n",
    "                model,\n",
    "                open(\n",
    "                    save_model_path,\n",
    "                    \"wb\",\n",
    "                ),\n",
    "            )\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[train_df[\"fold\"] == fold + 1] = valid_pred\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_predictions_df = pd.DataFrame()\n",
    "        oof_predictions_df[\"ID\"] = train_df[\"ID\"].values\n",
    "        oof_predictions_df[\"prediction\"] = oof_predictions\n",
    "        oof_predictions_df.to_csv(\n",
    "            CFG.OUTPUT_DIR / f\"oof_{method}_{target_col}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\", index=False\n",
    "        )\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        y_true = train_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        m = score(y_true.copy(), oof_predictions_df.copy(), \"ID\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{method} our out of folds CV score is {m}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 882\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606188\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0245529\n",
      "[1000]\tvalid_0's l2: 0.0243177\n",
      "[1500]\tvalid_0's l2: 0.0242623\n",
      "[2000]\tvalid_0's l2: 0.0243101\n",
      "Early stopping, best iteration is:\n",
      "[1674]\tvalid_0's l2: 0.0242342\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606660\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0237909\n",
      "[1000]\tvalid_0's l2: 0.0235958\n",
      "[1500]\tvalid_0's l2: 0.0236258\n",
      "Early stopping, best iteration is:\n",
      "[1083]\tvalid_0's l2: 0.0235698\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606506\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0237316\n",
      "[1000]\tvalid_0's l2: 0.0234178\n",
      "[1500]\tvalid_0's l2: 0.0233425\n",
      "[2000]\tvalid_0's l2: 0.0233909\n",
      "Early stopping, best iteration is:\n",
      "[1506]\tvalid_0's l2: 0.0233374\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606093\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246408\n",
      "[1000]\tvalid_0's l2: 0.0243834\n",
      "[1500]\tvalid_0's l2: 0.0243595\n",
      "[2000]\tvalid_0's l2: 0.0243833\n",
      "Early stopping, best iteration is:\n",
      "[1531]\tvalid_0's l2: 0.0243488\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606420\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0245169\n",
      "[1000]\tvalid_0's l2: 0.0243964\n",
      "[1500]\tvalid_0's l2: 0.0244417\n",
      "Early stopping, best iteration is:\n",
      "[1005]\tvalid_0's l2: 0.0243925\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606450\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0239337\n",
      "[1000]\tvalid_0's l2: 0.0238676\n",
      "Early stopping, best iteration is:\n",
      "[735]\tvalid_0's l2: 0.0238483\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605974\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0246561\n",
      "[1000]\tvalid_0's l2: 0.0244394\n",
      "[1500]\tvalid_0's l2: 0.0244303\n",
      "Early stopping, best iteration is:\n",
      "[1180]\tvalid_0's l2: 0.0244051\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605772\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0245534\n",
      "[1000]\tvalid_0's l2: 0.0242394\n",
      "[1500]\tvalid_0's l2: 0.0242053\n",
      "Early stopping, best iteration is:\n",
      "[1288]\tvalid_0's l2: 0.0241783\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.606044\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0248129\n",
      "[1000]\tvalid_0's l2: 0.0246196\n",
      "[1500]\tvalid_0's l2: 0.0246336\n",
      "Early stopping, best iteration is:\n",
      "[1134]\tvalid_0's l2: 0.0246006\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_kaplan\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score 0.605780\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0244014\n",
      "[1000]\tvalid_0's l2: 0.0239693\n",
      "[1500]\tvalid_0's l2: 0.0238417\n",
      "[2000]\tvalid_0's l2: 0.0238579\n",
      "Early stopping, best iteration is:\n",
      "[1771]\tvalid_0's l2: 0.0238364\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.673871587718856\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 882\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539329\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0587982\n",
      "[1000]\tvalid_0's l2: 0.058205\n",
      "Early stopping, best iteration is:\n",
      "[941]\tvalid_0's l2: 0.0581331\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538669\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0571779\n",
      "[1000]\tvalid_0's l2: 0.0567658\n",
      "[1500]\tvalid_0's l2: 0.0567253\n",
      "Early stopping, best iteration is:\n",
      "[1435]\tvalid_0's l2: 0.0566789\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538856\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0564313\n",
      "[1000]\tvalid_0's l2: 0.0556027\n",
      "[1500]\tvalid_0's l2: 0.0555014\n",
      "[2000]\tvalid_0's l2: 0.0554685\n",
      "[2500]\tvalid_0's l2: 0.0555897\n",
      "Early stopping, best iteration is:\n",
      "[2059]\tvalid_0's l2: 0.0554471\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539416\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0587519\n",
      "[1000]\tvalid_0's l2: 0.0581626\n",
      "[1500]\tvalid_0's l2: 0.0581315\n",
      "[2000]\tvalid_0's l2: 0.0581953\n",
      "Early stopping, best iteration is:\n",
      "[1695]\tvalid_0's l2: 0.0580954\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539032\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0585847\n",
      "[1000]\tvalid_0's l2: 0.0581752\n",
      "[1500]\tvalid_0's l2: 0.0583065\n",
      "Early stopping, best iteration is:\n",
      "[1203]\tvalid_0's l2: 0.0581018\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.538973\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0576346\n",
      "[1000]\tvalid_0's l2: 0.0574366\n",
      "[1500]\tvalid_0's l2: 0.0574815\n",
      "Early stopping, best iteration is:\n",
      "[1040]\tvalid_0's l2: 0.0574092\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 884\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539673\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0589712\n",
      "[1000]\tvalid_0's l2: 0.0585584\n",
      "[1500]\tvalid_0's l2: 0.0584176\n",
      "[2000]\tvalid_0's l2: 0.0585462\n",
      "Early stopping, best iteration is:\n",
      "[1612]\tvalid_0's l2: 0.0583865\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539898\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0584164\n",
      "[1000]\tvalid_0's l2: 0.0578516\n",
      "[1500]\tvalid_0's l2: 0.0577461\n",
      "[2000]\tvalid_0's l2: 0.0577174\n",
      "[2500]\tvalid_0's l2: 0.0577082\n",
      "Early stopping, best iteration is:\n",
      "[2376]\tvalid_0's l2: 0.0576271\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539493\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0588155\n",
      "[1000]\tvalid_0's l2: 0.0582973\n",
      "[1500]\tvalid_0's l2: 0.0582099\n",
      "[2000]\tvalid_0's l2: 0.0583278\n",
      "Early stopping, best iteration is:\n",
      "[1519]\tvalid_0's l2: 0.0581795\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 883\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 59\n",
      "[LightGBM] [Info] Start training from score -0.539971\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.058157\n",
      "[1000]\tvalid_0's l2: 0.057081\n",
      "[1500]\tvalid_0's l2: 0.0568842\n",
      "[2000]\tvalid_0's l2: 0.0567634\n",
      "Early stopping, best iteration is:\n",
      "[1977]\tvalid_0's l2: 0.0567345\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.6762491876537725\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17639\n",
      "[500]\tvalidation_0-rmse:0.15693\n",
      "[1000]\tvalidation_0-rmse:0.15583\n",
      "[1500]\tvalidation_0-rmse:0.15553\n",
      "[2000]\tvalidation_0-rmse:0.15574\n",
      "[2307]\tvalidation_0-rmse:0.15581\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17322\n",
      "[500]\tvalidation_0-rmse:0.15474\n",
      "[1000]\tvalidation_0-rmse:0.15382\n",
      "[1500]\tvalidation_0-rmse:0.15369\n",
      "[1923]\tvalidation_0-rmse:0.15382\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17501\n",
      "[500]\tvalidation_0-rmse:0.15454\n",
      "[1000]\tvalidation_0-rmse:0.15327\n",
      "[1500]\tvalidation_0-rmse:0.15309\n",
      "[2000]\tvalidation_0-rmse:0.15328\n",
      "[2166]\tvalidation_0-rmse:0.15335\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17815\n",
      "[500]\tvalidation_0-rmse:0.15732\n",
      "[1000]\tvalidation_0-rmse:0.15585\n",
      "[1500]\tvalidation_0-rmse:0.15550\n",
      "[2000]\tvalidation_0-rmse:0.15554\n",
      "[2077]\tvalidation_0-rmse:0.15559\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17425\n",
      "[500]\tvalidation_0-rmse:0.15726\n",
      "[1000]\tvalidation_0-rmse:0.15662\n",
      "[1500]\tvalidation_0-rmse:0.15643\n",
      "[2000]\tvalidation_0-rmse:0.15657\n",
      "[2100]\tvalidation_0-rmse:0.15653\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17421\n",
      "[500]\tvalidation_0-rmse:0.15519\n",
      "[1000]\tvalidation_0-rmse:0.15454\n",
      "[1457]\tvalidation_0-rmse:0.15456\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17675\n",
      "[500]\tvalidation_0-rmse:0.15820\n",
      "[1000]\tvalidation_0-rmse:0.15721\n",
      "[1500]\tvalidation_0-rmse:0.15728\n",
      "[1766]\tvalidation_0-rmse:0.15734\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17938\n",
      "[500]\tvalidation_0-rmse:0.15691\n",
      "[1000]\tvalidation_0-rmse:0.15561\n",
      "[1500]\tvalidation_0-rmse:0.15536\n",
      "[2000]\tvalidation_0-rmse:0.15536\n",
      "[2184]\tvalidation_0-rmse:0.15545\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17816\n",
      "[500]\tvalidation_0-rmse:0.15799\n",
      "[1000]\tvalidation_0-rmse:0.15698\n",
      "[1500]\tvalidation_0-rmse:0.15704\n",
      "[1714]\tvalidation_0-rmse:0.15713\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_kaplan\n",
      "[0]\tvalidation_0-rmse:0.17741\n",
      "[500]\tvalidation_0-rmse:0.15707\n",
      "[1000]\tvalidation_0-rmse:0.15530\n",
      "[1500]\tvalidation_0-rmse:0.15482\n",
      "[2000]\tvalidation_0-rmse:0.15453\n",
      "[2500]\tvalidation_0-rmse:0.15446\n",
      "[3000]\tvalidation_0-rmse:0.15462\n",
      "[3005]\tvalidation_0-rmse:0.15462\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6722085282189766\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27150\n",
      "[500]\tvalidation_0-rmse:0.24251\n",
      "[1000]\tvalidation_0-rmse:0.24094\n",
      "[1500]\tvalidation_0-rmse:0.24061\n",
      "[1762]\tvalidation_0-rmse:0.24067\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26740\n",
      "[500]\tvalidation_0-rmse:0.23968\n",
      "[1000]\tvalidation_0-rmse:0.23829\n",
      "[1500]\tvalidation_0-rmse:0.23812\n",
      "[1818]\tvalidation_0-rmse:0.23807\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26940\n",
      "[500]\tvalidation_0-rmse:0.23823\n",
      "[1000]\tvalidation_0-rmse:0.23657\n",
      "[1500]\tvalidation_0-rmse:0.23616\n",
      "[2000]\tvalidation_0-rmse:0.23644\n",
      "[2166]\tvalidation_0-rmse:0.23659\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27373\n",
      "[500]\tvalidation_0-rmse:0.24305\n",
      "[1000]\tvalidation_0-rmse:0.24095\n",
      "[1500]\tvalidation_0-rmse:0.24044\n",
      "[2000]\tvalidation_0-rmse:0.24049\n",
      "[2365]\tvalidation_0-rmse:0.24043\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26868\n",
      "[500]\tvalidation_0-rmse:0.24242\n",
      "[1000]\tvalidation_0-rmse:0.24148\n",
      "[1500]\tvalidation_0-rmse:0.24148\n",
      "[1733]\tvalidation_0-rmse:0.24164\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26903\n",
      "[500]\tvalidation_0-rmse:0.24037\n",
      "[1000]\tvalidation_0-rmse:0.23965\n",
      "[1436]\tvalidation_0-rmse:0.23993\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27211\n",
      "[500]\tvalidation_0-rmse:0.24409\n",
      "[1000]\tvalidation_0-rmse:0.24270\n",
      "[1500]\tvalidation_0-rmse:0.24263\n",
      "[1787]\tvalidation_0-rmse:0.24283\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27535\n",
      "[500]\tvalidation_0-rmse:0.24213\n",
      "[1000]\tvalidation_0-rmse:0.24013\n",
      "[1500]\tvalidation_0-rmse:0.23963\n",
      "[2000]\tvalidation_0-rmse:0.23975\n",
      "[2350]\tvalidation_0-rmse:0.23996\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27381\n",
      "[500]\tvalidation_0-rmse:0.24323\n",
      "[1000]\tvalidation_0-rmse:0.24200\n",
      "[1500]\tvalidation_0-rmse:0.24216\n",
      "[1722]\tvalidation_0-rmse:0.24210\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27295\n",
      "[500]\tvalidation_0-rmse:0.24217\n",
      "[1000]\tvalidation_0-rmse:0.23960\n",
      "[1500]\tvalidation_0-rmse:0.23881\n",
      "[2000]\tvalidation_0-rmse:0.23893\n",
      "[2136]\tvalidation_0-rmse:0.23897\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6755384377204804\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_kaplan\n",
      "0:\tlearn: 0.1762565\ttest: 0.1763624\tbest: 0.1763624 (0)\ttotal: 11.5ms\tremaining: 3h 11m 29s\n",
      "500:\tlearn: 0.1514130\ttest: 0.1581382\tbest: 0.1581368 (498)\ttotal: 5.23s\tremaining: 2h 53m 48s\n",
      "1000:\tlearn: 0.1461329\ttest: 0.1567276\tbest: 0.1567122 (995)\ttotal: 11.1s\tremaining: 3h 5m 9s\n",
      "1500:\tlearn: 0.1422130\ttest: 0.1560987\tbest: 0.1560987 (1500)\ttotal: 17.2s\tremaining: 3h 10m 32s\n",
      "2000:\tlearn: 0.1387938\ttest: 0.1558397\tbest: 0.1558395 (1993)\ttotal: 23.3s\tremaining: 3h 13m 17s\n",
      "2500:\tlearn: 0.1356893\ttest: 0.1556900\tbest: 0.1556698 (2458)\ttotal: 29.3s\tremaining: 3h 15m\n",
      "3000:\tlearn: 0.1328440\ttest: 0.1555044\tbest: 0.1554956 (2991)\ttotal: 35.6s\tremaining: 3h 17m 4s\n",
      "3500:\tlearn: 0.1302157\ttest: 0.1554575\tbest: 0.1554358 (3430)\ttotal: 41.7s\tremaining: 3h 17m 35s\n",
      "bestTest = 0.1554357974\n",
      "bestIteration = 3430\n",
      "Shrink model to first 3431 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_kaplan\n",
      "0:\tlearn: 0.1766079\ttest: 0.1731950\tbest: 0.1731950 (0)\ttotal: 9.13ms\tremaining: 2h 32m 5s\n",
      "500:\tlearn: 0.1515538\ttest: 0.1556616\tbest: 0.1556616 (500)\ttotal: 5.63s\tremaining: 3h 7m 11s\n",
      "1000:\tlearn: 0.1465096\ttest: 0.1546099\tbest: 0.1546063 (993)\ttotal: 11.6s\tremaining: 3h 12m 38s\n",
      "1500:\tlearn: 0.1427590\ttest: 0.1543534\tbest: 0.1543534 (1500)\ttotal: 17.5s\tremaining: 3h 13m 48s\n",
      "2000:\tlearn: 0.1394649\ttest: 0.1541128\tbest: 0.1541109 (1998)\ttotal: 23.7s\tremaining: 3h 17m 10s\n",
      "2500:\tlearn: 0.1365153\ttest: 0.1539689\tbest: 0.1539679 (2489)\ttotal: 30.1s\tremaining: 3h 19m 50s\n",
      "3000:\tlearn: 0.1337125\ttest: 0.1538469\tbest: 0.1538311 (2914)\ttotal: 36.3s\tremaining: 3h 21m 2s\n",
      "3500:\tlearn: 0.1311629\ttest: 0.1538426\tbest: 0.1538105 (3191)\ttotal: 42.5s\tremaining: 3h 21m 45s\n",
      "bestTest = 0.1538104969\n",
      "bestIteration = 3191\n",
      "Shrink model to first 3192 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_kaplan\n",
      "0:\tlearn: 0.1764088\ttest: 0.1750198\tbest: 0.1750198 (0)\ttotal: 11.6ms\tremaining: 3h 12m 57s\n",
      "500:\tlearn: 0.1515116\ttest: 0.1558824\tbest: 0.1558824 (500)\ttotal: 5.64s\tremaining: 3h 7m 33s\n",
      "1000:\tlearn: 0.1462684\ttest: 0.1546236\tbest: 0.1546202 (998)\ttotal: 11.3s\tremaining: 3h 7m 55s\n",
      "1500:\tlearn: 0.1423669\ttest: 0.1539908\tbest: 0.1539908 (1500)\ttotal: 17.7s\tremaining: 3h 16m 2s\n",
      "2000:\tlearn: 0.1390634\ttest: 0.1537650\tbest: 0.1537637 (1997)\ttotal: 24.1s\tremaining: 3h 20m 14s\n",
      "2500:\tlearn: 0.1361065\ttest: 0.1536374\tbest: 0.1536367 (2498)\ttotal: 30s\tremaining: 3h 19m 29s\n",
      "3000:\tlearn: 0.1333706\ttest: 0.1534227\tbest: 0.1534227 (3000)\ttotal: 36.1s\tremaining: 3h 20m 5s\n",
      "3500:\tlearn: 0.1308379\ttest: 0.1532845\tbest: 0.1532735 (3482)\ttotal: 42.3s\tremaining: 3h 20m 53s\n",
      "4000:\tlearn: 0.1284273\ttest: 0.1532613\tbest: 0.1532593 (3990)\ttotal: 48.5s\tremaining: 3h 21m 13s\n",
      "4500:\tlearn: 0.1261221\ttest: 0.1533381\tbest: 0.1532262 (4041)\ttotal: 54.9s\tremaining: 3h 22m 20s\n",
      "bestTest = 0.153226204\n",
      "bestIteration = 4041\n",
      "Shrink model to first 4042 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_kaplan\n",
      "0:\tlearn: 0.1760690\ttest: 0.1781163\tbest: 0.1781163 (0)\ttotal: 11.8ms\tremaining: 3h 16m 16s\n",
      "500:\tlearn: 0.1510184\ttest: 0.1586642\tbest: 0.1586642 (500)\ttotal: 5.42s\tremaining: 3h 5s\n",
      "1000:\tlearn: 0.1458109\ttest: 0.1571779\tbest: 0.1571777 (999)\ttotal: 11.4s\tremaining: 3h 9m 42s\n",
      "1500:\tlearn: 0.1419099\ttest: 0.1565074\tbest: 0.1565009 (1466)\ttotal: 17.8s\tremaining: 3h 16m 48s\n",
      "2000:\tlearn: 0.1383160\ttest: 0.1561359\tbest: 0.1561335 (1997)\ttotal: 23.8s\tremaining: 3h 17m 48s\n",
      "2500:\tlearn: 0.1352391\ttest: 0.1559989\tbest: 0.1559895 (2421)\ttotal: 30.1s\tremaining: 3h 20m 16s\n",
      "3000:\tlearn: 0.1323474\ttest: 0.1557624\tbest: 0.1557565 (2999)\ttotal: 36.2s\tremaining: 3h 20m 17s\n",
      "3500:\tlearn: 0.1296850\ttest: 0.1556946\tbest: 0.1556814 (3395)\ttotal: 42.6s\tremaining: 3h 21m 52s\n",
      "4000:\tlearn: 0.1271259\ttest: 0.1556355\tbest: 0.1556348 (3994)\ttotal: 48.7s\tremaining: 3h 22m 2s\n",
      "4500:\tlearn: 0.1246915\ttest: 0.1556003\tbest: 0.1556003 (4500)\ttotal: 55s\tremaining: 3h 22m 54s\n",
      "5000:\tlearn: 0.1224058\ttest: 0.1556399\tbest: 0.1555408 (4744)\ttotal: 1m 1s\tremaining: 3h 22m 30s\n",
      "bestTest = 0.1555407896\n",
      "bestIteration = 4744\n",
      "Shrink model to first 4745 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_kaplan\n",
      "0:\tlearn: 0.1764712\ttest: 0.1742453\tbest: 0.1742453 (0)\ttotal: 11.5ms\tremaining: 3h 10m 51s\n",
      "500:\tlearn: 0.1511659\ttest: 0.1577345\tbest: 0.1577313 (499)\ttotal: 5.35s\tremaining: 2h 57m 49s\n",
      "1000:\tlearn: 0.1458718\ttest: 0.1570202\tbest: 0.1570202 (1000)\ttotal: 11.8s\tremaining: 3h 15m 47s\n",
      "1500:\tlearn: 0.1418056\ttest: 0.1568347\tbest: 0.1568026 (1466)\ttotal: 18.1s\tremaining: 3h 20m 28s\n",
      "2000:\tlearn: 0.1383094\ttest: 0.1567861\tbest: 0.1567842 (1999)\ttotal: 24.6s\tremaining: 3h 24m 44s\n",
      "2500:\tlearn: 0.1351584\ttest: 0.1567302\tbest: 0.1566825 (2335)\ttotal: 31.1s\tremaining: 3h 27m 3s\n",
      "bestTest = 0.1566824629\n",
      "bestIteration = 2335\n",
      "Shrink model to first 2336 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_kaplan\n",
      "0:\tlearn: 0.1764928\ttest: 0.1741790\tbest: 0.1741790 (0)\ttotal: 8.71ms\tremaining: 2h 25m 14s\n",
      "500:\tlearn: 0.1515514\ttest: 0.1563570\tbest: 0.1563570 (500)\ttotal: 5.47s\tremaining: 3h 1m 45s\n",
      "1000:\tlearn: 0.1463578\ttest: 0.1552994\tbest: 0.1552852 (996)\ttotal: 11.4s\tremaining: 3h 9m 3s\n",
      "1500:\tlearn: 0.1424089\ttest: 0.1548863\tbest: 0.1548863 (1500)\ttotal: 17.3s\tremaining: 3h 11m 53s\n",
      "2000:\tlearn: 0.1389585\ttest: 0.1546072\tbest: 0.1546036 (1996)\ttotal: 23.3s\tremaining: 3h 13m 57s\n",
      "2500:\tlearn: 0.1358786\ttest: 0.1545755\tbest: 0.1545604 (2488)\ttotal: 29.2s\tremaining: 3h 14m 25s\n",
      "3000:\tlearn: 0.1330504\ttest: 0.1545120\tbest: 0.1545054 (2974)\ttotal: 35.4s\tremaining: 3h 15m 50s\n",
      "3500:\tlearn: 0.1304210\ttest: 0.1544453\tbest: 0.1544225 (3283)\ttotal: 41.2s\tremaining: 3h 15m 33s\n",
      "4000:\tlearn: 0.1279756\ttest: 0.1544817\tbest: 0.1544123 (3567)\ttotal: 47.7s\tremaining: 3h 17m 43s\n",
      "bestTest = 0.1544122921\n",
      "bestIteration = 3567\n",
      "Shrink model to first 3568 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_kaplan\n",
      "0:\tlearn: 0.1762187\ttest: 0.1767416\tbest: 0.1767416 (0)\ttotal: 8.61ms\tremaining: 2h 23m 30s\n",
      "500:\tlearn: 0.1510718\ttest: 0.1592101\tbest: 0.1592101 (500)\ttotal: 5.63s\tremaining: 3h 7m 8s\n",
      "1000:\tlearn: 0.1456179\ttest: 0.1579656\tbest: 0.1579656 (1000)\ttotal: 11.6s\tremaining: 3h 12m 49s\n",
      "1500:\tlearn: 0.1414672\ttest: 0.1574997\tbest: 0.1574948 (1495)\ttotal: 17.8s\tremaining: 3h 17m 4s\n",
      "2000:\tlearn: 0.1380023\ttest: 0.1571705\tbest: 0.1571555 (1978)\ttotal: 24.2s\tremaining: 3h 21m 32s\n",
      "2500:\tlearn: 0.1348991\ttest: 0.1570705\tbest: 0.1570684 (2476)\ttotal: 30.5s\tremaining: 3h 22m 58s\n",
      "3000:\tlearn: 0.1319206\ttest: 0.1570732\tbest: 0.1570522 (2860)\ttotal: 36.4s\tremaining: 3h 21m 45s\n",
      "bestTest = 0.1570521723\n",
      "bestIteration = 2860\n",
      "Shrink model to first 2861 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_kaplan\n",
      "0:\tlearn: 0.1759158\ttest: 0.1793696\tbest: 0.1793696 (0)\ttotal: 13.8ms\tremaining: 3h 49m 47s\n",
      "500:\tlearn: 0.1513074\ttest: 0.1585006\tbest: 0.1585006 (500)\ttotal: 5.34s\tremaining: 2h 57m 37s\n",
      "1000:\tlearn: 0.1460723\ttest: 0.1569840\tbest: 0.1569840 (1000)\ttotal: 11.5s\tremaining: 3h 11m 10s\n",
      "1500:\tlearn: 0.1421496\ttest: 0.1563391\tbest: 0.1563391 (1500)\ttotal: 17.6s\tremaining: 3h 15m 18s\n",
      "2000:\tlearn: 0.1387356\ttest: 0.1558900\tbest: 0.1558900 (2000)\ttotal: 23.9s\tremaining: 3h 19m 4s\n",
      "2500:\tlearn: 0.1356358\ttest: 0.1555702\tbest: 0.1555697 (2499)\ttotal: 30.1s\tremaining: 3h 20m 6s\n",
      "3000:\tlearn: 0.1328105\ttest: 0.1554468\tbest: 0.1554360 (2992)\ttotal: 36.8s\tremaining: 3h 23m 48s\n",
      "3500:\tlearn: 0.1301425\ttest: 0.1553905\tbest: 0.1553793 (3188)\ttotal: 43.5s\tremaining: 3h 26m 13s\n",
      "4000:\tlearn: 0.1276816\ttest: 0.1553878\tbest: 0.1553647 (3717)\ttotal: 50.1s\tremaining: 3h 27m 40s\n",
      "bestTest = 0.1553646691\n",
      "bestIteration = 3717\n",
      "Shrink model to first 3718 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_kaplan\n",
      "0:\tlearn: 0.1760691\ttest: 0.1781318\tbest: 0.1781318 (0)\ttotal: 9.68ms\tremaining: 2h 41m 24s\n",
      "500:\tlearn: 0.1512087\ttest: 0.1590164\tbest: 0.1590154 (499)\ttotal: 5.83s\tremaining: 3h 13m 43s\n",
      "1000:\tlearn: 0.1460010\ttest: 0.1577442\tbest: 0.1577442 (1000)\ttotal: 11.8s\tremaining: 3h 16m 56s\n",
      "1500:\tlearn: 0.1419611\ttest: 0.1573106\tbest: 0.1573093 (1497)\ttotal: 18.4s\tremaining: 3h 24m 31s\n",
      "2000:\tlearn: 0.1386566\ttest: 0.1570318\tbest: 0.1570199 (1981)\ttotal: 24.7s\tremaining: 3h 25m 40s\n",
      "2500:\tlearn: 0.1355754\ttest: 0.1568819\tbest: 0.1568537 (2341)\ttotal: 31s\tremaining: 3h 26m 11s\n",
      "3000:\tlearn: 0.1327138\ttest: 0.1566743\tbest: 0.1566612 (2991)\ttotal: 37.3s\tremaining: 3h 26m 26s\n",
      "3500:\tlearn: 0.1301125\ttest: 0.1566477\tbest: 0.1566269 (3263)\ttotal: 43.8s\tremaining: 3h 27m 41s\n",
      "4000:\tlearn: 0.1275610\ttest: 0.1566241\tbest: 0.1566241 (4000)\ttotal: 50.4s\tremaining: 3h 29m 9s\n",
      "4500:\tlearn: 0.1251760\ttest: 0.1567741\tbest: 0.1566068 (4049)\ttotal: 56.6s\tremaining: 3h 28m 48s\n",
      "bestTest = 0.1566067586\n",
      "bestIteration = 4049\n",
      "Shrink model to first 4050 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_kaplan\n",
      "0:\tlearn: 0.1761428\ttest: 0.1774015\tbest: 0.1774015 (0)\ttotal: 9.14ms\tremaining: 2h 32m 24s\n",
      "500:\tlearn: 0.1515392\ttest: 0.1586494\tbest: 0.1586448 (499)\ttotal: 5.59s\tremaining: 3h 6m 2s\n",
      "1000:\tlearn: 0.1464440\ttest: 0.1570582\tbest: 0.1570582 (1000)\ttotal: 11.7s\tremaining: 3h 14m 28s\n",
      "1500:\tlearn: 0.1426020\ttest: 0.1564577\tbest: 0.1564556 (1499)\ttotal: 18.4s\tremaining: 3h 23m 33s\n",
      "2000:\tlearn: 0.1392022\ttest: 0.1560857\tbest: 0.1560601 (1976)\ttotal: 24.7s\tremaining: 3h 25m 29s\n",
      "2500:\tlearn: 0.1362162\ttest: 0.1557416\tbest: 0.1557416 (2500)\ttotal: 31.3s\tremaining: 3h 27m 49s\n",
      "3000:\tlearn: 0.1333436\ttest: 0.1556452\tbest: 0.1556337 (2957)\ttotal: 37.6s\tremaining: 3h 28m 18s\n",
      "3500:\tlearn: 0.1306369\ttest: 0.1555654\tbest: 0.1555342 (3459)\ttotal: 44.2s\tremaining: 3h 29m 36s\n",
      "4000:\tlearn: 0.1282110\ttest: 0.1554241\tbest: 0.1554189 (3994)\ttotal: 50.8s\tremaining: 3h 30m 47s\n",
      "4500:\tlearn: 0.1258789\ttest: 0.1553770\tbest: 0.1553550 (4341)\ttotal: 57.2s\tremaining: 3h 30m 49s\n",
      "5000:\tlearn: 0.1236335\ttest: 0.1553225\tbest: 0.1552990 (4884)\ttotal: 1m 3s\tremaining: 3h 31m 18s\n",
      "bestTest = 0.1552990267\n",
      "bestIteration = 4884\n",
      "Shrink model to first 4885 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6747413936922866\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_nelson\n",
      "0:\tlearn: 0.2713487\ttest: 0.2714857\tbest: 0.2714857 (0)\ttotal: 9.41ms\tremaining: 2h 36m 53s\n",
      "500:\tlearn: 0.2340107\ttest: 0.2435721\tbest: 0.2435721 (500)\ttotal: 5.32s\tremaining: 2h 56m 44s\n",
      "1000:\tlearn: 0.2262781\ttest: 0.2415685\tbest: 0.2415657 (995)\ttotal: 11.7s\tremaining: 3h 15m 1s\n",
      "1500:\tlearn: 0.2203583\ttest: 0.2407519\tbest: 0.2407519 (1500)\ttotal: 17.8s\tremaining: 3h 16m 52s\n",
      "2000:\tlearn: 0.2152946\ttest: 0.2404091\tbest: 0.2404078 (1952)\ttotal: 23.9s\tremaining: 3h 18m 43s\n",
      "2500:\tlearn: 0.2105309\ttest: 0.2401503\tbest: 0.2401250 (2449)\ttotal: 30.2s\tremaining: 3h 20m 41s\n",
      "3000:\tlearn: 0.2063303\ttest: 0.2397809\tbest: 0.2397809 (3000)\ttotal: 36.7s\tremaining: 3h 23m 15s\n",
      "3500:\tlearn: 0.2024005\ttest: 0.2397213\tbest: 0.2396716 (3393)\ttotal: 43.1s\tremaining: 3h 24m 18s\n",
      "4000:\tlearn: 0.1987041\ttest: 0.2395201\tbest: 0.2395201 (4000)\ttotal: 48.9s\tremaining: 3h 22m 53s\n",
      "4500:\tlearn: 0.1952722\ttest: 0.2395405\tbest: 0.2395107 (4403)\ttotal: 55s\tremaining: 3h 22m 53s\n",
      "bestTest = 0.2395107104\n",
      "bestIteration = 4403\n",
      "Shrink model to first 4404 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_nelson\n",
      "0:\tlearn: 0.2717946\ttest: 0.2673893\tbest: 0.2673893 (0)\ttotal: 11.2ms\tremaining: 3h 5m 56s\n",
      "500:\tlearn: 0.2340983\ttest: 0.2413059\tbest: 0.2413059 (500)\ttotal: 5.58s\tremaining: 3h 5m 28s\n",
      "1000:\tlearn: 0.2265143\ttest: 0.2398331\tbest: 0.2398326 (999)\ttotal: 11.4s\tremaining: 3h 9m 21s\n",
      "1500:\tlearn: 0.2210140\ttest: 0.2392990\tbest: 0.2392894 (1493)\ttotal: 17.3s\tremaining: 3h 11m 20s\n",
      "2000:\tlearn: 0.2160590\ttest: 0.2389666\tbest: 0.2389546 (1995)\ttotal: 23.7s\tremaining: 3h 17m 5s\n",
      "2500:\tlearn: 0.2116573\ttest: 0.2388497\tbest: 0.2388490 (2496)\ttotal: 30s\tremaining: 3h 19m 29s\n",
      "3000:\tlearn: 0.2075365\ttest: 0.2388731\tbest: 0.2388085 (2611)\ttotal: 36s\tremaining: 3h 19m 8s\n",
      "bestTest = 0.2388085262\n",
      "bestIteration = 2611\n",
      "Shrink model to first 2612 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_nelson\n",
      "0:\tlearn: 0.2715766\ttest: 0.2694078\tbest: 0.2694078 (0)\ttotal: 9.12ms\tremaining: 2h 31m 55s\n",
      "500:\tlearn: 0.2343532\ttest: 0.2403023\tbest: 0.2403023 (500)\ttotal: 5.54s\tremaining: 3h 4m 15s\n",
      "1000:\tlearn: 0.2265397\ttest: 0.2382056\tbest: 0.2382024 (996)\ttotal: 11.5s\tremaining: 3h 12m 1s\n",
      "1500:\tlearn: 0.2207344\ttest: 0.2373116\tbest: 0.2373072 (1498)\ttotal: 17.7s\tremaining: 3h 16m 28s\n",
      "2000:\tlearn: 0.2157542\ttest: 0.2369387\tbest: 0.2369376 (1999)\ttotal: 23.7s\tremaining: 3h 17m 1s\n",
      "2500:\tlearn: 0.2113980\ttest: 0.2367712\tbest: 0.2367586 (2458)\ttotal: 29.5s\tremaining: 3h 16m 21s\n",
      "3000:\tlearn: 0.2072714\ttest: 0.2366429\tbest: 0.2366212 (2850)\ttotal: 35.6s\tremaining: 3h 17m 21s\n",
      "3500:\tlearn: 0.2034881\ttest: 0.2363806\tbest: 0.2363806 (3500)\ttotal: 42s\tremaining: 3h 19m 24s\n",
      "4000:\tlearn: 0.1998820\ttest: 0.2364807\tbest: 0.2363671 (3504)\ttotal: 48.3s\tremaining: 3h 20m 18s\n",
      "bestTest = 0.2363670632\n",
      "bestIteration = 3504\n",
      "Shrink model to first 3505 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_nelson\n",
      "0:\tlearn: 0.2711050\ttest: 0.2736921\tbest: 0.2736921 (0)\ttotal: 9.79ms\tremaining: 2h 43m 8s\n",
      "500:\tlearn: 0.2335405\ttest: 0.2448483\tbest: 0.2448483 (500)\ttotal: 5.85s\tremaining: 3h 14m 23s\n",
      "1000:\tlearn: 0.2259188\ttest: 0.2428248\tbest: 0.2428245 (999)\ttotal: 11.9s\tremaining: 3h 17m 51s\n",
      "1500:\tlearn: 0.2200232\ttest: 0.2419456\tbest: 0.2419374 (1481)\ttotal: 18s\tremaining: 3h 19m 44s\n",
      "2000:\tlearn: 0.2148076\ttest: 0.2415619\tbest: 0.2415587 (1999)\ttotal: 24s\tremaining: 3h 19m 40s\n",
      "2500:\tlearn: 0.2101823\ttest: 0.2413378\tbest: 0.2413378 (2500)\ttotal: 29.8s\tremaining: 3h 18m 16s\n",
      "3000:\tlearn: 0.2057830\ttest: 0.2410870\tbest: 0.2410718 (2974)\ttotal: 35.6s\tremaining: 3h 17m 12s\n",
      "3500:\tlearn: 0.2017413\ttest: 0.2411315\tbest: 0.2410377 (3207)\ttotal: 41.7s\tremaining: 3h 17m 46s\n",
      "bestTest = 0.2410376595\n",
      "bestIteration = 3207\n",
      "Shrink model to first 3208 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_nelson\n",
      "0:\tlearn: 0.2716334\ttest: 0.2686787\tbest: 0.2686787 (0)\ttotal: 14.8ms\tremaining: 4h 6m\n",
      "500:\tlearn: 0.2336582\ttest: 0.2434816\tbest: 0.2434816 (500)\ttotal: 5.43s\tremaining: 3h 37s\n",
      "1000:\tlearn: 0.2256962\ttest: 0.2424027\tbest: 0.2424012 (999)\ttotal: 11.3s\tremaining: 3h 7m 38s\n",
      "1500:\tlearn: 0.2197640\ttest: 0.2420341\tbest: 0.2420307 (1495)\ttotal: 17.5s\tremaining: 3h 14m 3s\n",
      "2000:\tlearn: 0.2145074\ttest: 0.2418334\tbest: 0.2418300 (1997)\ttotal: 23.9s\tremaining: 3h 18m 18s\n",
      "2500:\tlearn: 0.2098295\ttest: 0.2417768\tbest: 0.2417534 (2164)\ttotal: 29.9s\tremaining: 3h 18m 42s\n",
      "bestTest = 0.2417534113\n",
      "bestIteration = 2164\n",
      "Shrink model to first 2165 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_nelson\n",
      "0:\tlearn: 0.2716045\ttest: 0.2689873\tbest: 0.2689873 (0)\ttotal: 9.01ms\tremaining: 2h 30m 8s\n",
      "500:\tlearn: 0.2342342\ttest: 0.2418397\tbest: 0.2418397 (500)\ttotal: 5.54s\tremaining: 3h 4m 16s\n",
      "1000:\tlearn: 0.2265020\ttest: 0.2402833\tbest: 0.2402698 (996)\ttotal: 11.6s\tremaining: 3h 12m 45s\n",
      "1500:\tlearn: 0.2205053\ttest: 0.2397339\tbest: 0.2397339 (1500)\ttotal: 17.9s\tremaining: 3h 18m 26s\n",
      "2000:\tlearn: 0.2154328\ttest: 0.2394824\tbest: 0.2394800 (1999)\ttotal: 24.2s\tremaining: 3h 21m 21s\n",
      "2500:\tlearn: 0.2109361\ttest: 0.2393328\tbest: 0.2393258 (2483)\ttotal: 30.8s\tremaining: 3h 24m 31s\n",
      "3000:\tlearn: 0.2067357\ttest: 0.2392815\tbest: 0.2392794 (2606)\ttotal: 37.5s\tremaining: 3h 27m 32s\n",
      "3500:\tlearn: 0.2026853\ttest: 0.2392434\tbest: 0.2392153 (3447)\ttotal: 44s\tremaining: 3h 28m 29s\n",
      "4000:\tlearn: 0.1989850\ttest: 0.2392277\tbest: 0.2391865 (3931)\ttotal: 50.4s\tremaining: 3h 29m 1s\n",
      "bestTest = 0.2391865305\n",
      "bestIteration = 3931\n",
      "Shrink model to first 3932 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_nelson\n",
      "0:\tlearn: 0.2712665\ttest: 0.2720674\tbest: 0.2720674 (0)\ttotal: 8.69ms\tremaining: 2h 24m 48s\n",
      "500:\tlearn: 0.2333975\ttest: 0.2457910\tbest: 0.2457910 (500)\ttotal: 5.41s\tremaining: 2h 59m 57s\n",
      "1000:\tlearn: 0.2253116\ttest: 0.2439559\tbest: 0.2439559 (1000)\ttotal: 11.8s\tremaining: 3h 15m 39s\n",
      "1500:\tlearn: 0.2192589\ttest: 0.2432763\tbest: 0.2432763 (1500)\ttotal: 18.3s\tremaining: 3h 22m 36s\n",
      "2000:\tlearn: 0.2139884\ttest: 0.2427928\tbest: 0.2427866 (1999)\ttotal: 24.8s\tremaining: 3h 26m 1s\n",
      "2500:\tlearn: 0.2093497\ttest: 0.2427545\tbest: 0.2427341 (2295)\ttotal: 31.4s\tremaining: 3h 28m 59s\n",
      "3000:\tlearn: 0.2048565\ttest: 0.2425707\tbest: 0.2425603 (2990)\ttotal: 38s\tremaining: 3h 30m 36s\n",
      "bestTest = 0.2425603041\n",
      "bestIteration = 2990\n",
      "Shrink model to first 2991 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_nelson\n",
      "0:\tlearn: 0.2709002\ttest: 0.2752810\tbest: 0.2752810 (0)\ttotal: 10.3ms\tremaining: 2h 52m 26s\n",
      "500:\tlearn: 0.2339572\ttest: 0.2441433\tbest: 0.2441433 (500)\ttotal: 6.08s\tremaining: 3h 22m 4s\n",
      "1000:\tlearn: 0.2261909\ttest: 0.2420089\tbest: 0.2420089 (1000)\ttotal: 12.1s\tremaining: 3h 21m 43s\n",
      "1500:\tlearn: 0.2203641\ttest: 0.2413763\tbest: 0.2413751 (1497)\ttotal: 18.6s\tremaining: 3h 26m 12s\n",
      "2000:\tlearn: 0.2152358\ttest: 0.2407760\tbest: 0.2407702 (1996)\ttotal: 25s\tremaining: 3h 28m 8s\n",
      "2500:\tlearn: 0.2105752\ttest: 0.2404788\tbest: 0.2404751 (2499)\ttotal: 31.3s\tremaining: 3h 28m 19s\n",
      "3000:\tlearn: 0.2063498\ttest: 0.2403724\tbest: 0.2403158 (2715)\ttotal: 37.6s\tremaining: 3h 28m 23s\n",
      "3500:\tlearn: 0.2023364\ttest: 0.2402650\tbest: 0.2402332 (3383)\ttotal: 44.1s\tremaining: 3h 29m 26s\n",
      "bestTest = 0.2402332061\n",
      "bestIteration = 3383\n",
      "Shrink model to first 3384 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_nelson\n",
      "0:\tlearn: 0.2710991\ttest: 0.2737969\tbest: 0.2737969 (0)\ttotal: 17.1ms\tremaining: 4h 44m 15s\n",
      "500:\tlearn: 0.2338567\ttest: 0.2447822\tbest: 0.2447822 (500)\ttotal: 5.68s\tremaining: 3h 8m 46s\n",
      "1000:\tlearn: 0.2261079\ttest: 0.2431117\tbest: 0.2431102 (996)\ttotal: 11.7s\tremaining: 3h 14m 12s\n",
      "1500:\tlearn: 0.2201072\ttest: 0.2423112\tbest: 0.2423112 (1500)\ttotal: 17.6s\tremaining: 3h 15m 2s\n",
      "2000:\tlearn: 0.2151170\ttest: 0.2419417\tbest: 0.2419237 (1954)\ttotal: 23.8s\tremaining: 3h 17m 33s\n",
      "2500:\tlearn: 0.2104319\ttest: 0.2417031\tbest: 0.2416238 (2342)\ttotal: 29.6s\tremaining: 3h 16m 51s\n",
      "3000:\tlearn: 0.2061204\ttest: 0.2415116\tbest: 0.2414968 (2984)\ttotal: 36s\tremaining: 3h 19m 25s\n",
      "bestTest = 0.2414968398\n",
      "bestIteration = 2984\n",
      "Shrink model to first 2985 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_nelson\n",
      "0:\tlearn: 0.2711859\ttest: 0.2729184\tbest: 0.2729184 (0)\ttotal: 8.72ms\tremaining: 2h 25m 23s\n",
      "500:\tlearn: 0.2342701\ttest: 0.2444689\tbest: 0.2444603 (499)\ttotal: 5.76s\tremaining: 3h 11m 33s\n",
      "1000:\tlearn: 0.2265723\ttest: 0.2422216\tbest: 0.2422197 (999)\ttotal: 11.9s\tremaining: 3h 18m 35s\n",
      "1500:\tlearn: 0.2208410\ttest: 0.2411252\tbest: 0.2411197 (1498)\ttotal: 18.1s\tremaining: 3h 20m 49s\n",
      "2000:\tlearn: 0.2158259\ttest: 0.2405686\tbest: 0.2405524 (1962)\ttotal: 24.4s\tremaining: 3h 22m 36s\n",
      "2500:\tlearn: 0.2114112\ttest: 0.2401272\tbest: 0.2401248 (2460)\ttotal: 30.4s\tremaining: 3h 21m 56s\n",
      "3000:\tlearn: 0.2071375\ttest: 0.2398079\tbest: 0.2398061 (2997)\ttotal: 36.4s\tremaining: 3h 21m 39s\n",
      "3500:\tlearn: 0.2032417\ttest: 0.2397862\tbest: 0.2397329 (3444)\ttotal: 42.4s\tremaining: 3h 21m 21s\n",
      "4000:\tlearn: 0.1995797\ttest: 0.2396562\tbest: 0.2396381 (3990)\ttotal: 48.5s\tremaining: 3h 21m 11s\n",
      "4500:\tlearn: 0.1960984\ttest: 0.2395703\tbest: 0.2395079 (4367)\ttotal: 54.8s\tremaining: 3h 21m 54s\n",
      "5000:\tlearn: 0.1927885\ttest: 0.2394397\tbest: 0.2394312 (4978)\ttotal: 1m 1s\tremaining: 3h 22m 51s\n",
      "5500:\tlearn: 0.1895903\ttest: 0.2394116\tbest: 0.2394065 (5438)\ttotal: 1m 7s\tremaining: 3h 24m 12s\n",
      "6000:\tlearn: 0.1866065\ttest: 0.2394031\tbest: 0.2392885 (5837)\ttotal: 1m 13s\tremaining: 3h 23m 57s\n",
      "bestTest = 0.2392884582\n",
      "bestIteration = 5837\n",
      "Shrink model to first 5838 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6776248070972858\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 1 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61913\n",
      "[500]\tvalidation_0-cox-nloglik:7.43048\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41847\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41487\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41374\n",
      "[2500]\tvalidation_0-cox-nloglik:7.41319\n",
      "[2688]\tvalidation_0-cox-nloglik:7.41317\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 2 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61516\n",
      "[500]\tvalidation_0-cox-nloglik:7.42103\n",
      "[1000]\tvalidation_0-cox-nloglik:7.40891\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40448\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40481\n",
      "[2082]\tvalidation_0-cox-nloglik:7.40500\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 3 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61993\n",
      "[500]\tvalidation_0-cox-nloglik:7.40996\n",
      "[1000]\tvalidation_0-cox-nloglik:7.39541\n",
      "[1500]\tvalidation_0-cox-nloglik:7.39046\n",
      "[2000]\tvalidation_0-cox-nloglik:7.38825\n",
      "[2500]\tvalidation_0-cox-nloglik:7.38728\n",
      "[2962]\tvalidation_0-cox-nloglik:7.38794\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 4 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.62001\n",
      "[500]\tvalidation_0-cox-nloglik:7.42765\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41295\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40712\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40434\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40317\n",
      "[3000]\tvalidation_0-cox-nloglik:7.40299\n",
      "[3345]\tvalidation_0-cox-nloglik:7.40272\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 5 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.62019\n",
      "[500]\tvalidation_0-cox-nloglik:7.43255\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41955\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41571\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41528\n",
      "[2135]\tvalidation_0-cox-nloglik:7.41503\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 6 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61935\n",
      "[500]\tvalidation_0-cox-nloglik:7.43071\n",
      "[1000]\tvalidation_0-cox-nloglik:7.42255\n",
      "[1500]\tvalidation_0-cox-nloglik:7.41853\n",
      "[2000]\tvalidation_0-cox-nloglik:7.41778\n",
      "[2429]\tvalidation_0-cox-nloglik:7.41851\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 7 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61899\n",
      "[500]\tvalidation_0-cox-nloglik:7.44408\n",
      "[1000]\tvalidation_0-cox-nloglik:7.43173\n",
      "[1500]\tvalidation_0-cox-nloglik:7.42757\n",
      "[2000]\tvalidation_0-cox-nloglik:7.42744\n",
      "[2098]\tvalidation_0-cox-nloglik:7.42741\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 8 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61997\n",
      "[500]\tvalidation_0-cox-nloglik:7.39626\n",
      "[1000]\tvalidation_0-cox-nloglik:7.37956\n",
      "[1500]\tvalidation_0-cox-nloglik:7.37325\n",
      "[2000]\tvalidation_0-cox-nloglik:7.36957\n",
      "[2500]\tvalidation_0-cox-nloglik:7.36876\n",
      "[3000]\tvalidation_0-cox-nloglik:7.36830\n",
      "[3500]\tvalidation_0-cox-nloglik:7.36853\n",
      "[3784]\tvalidation_0-cox-nloglik:7.36909\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 9 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61958\n",
      "[500]\tvalidation_0-cox-nloglik:7.42300\n",
      "[1000]\tvalidation_0-cox-nloglik:7.40921\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40498\n",
      "[2000]\tvalidation_0-cox-nloglik:7.40331\n",
      "[2500]\tvalidation_0-cox-nloglik:7.40270\n",
      "[2865]\tvalidation_0-cox-nloglik:7.40382\n",
      "--------------------------------------------------\n",
      "xgboost_cox training fold 10 efs_time2\n",
      "[0]\tvalidation_0-cox-nloglik:7.61915\n",
      "[500]\tvalidation_0-cox-nloglik:7.42752\n",
      "[1000]\tvalidation_0-cox-nloglik:7.41066\n",
      "[1500]\tvalidation_0-cox-nloglik:7.40279\n",
      "[2000]\tvalidation_0-cox-nloglik:7.39812\n",
      "[2500]\tvalidation_0-cox-nloglik:7.39609\n",
      "[3000]\tvalidation_0-cox-nloglik:7.39483\n",
      "[3466]\tvalidation_0-cox-nloglik:7.39514\n",
      "==================================================\n",
      "xgboost_cox our out of folds CV score is 0.6736017081375693\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 1 efs_time2\n",
      "0:\tlearn: -137189.3077450\ttest: -11837.7556858\tbest: -11837.7556858 (0)\ttotal: 30.2ms\tremaining: 8h 23m 58s\n",
      "500:\tlearn: -133896.7672740\ttest: -11540.1423508\tbest: -11540.1423508 (500)\ttotal: 8.72s\tremaining: 4h 49m 57s\n",
      "1000:\tlearn: -133191.8440304\ttest: -11517.6463710\tbest: -11517.5032427 (997)\ttotal: 17.2s\tremaining: 4h 45m 20s\n",
      "1500:\tlearn: -132741.8840634\ttest: -11512.5230886\tbest: -11512.3106746 (1471)\ttotal: 25.6s\tremaining: 4h 43m 37s\n",
      "2000:\tlearn: -132366.8459087\ttest: -11509.6629021\tbest: -11509.6507967 (1999)\ttotal: 33.9s\tremaining: 4h 42m 6s\n",
      "2500:\tlearn: -132057.6790453\ttest: -11510.5011325\tbest: -11509.3168143 (2154)\ttotal: 42.1s\tremaining: 4h 39m 50s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11509.31681\n",
      "bestIteration = 2154\n",
      "\n",
      "Shrink model to first 2155 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 2 efs_time2\n",
      "0:\tlearn: -137202.5995846\ttest: -11832.3506454\tbest: -11832.3506454 (0)\ttotal: 22.1ms\tremaining: 6h 8m 49s\n",
      "500:\tlearn: -133940.3587167\ttest: -11529.7069581\tbest: -11529.6968438 (499)\ttotal: 8.72s\tremaining: 4h 49m 59s\n",
      "1000:\tlearn: -133205.3190910\ttest: -11513.3032567\tbest: -11513.2352011 (984)\ttotal: 17.2s\tremaining: 4h 46m 3s\n",
      "1500:\tlearn: -132750.8584403\ttest: -11509.3071835\tbest: -11509.1168594 (1490)\ttotal: 25.8s\tremaining: 4h 45m 41s\n",
      "2000:\tlearn: -132412.4546087\ttest: -11509.3600520\tbest: -11508.5153803 (1744)\ttotal: 33.9s\tremaining: 4h 42m 11s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11508.51538\n",
      "bestIteration = 1744\n",
      "\n",
      "Shrink model to first 1745 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 3 efs_time2\n",
      "0:\tlearn: -137188.0147447\ttest: -11839.0302130\tbest: -11839.0302130 (0)\ttotal: 28.7ms\tremaining: 7h 58m 35s\n",
      "500:\tlearn: -133899.7483814\ttest: -11507.2798216\tbest: -11507.2798216 (500)\ttotal: 8.97s\tremaining: 4h 58m 12s\n",
      "1000:\tlearn: -133135.5614495\ttest: -11490.4942199\tbest: -11490.4768909 (998)\ttotal: 17.6s\tremaining: 4h 52m 34s\n",
      "1500:\tlearn: -132672.9874902\ttest: -11486.7770410\tbest: -11486.6467015 (1476)\ttotal: 26s\tremaining: 4h 48m 38s\n",
      "2000:\tlearn: -132326.4175503\ttest: -11486.9959317\tbest: -11485.6496902 (1722)\ttotal: 34.3s\tremaining: 4h 45m 31s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11485.64969\n",
      "bestIteration = 1722\n",
      "\n",
      "Shrink model to first 1723 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 4 efs_time2\n",
      "0:\tlearn: -137198.5745979\ttest: -11832.4474258\tbest: -11832.4474258 (0)\ttotal: 19.6ms\tremaining: 5h 27m 24s\n",
      "500:\tlearn: -133923.4990259\ttest: -11533.8620694\tbest: -11533.8620694 (500)\ttotal: 8.7s\tremaining: 4h 49m 7s\n",
      "1000:\tlearn: -133217.2632634\ttest: -11517.6340456\tbest: -11517.6340456 (1000)\ttotal: 17.2s\tremaining: 4h 45m 53s\n",
      "1500:\tlearn: -132743.8501029\ttest: -11514.0613720\tbest: -11514.0544405 (1424)\ttotal: 25.6s\tremaining: 4h 43m 16s\n",
      "2000:\tlearn: -132423.1067254\ttest: -11514.6928720\tbest: -11513.5953222 (1528)\ttotal: 33.7s\tremaining: 4h 40m 27s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11513.59532\n",
      "bestIteration = 1528\n",
      "\n",
      "Shrink model to first 1529 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 5 efs_time2\n",
      "0:\tlearn: -137203.5696104\ttest: -11833.0166421\tbest: -11833.0166421 (0)\ttotal: 21.5ms\tremaining: 5h 59m 1s\n",
      "500:\tlearn: -133910.1424455\ttest: -11545.1267568\tbest: -11545.1267568 (500)\ttotal: 8.9s\tremaining: 4h 55m 54s\n",
      "1000:\tlearn: -133128.9915965\ttest: -11535.9627049\tbest: -11535.9350293 (999)\ttotal: 17.5s\tremaining: 4h 51m 35s\n",
      "1500:\tlearn: -132664.8583587\ttest: -11533.8370944\tbest: -11533.7920994 (1496)\ttotal: 26.1s\tremaining: 4h 49m 10s\n",
      "2000:\tlearn: -132329.5297288\ttest: -11534.8844650\tbest: -11532.9128559 (1727)\ttotal: 34.3s\tremaining: 4h 44m 58s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11532.91286\n",
      "bestIteration = 1727\n",
      "\n",
      "Shrink model to first 1728 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 6 efs_time2\n",
      "0:\tlearn: -137215.7949355\ttest: -11823.6207905\tbest: -11823.6207905 (0)\ttotal: 21.6ms\tremaining: 5h 59m 53s\n",
      "500:\tlearn: -133911.9780328\ttest: -11528.5667436\tbest: -11528.5485366 (499)\ttotal: 8.76s\tremaining: 4h 51m 16s\n",
      "1000:\tlearn: -133187.1551511\ttest: -11520.8805309\tbest: -11520.5931721 (965)\ttotal: 17.4s\tremaining: 4h 49m 21s\n",
      "1500:\tlearn: -132795.6323969\ttest: -11520.8100593\tbest: -11520.3434723 (1166)\ttotal: 25.8s\tremaining: 4h 46m 34s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11520.34347\n",
      "bestIteration = 1166\n",
      "\n",
      "Shrink model to first 1167 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 7 efs_time2\n",
      "0:\tlearn: -137210.5079381\ttest: -11823.4044431\tbest: -11823.4044431 (0)\ttotal: 25.4ms\tremaining: 7h 3m 25s\n",
      "500:\tlearn: -133902.4897901\ttest: -11553.2026922\tbest: -11553.1776458 (499)\ttotal: 9.15s\tremaining: 5h 4m 10s\n",
      "1000:\tlearn: -133142.8724027\ttest: -11538.1355336\tbest: -11538.0609056 (999)\ttotal: 17.7s\tremaining: 4h 54m 19s\n",
      "1500:\tlearn: -132701.7939306\ttest: -11536.8775464\tbest: -11536.6421771 (1437)\ttotal: 26.3s\tremaining: 4h 51m 21s\n",
      "2000:\tlearn: -132346.6109545\ttest: -11535.4494672\tbest: -11535.4074718 (1995)\ttotal: 34.5s\tremaining: 4h 46m 48s\n",
      "2500:\tlearn: -132027.9590527\ttest: -11537.4307718\tbest: -11535.2367362 (2026)\ttotal: 42.6s\tremaining: 4h 43m 24s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11535.23674\n",
      "bestIteration = 2026\n",
      "\n",
      "Shrink model to first 2027 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 8 efs_time2\n",
      "0:\tlearn: -137206.5982321\ttest: -11823.7739184\tbest: -11823.7739184 (0)\ttotal: 25.6ms\tremaining: 7h 6m 13s\n",
      "500:\tlearn: -133984.1446242\ttest: -11480.0750298\tbest: -11480.0577321 (498)\ttotal: 8.81s\tremaining: 4h 52m 55s\n",
      "1000:\tlearn: -133216.4401803\ttest: -11462.1081496\tbest: -11462.0405028 (992)\ttotal: 17.7s\tremaining: 4h 54m 17s\n",
      "1500:\tlearn: -132805.0523778\ttest: -11459.0410072\tbest: -11458.8298935 (1447)\ttotal: 26.2s\tremaining: 4h 50m 3s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11458.82989\n",
      "bestIteration = 1447\n",
      "\n",
      "Shrink model to first 1448 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 9 efs_time2\n",
      "0:\tlearn: -137190.0338615\ttest: -11838.3203785\tbest: -11838.3203785 (0)\ttotal: 22.9ms\tremaining: 6h 21m 12s\n",
      "500:\tlearn: -133931.2039681\ttest: -11534.9498983\tbest: -11534.9498983 (500)\ttotal: 8.79s\tremaining: 4h 52m 18s\n",
      "1000:\tlearn: -133194.1818632\ttest: -11522.4211842\tbest: -11522.4211842 (1000)\ttotal: 17.5s\tremaining: 4h 51m 39s\n",
      "1500:\tlearn: -132763.2264564\ttest: -11520.7876286\tbest: -11520.3224800 (1480)\ttotal: 26.1s\tremaining: 4h 49m 19s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11520.32248\n",
      "bestIteration = 1480\n",
      "\n",
      "Shrink model to first 1481 iterations.\n",
      "--------------------------------------------------\n",
      "catboost_cox training fold 10 efs_time2\n",
      "0:\tlearn: -137188.5978555\ttest: -11837.4850753\tbest: -11837.4850753 (0)\ttotal: 21ms\tremaining: 5h 50m 4s\n",
      "500:\tlearn: -133915.6389266\ttest: -11541.3203682\tbest: -11541.3203682 (500)\ttotal: 8.75s\tremaining: 4h 51m 5s\n",
      "1000:\tlearn: -133182.3812277\ttest: -11521.7482195\tbest: -11521.6902021 (998)\ttotal: 17.2s\tremaining: 4h 46m 44s\n",
      "1500:\tlearn: -132698.5544284\ttest: -11514.7232231\tbest: -11514.7032691 (1499)\ttotal: 25.8s\tremaining: 4h 46m 20s\n",
      "2000:\tlearn: -132339.4706809\ttest: -11512.7460420\tbest: -11512.1765160 (1905)\ttotal: 34.2s\tremaining: 4h 43m 54s\n",
      "2500:\tlearn: -132040.2051440\ttest: -11513.0673654\tbest: -11511.9372478 (2276)\ttotal: 42.3s\tremaining: 4h 41m\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = -11511.93725\n",
      "bestIteration = 2276\n",
      "\n",
      "Shrink model to first 2277 iterations.\n",
      "==================================================\n",
      "catboost_cox our out of folds CV score is 0.6717630574738233\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Training\n",
    "# ====================================================\n",
    "# for method in CFG.METHOD_LIST:\n",
    "#     gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n",
    "\n",
    "# kaplan-meier & nelson-aalen models\n",
    "for method in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n",
    "    gradient_boosting_model_cv_training(method, train.to_pandas(), CFG.target_col_list, FEATURES, CATS)\n",
    "# Cox models\n",
    "for method in [\"xgboost_cox\", \"catboost_cox\"]:\n",
    "    gradient_boosting_model_cv_training(method, train.to_pandas(), CFG.cox_target_col_list, FEATURES, CATS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall CV for Ensemble = 0.681416454389993\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Overall CV\n",
    "# ====================================================\n",
    "# kaplan-meier models\n",
    "oof_lgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_kaplan = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_kaplan_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# nelson-aalen models\n",
    "oof_lgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_lightgbm_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_xgb_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cat_nelson = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_y_nelson_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "# Cox models\n",
    "oof_cox_xgb = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_xgboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "oof_cox_cat = (\n",
    "    pl.read_csv(CFG.OUTPUT_DIR / f\"oof_catboost_cox_efs_time2_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\")\n",
    "    .get_column(\"prediction\")\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].clone()\n",
    "y_pred = train[[\"ID\"]].clone()\n",
    "ensamble_prediction = (\n",
    "    rankdata(oof_xgb_kaplan)\n",
    "    + rankdata(oof_cat_kaplan)\n",
    "    + rankdata(oof_lgb_kaplan)\n",
    "    + rankdata(oof_lgb_nelson)\n",
    "    + rankdata(oof_xgb_nelson)\n",
    "    + rankdata(oof_cat_nelson)\n",
    "    + rankdata(oof_cox_xgb)\n",
    "    + rankdata(oof_cox_cat)\n",
    ")\n",
    "y_pred = y_pred.with_columns(pl.Series(ensamble_prediction).alias(\"prediction\"))\n",
    "m = score(y_true.to_pandas().copy(), y_pred.to_pandas().copy(), \"ID\")\n",
    "print(\"\\nOverall CV for Ensemble =\", m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適な重み: [ 0.14141827 -0.0601381   0.18784457  0.23719507 -0.05864717  0.16703116\n",
      "  0.2721275   0.19052066]\n",
      "最適化後のスコア: -0.6826320346725909\n"
     ]
    }
   ],
   "source": [
    "def ensemble_score(weights):\n",
    "    # 重み付けした予測値を計算\n",
    "    weighted_pred = (\n",
    "        weights[0] * rankdata(oof_lgb_kaplan)\n",
    "        + weights[1] * rankdata(oof_xgb_kaplan)\n",
    "        + weights[2] * rankdata(oof_cat_kaplan)\n",
    "        + weights[3] * rankdata(oof_lgb_nelson)\n",
    "        + weights[4] * rankdata(oof_xgb_nelson)\n",
    "        + weights[5] * rankdata(oof_cat_nelson)\n",
    "        + weights[6] * rankdata(oof_cox_xgb)\n",
    "        + weights[7] * rankdata(oof_cox_cat)\n",
    "    )\n",
    "\n",
    "    y_pred = pd.DataFrame({\"ID\": train[\"ID\"], \"prediction\": weighted_pred})\n",
    "    y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].clone().to_pandas()\n",
    "\n",
    "    return -score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "\n",
    "\n",
    "# 8つのモデルの初期重みを均等に設定\n",
    "initial_weights = [1 / 8] * 8\n",
    "\n",
    "# 最適化実行\n",
    "result = minimize(ensemble_score, initial_weights, method=\"Nelder-Mead\")\n",
    "\n",
    "print(\"最適な重み:\", result.x)\n",
    "print(\"最適化後のスコア:\", result.fun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Inference functions\n",
    "# ====================================================\n",
    "def lightgbm_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"lightgbm_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def xgboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        # pred = model.predict(xgb.DMatrix(x_test, enable_categorical=True))\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list, target_col: str):\n",
    "    x_test = test_df[features]\n",
    "    if method == \"lightgbm\":\n",
    "        test_pred = lightgbm_inference(x_test, target_col)\n",
    "    if method == \"xgboost\":\n",
    "        test_pred = xgboost_inference(x_test, target_col)\n",
    "    if method == \"catboost\":\n",
    "        test_pred = catboost_inference(x_test, target_col)\n",
    "    # Cox models\n",
    "    elif method == \"xgboost_cox\":\n",
    "        test_pred = xgboost_cox_inference(x_test, target_col)\n",
    "    elif method == \"catboost_cox\":\n",
    "        test_pred = catboost_cox_inference(x_test, target_col)\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "def predicting(method_list: list, input_df: pd.DataFrame, target_col_list: list, features: list):\n",
    "    output_df = input_df.copy()\n",
    "    for target_col in target_col_list:\n",
    "        # output_df[target_col] = 0\n",
    "        for method in method_list:\n",
    "            output_df[f\"{method}_pred_{target_col}\"] = gradient_boosting_model_inference(\n",
    "                method, input_df, features, target_col\n",
    "            )\n",
    "            # output_df[target_col] += CFG.model_weight_dict[method] * output_df[f\"{method}_pred_{target_col}\"]\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub shape: (3, 2)\n",
      "      ID  prediction\n",
      "0  28800        16.0\n",
      "1  28801        24.0\n",
      "2  28802         8.0\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Inference\n",
    "# ====================================================\n",
    "# kaplan-meier & nelson-aalen models\n",
    "output_df = predicting([\"lightgbm\", \"xgboost\", \"catboost\"], test.to_pandas(), CFG.target_col_list, FEATURES)\n",
    "pred_lgb_kaplan = output_df[\"lightgbm_pred_y_kaplan\"]\n",
    "pred_xgb_kaplan = output_df[\"xgboost_pred_y_kaplan\"]\n",
    "pred_cat_kaplan = output_df[\"catboost_pred_y_kaplan\"]\n",
    "pred_lgb_nelson = output_df[\"lightgbm_pred_y_nelson\"]\n",
    "pred_xgb_nelson = output_df[\"xgboost_pred_y_nelson\"]\n",
    "pred_cat_nelson = output_df[\"catboost_pred_y_nelson\"]\n",
    "# Cox models\n",
    "cox_output_df = predicting([\"xgboost_cox\", \"catboost_cox\"], test.to_pandas(), CFG.cox_target_col_list, FEATURES)\n",
    "pred_cox_xgb = cox_output_df[\"xgboost_cox_pred_efs_time2\"]\n",
    "pred_cox_cat = cox_output_df[\"catboost_cox_pred_efs_time2\"]\n",
    "\n",
    "submission = pd.read_csv(CFG.DATA_PATH / \"sample_submission.csv\")\n",
    "submission[\"prediction\"] = (\n",
    "    rankdata(pred_lgb_kaplan)\n",
    "    + rankdata(pred_xgb_kaplan)\n",
    "    + rankdata(pred_cat_kaplan)\n",
    "    + rankdata(pred_lgb_nelson)\n",
    "    + rankdata(pred_xgb_nelson)\n",
    "    + rankdata(pred_cat_nelson)\n",
    "    + rankdata(pred_cox_xgb)\n",
    "    + rankdata(pred_cox_cat)\n",
    ")\n",
    "submission.to_csv(CFG.OUTPUT_DIR / \"submission.csv\", index=False)\n",
    "print(\"Sub shape:\", submission.shape)\n",
    "print(submission.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
