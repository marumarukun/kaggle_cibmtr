{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import config  # edit config.py as needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import safetensors\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from jinja2 import Template\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter, NelsonAalenFitter\n",
    "from metric import score  # edit metric.py as needed\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from scipy.stats import rankdata\n",
    "from seed import seed_everything  # edit seed.py as needed\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# TODO: kaggle上でのpeft, bitsandbytesのインストール\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    DRY_RUN = False\n",
    "    EXP_NAME = config.EXP_NAME\n",
    "    AUTHOR = \"marumarukun\"\n",
    "    COMPETITION = config.KAGGLE_COMPETITION_NAME\n",
    "    DATA_PATH = config.COMP_DATASET_DIR\n",
    "    OUTPUT_DIR = config.OUTPUT_DIR\n",
    "    MODEL_PATH = config.OUTPUT_DIR / \"models\"  # モデル作成・実験時はこちらを使用\n",
    "    # MODEL_PATH = config.ARTIFACT_EXP_DIR(config.EXP_NAME) / \"models\"  # 提出時はこちらを使用\n",
    "    METHOD_LIST = [\"lightgbm\", \"xgboost\", \"catboost\"]\n",
    "    SEED = 42\n",
    "    n_folds = 2 if DRY_RUN else 5\n",
    "    target_col = \"y\"\n",
    "    # cox_target_col_list = [\"efs_time2\"]\n",
    "    # group_col = \"race_group\"  # Required for GroupKFold (edit as needed)\n",
    "    stratified_col = \"race_group_efs\"  # Required for StratifiedKFold (edit as needed)\n",
    "\n",
    "    # model\n",
    "    # model_path = \"unsloth/gemma-2-9b-it-bnb-4bit\"\n",
    "    model_path = \"unsloth/gemma-2-2b-it-bnb-4bit\"\n",
    "    metric = \"rmse\"\n",
    "    max_length = 1024\n",
    "    bf16 = is_torch_bf16_gpu_available()\n",
    "    fp16 = False if bf16 else True\n",
    "    learning_rate = 0.0001\n",
    "    epochs = 2\n",
    "    per_device_train_batch_size = 4\n",
    "    gradient_accumulation_steps = 16\n",
    "    per_device_eval_batch_size = 8\n",
    "    steps = 50\n",
    "    lr_scheduler_type = \"cosine\"\n",
    "    weight_decay = 0.01\n",
    "    optim = \"adamw_torch_fused\"\n",
    "    lora_r = 16\n",
    "    lora_alpha = 32\n",
    "    lora_dropout = 0.05\n",
    "    lora_bias = \"none\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "seed_everything(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "train = pl.read_csv(CFG.DATA_PATH / \"train.csv\", try_parse_dates=True)\n",
    "test = pl.read_csv(CFG.DATA_PATH / \"test.csv\", try_parse_dates=True)\n",
    "\n",
    "if CFG.DRY_RUN:\n",
    "    train = train.sample(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# fold column\n",
    "# ====================================================\n",
    "# race_group_efs列を作成\n",
    "train = train.with_columns((pl.col(\"race_group\").cast(str) + \"_\" + pl.col(\"efs\").cast(str)).alias(\"race_group_efs\"))\n",
    "\n",
    "fold_array = np.zeros(train.height)\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train, train[CFG.stratified_col]), start=1):\n",
    "    fold_array[val_idx] = fold\n",
    "train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# target column\n",
    "# ====================================================\n",
    "def transform_survival_probability(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(df[time_col], df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    return y\n",
    "\n",
    "\n",
    "y = transform_survival_probability(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y).alias(\"y\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 63)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>dri_score</th><th>psych_disturb</th><th>cyto_score</th><th>diabetes</th><th>hla_match_c_high</th><th>hla_high_res_8</th><th>tbi_status</th><th>arrhythmia</th><th>hla_low_res_6</th><th>graft_type</th><th>vent_hist</th><th>renal_issue</th><th>pulm_severe</th><th>prim_disease_hct</th><th>hla_high_res_6</th><th>cmv_status</th><th>hla_high_res_10</th><th>hla_match_dqb1_high</th><th>tce_imm_match</th><th>hla_nmdp_6</th><th>hla_match_c_low</th><th>rituximab</th><th>hla_match_drb1_low</th><th>hla_match_dqb1_low</th><th>prod_type</th><th>cyto_score_detail</th><th>conditioning_intensity</th><th>ethnicity</th><th>year_hct</th><th>obesity</th><th>mrd_hct</th><th>in_vivo_tcd</th><th>tce_match</th><th>hla_match_a_high</th><th>hepatic_severe</th><th>donor_age</th><th>prior_tumor</th><th>hla_match_b_low</th><th>peptic_ulcer</th><th>age_at_hct</th><th>hla_match_a_low</th><th>gvhd_proph</th><th>rheum_issue</th><th>sex_match</th><th>hla_match_b_high</th><th>race_group</th><th>comorbidity_score</th><th>karnofsky_score</th><th>hepatic_mild</th><th>tce_div_match</th><th>donor_related</th><th>melphalan_dose</th><th>hla_low_res_8</th><th>cardiac</th><th>hla_match_drb1_high</th><th>pulm_moderate</th><th>hla_low_res_10</th><th>efs</th><th>efs_time</th><th>race_group_efs</th><th>fold</th><th>y</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>154</td><td>2062</td><td>8068</td><td>2119</td><td>4620</td><td>5829</td><td>0</td><td>2202</td><td>3270</td><td>0</td><td>259</td><td>1915</td><td>2135</td><td>0</td><td>5284</td><td>634</td><td>7163</td><td>5199</td><td>11133</td><td>4197</td><td>2800</td><td>2148</td><td>2643</td><td>4194</td><td>0</td><td>11923</td><td>4789</td><td>587</td><td>0</td><td>1760</td><td>16597</td><td>225</td><td>18996</td><td>4301</td><td>1871</td><td>1808</td><td>1678</td><td>2565</td><td>2419</td><td>0</td><td>2390</td><td>225</td><td>2183</td><td>261</td><td>4088</td><td>0</td><td>477</td><td>870</td><td>1917</td><td>11396</td><td>158</td><td>1405</td><td>3653</td><td>2542</td><td>3352</td><td>2047</td><td>5064</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 63)\n",
       "┌─────┬───────────┬───────────────┬────────────┬───┬──────────┬────────────────┬──────┬─────┐\n",
       "│ ID  ┆ dri_score ┆ psych_disturb ┆ cyto_score ┆ … ┆ efs_time ┆ race_group_efs ┆ fold ┆ y   │\n",
       "│ --- ┆ ---       ┆ ---           ┆ ---        ┆   ┆ ---      ┆ ---            ┆ ---  ┆ --- │\n",
       "│ u32 ┆ u32       ┆ u32           ┆ u32        ┆   ┆ u32      ┆ u32            ┆ u32  ┆ u32 │\n",
       "╞═════╪═══════════╪═══════════════╪════════════╪═══╪══════════╪════════════════╪══════╪═════╡\n",
       "│ 0   ┆ 154       ┆ 2062          ┆ 8068       ┆ … ┆ 0        ┆ 0              ┆ 0    ┆ 0   │\n",
       "└─────┴───────────┴───────────────┴────────────┴───┴──────────┴────────────────┴──────┴─────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.null_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 63)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>dri_score</th><th>psych_disturb</th><th>cyto_score</th><th>diabetes</th><th>hla_match_c_high</th><th>hla_high_res_8</th><th>tbi_status</th><th>arrhythmia</th><th>hla_low_res_6</th><th>graft_type</th><th>vent_hist</th><th>renal_issue</th><th>pulm_severe</th><th>prim_disease_hct</th><th>hla_high_res_6</th><th>cmv_status</th><th>hla_high_res_10</th><th>hla_match_dqb1_high</th><th>tce_imm_match</th><th>hla_nmdp_6</th><th>hla_match_c_low</th><th>rituximab</th><th>hla_match_drb1_low</th><th>hla_match_dqb1_low</th><th>prod_type</th><th>cyto_score_detail</th><th>conditioning_intensity</th><th>ethnicity</th><th>year_hct</th><th>obesity</th><th>mrd_hct</th><th>in_vivo_tcd</th><th>tce_match</th><th>hla_match_a_high</th><th>hepatic_severe</th><th>donor_age</th><th>prior_tumor</th><th>hla_match_b_low</th><th>peptic_ulcer</th><th>age_at_hct</th><th>hla_match_a_low</th><th>gvhd_proph</th><th>rheum_issue</th><th>sex_match</th><th>hla_match_b_high</th><th>race_group</th><th>comorbidity_score</th><th>karnofsky_score</th><th>hepatic_mild</th><th>tce_div_match</th><th>donor_related</th><th>melphalan_dose</th><th>hla_low_res_8</th><th>cardiac</th><th>hla_match_drb1_high</th><th>pulm_moderate</th><th>hla_low_res_10</th><th>efs</th><th>efs_time</th><th>race_group_efs</th><th>fold</th><th>y</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>154</td><td>2062</td><td>8068</td><td>2119</td><td>4620</td><td>5829</td><td>0</td><td>2202</td><td>3270</td><td>0</td><td>259</td><td>1915</td><td>2135</td><td>0</td><td>5284</td><td>634</td><td>7163</td><td>5199</td><td>11133</td><td>4197</td><td>2800</td><td>2148</td><td>2643</td><td>4194</td><td>0</td><td>11923</td><td>4789</td><td>587</td><td>0</td><td>1760</td><td>16597</td><td>225</td><td>18996</td><td>4301</td><td>1871</td><td>1808</td><td>1678</td><td>2565</td><td>2419</td><td>0</td><td>2390</td><td>225</td><td>2183</td><td>261</td><td>4088</td><td>0</td><td>477</td><td>870</td><td>1917</td><td>11396</td><td>158</td><td>1405</td><td>3653</td><td>2542</td><td>3352</td><td>2047</td><td>5064</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 63)\n",
       "┌─────┬───────────┬───────────────┬────────────┬───┬──────────┬────────────────┬──────┬─────┐\n",
       "│ ID  ┆ dri_score ┆ psych_disturb ┆ cyto_score ┆ … ┆ efs_time ┆ race_group_efs ┆ fold ┆ y   │\n",
       "│ --- ┆ ---       ┆ ---           ┆ ---        ┆   ┆ ---      ┆ ---            ┆ ---  ┆ --- │\n",
       "│ u32 ┆ u32       ┆ u32           ┆ u32        ┆   ┆ u32      ┆ u32            ┆ u32  ┆ u32 │\n",
       "╞═════╪═══════════╪═══════════════╪════════════╪═══╪══════════╪════════════════╪══════╪═════╡\n",
       "│ 0   ┆ 154       ┆ 2062          ┆ 8068       ┆ … ┆ 0        ┆ 0              ┆ 0    ┆ 0   │\n",
       "└─────┴───────────┴───────────────┴────────────┴───┴──────────┴────────────────┴──────┴─────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 63)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>dri_score</th><th>psych_disturb</th><th>cyto_score</th><th>diabetes</th><th>hla_match_c_high</th><th>hla_high_res_8</th><th>tbi_status</th><th>arrhythmia</th><th>hla_low_res_6</th><th>graft_type</th><th>vent_hist</th><th>renal_issue</th><th>pulm_severe</th><th>prim_disease_hct</th><th>hla_high_res_6</th><th>cmv_status</th><th>hla_high_res_10</th><th>hla_match_dqb1_high</th><th>tce_imm_match</th><th>hla_nmdp_6</th><th>hla_match_c_low</th><th>rituximab</th><th>hla_match_drb1_low</th><th>hla_match_dqb1_low</th><th>prod_type</th><th>cyto_score_detail</th><th>conditioning_intensity</th><th>ethnicity</th><th>year_hct</th><th>obesity</th><th>mrd_hct</th><th>in_vivo_tcd</th><th>tce_match</th><th>hla_match_a_high</th><th>hepatic_severe</th><th>donor_age</th><th>prior_tumor</th><th>hla_match_b_low</th><th>peptic_ulcer</th><th>age_at_hct</th><th>hla_match_a_low</th><th>gvhd_proph</th><th>rheum_issue</th><th>sex_match</th><th>hla_match_b_high</th><th>race_group</th><th>comorbidity_score</th><th>karnofsky_score</th><th>hepatic_mild</th><th>tce_div_match</th><th>donor_related</th><th>melphalan_dose</th><th>hla_low_res_8</th><th>cardiac</th><th>hla_match_drb1_high</th><th>pulm_moderate</th><th>hla_low_res_10</th><th>efs</th><th>efs_time</th><th>race_group_efs</th><th>fold</th><th>y</th></tr><tr><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td><td>u32</td></tr></thead><tbody><tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 63)\n",
       "┌─────┬───────────┬───────────────┬────────────┬───┬──────────┬────────────────┬──────┬─────┐\n",
       "│ ID  ┆ dri_score ┆ psych_disturb ┆ cyto_score ┆ … ┆ efs_time ┆ race_group_efs ┆ fold ┆ y   │\n",
       "│ --- ┆ ---       ┆ ---           ┆ ---        ┆   ┆ ---      ┆ ---            ┆ ---  ┆ --- │\n",
       "│ u32 ┆ u32       ┆ u32           ┆ u32        ┆   ┆ u32      ┆ u32            ┆ u32  ┆ u32 │\n",
       "╞═════╪═══════════╪═══════════════╪════════════╪═══╪══════════╪════════════════╪══════╪═════╡\n",
       "│ 0   ┆ 0         ┆ 0             ┆ 0          ┆ … ┆ 0        ┆ 0              ┆ 0    ┆ 0   │\n",
       "└─────┴───────────┴───────────────┴────────────┴───┴──────────┴────────────────┴──────┴─────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# preprocess(欠損値補完)\n",
    "# ====================================================\n",
    "def preprocess_dataframe(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"欠損値を適切な値で埋める\"\"\"\n",
    "\n",
    "    # 数値列は-1、文字列列は\"Unknown\"で埋める\n",
    "    df = df.with_columns(cs.numeric().fill_null(-1), cs.string().fill_null(\"Unknown\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "display(train.null_count())\n",
    "\n",
    "train = preprocess_dataframe(train)\n",
    "test = preprocess_dataframe(test)\n",
    "\n",
    "display(train.null_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# prompt\n",
    "# ====================================================\n",
    "\n",
    "PROMPT_TEMPLATE = Template(\"\"\"As a medical expert specializing in hematopoietic cell transplantation (HCT), predict the survival probability for the following patient. Provide your prediction as a number between 0 and 1.\n",
    "\n",
    "PRIMARY RISK FACTORS:\n",
    "1. Disease Characteristics\n",
    "- Primary Disease: {{ prim_disease_hct }}\n",
    "- Disease Risk Index: {{ dri_score }}\n",
    "- Cytogenetic Score: {{ cyto_score_detail }}\n",
    "- MRD Status at HCT: {{ mrd_hct }}\n",
    "\n",
    "2. Patient Status\n",
    "- Age at HCT: {{ age_at_hct }} years\n",
    "- Karnofsky Performance Score: {{ karnofsky_score }}\n",
    "- HCT Comorbidity Index: {{ comorbidity_score }}\n",
    "\n",
    "3. HLA & Immunological Factors\n",
    "- 10/10 Match Score: {{ hla_high_res_10 }}/10\n",
    "- T-cell Epitope Status: {{ tce_match }}\n",
    "- TCE Immunogenicity: {{ tce_imm_match }}\n",
    "- CMV Status (D/R): {{ cmv_status }}\n",
    "\n",
    "TRANSPLANT CHARACTERISTICS:\n",
    "4. Donor & Graft Information\n",
    "- Donor Type: {{ donor_related }}\n",
    "- Donor Age: {{ donor_age }}\n",
    "- Graft Type: {{ graft_type }}\n",
    "- Sex Match (D-R): {{ sex_match }}\n",
    "\n",
    "5. Treatment Protocol\n",
    "- Conditioning Intensity: {{ conditioning_intensity }}\n",
    "- TBI Status: {{ tbi_status }}\n",
    "- GVHD Prophylaxis: {{ gvhd_proph }}\n",
    "- In-vivo T-cell Depletion: {{ in_vivo_tcd }}\n",
    "- Rituximab in Conditioning: {{ rituximab }}\n",
    "- Melphalan Dose: {{ melphalan_dose }}\n",
    "\n",
    "COMORBIDITIES & MEDICAL HISTORY:\n",
    "6. Major Organ Systems\n",
    "- Cardiac Disease: {{ cardiac }}\n",
    "- Arrhythmia: {{ arrhythmia }}\n",
    "- Pulmonary Disease (Severe): {{ pulm_severe }}\n",
    "- Pulmonary Disease (Moderate): {{ pulm_moderate }}\n",
    "- Renal Disease: {{ renal_issue }}\n",
    "- Hepatic Disease (Severe): {{ hepatic_severe }}\n",
    "- Hepatic Disease (Mild): {{ hepatic_mild }}\n",
    "\n",
    "7. Other Medical Conditions\n",
    "- Diabetes: {{ diabetes }}\n",
    "- Obesity: {{ obesity }}\n",
    "- Psychiatric Condition: {{ psych_disturb }}\n",
    "- Peptic Ulcer: {{ peptic_ulcer }}\n",
    "- Rheumatologic Disease: {{ rheum_issue }}\n",
    "- Prior Solid Tumor: {{ prior_tumor }}\n",
    "- History of Mechanical Ventilation: {{ vent_hist }}\n",
    "\n",
    "8. Demographics\n",
    "- Race: {{ race_group }}\n",
    "- Ethnicity: {{ ethnicity }}\n",
    "- Transplant Year: {{ year_hct }}\n",
    "\n",
    "Key Considerations for Survival Prediction:\n",
    "1. Disease severity and risk status\n",
    "2. HLA matching and immunological compatibility\n",
    "3. Patient fitness and comorbidity burden\n",
    "4. Donor and graft characteristics\n",
    "5. Treatment intensity and protocol\n",
    "6. Historical outcomes for similar profiles\n",
    "\n",
    "Based on these comprehensive factors, particularly noting the {{ prim_disease_hct }} diagnosis, {{ conditioning_intensity }} conditioning, and comorbidity score of {{ comorbidity_score }}, provide your survival probability prediction.\n",
    "\n",
    "Required format:\n",
    "Survival probability: [number between 0-1]\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def make_prompt_column(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    prompts = []\n",
    "    for row in df.iter_rows(named=True):\n",
    "        prompt = PROMPT_TEMPLATE.render(**row)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return df.with_columns(pl.Series(prompts).alias(\"prompt\"))\n",
    "\n",
    "\n",
    "train = make_prompt_column(train)\n",
    "test = make_prompt_column(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a medical expert specializing in hematopoietic cell transplantation (HCT), predict the survival probability for the following patient. Provide your prediction as a number between 0 and 1.\n",
      "\n",
      "PRIMARY RISK FACTORS:\n",
      "1. Disease Characteristics\n",
      "- Primary Disease: IEA\n",
      "- Disease Risk Index: N/A - non-malignant indication\n",
      "- Cytogenetic Score: Unknown\n",
      "- MRD Status at HCT: Unknown\n",
      "\n",
      "2. Patient Status\n",
      "- Age at HCT: 9.942 years\n",
      "- Karnofsky Performance Score: 90.0\n",
      "- HCT Comorbidity Index: 0.0\n",
      "\n",
      "3. HLA & Immunological Factors\n",
      "- 10/10 Match Score: -1.0/10\n",
      "- T-cell Epitope Status: Unknown\n",
      "- TCE Immunogenicity: Unknown\n",
      "- CMV Status (D/R): +/+\n",
      "\n",
      "TRANSPLANT CHARACTERISTICS:\n",
      "4. Donor & Graft Information\n",
      "- Donor Type: Unrelated\n",
      "- Donor Age: -1.0\n",
      "- Graft Type: Bone marrow\n",
      "- Sex Match (D-R): M-F\n",
      "\n",
      "5. Treatment Protocol\n",
      "- Conditioning Intensity: Unknown\n",
      "- TBI Status: No TBI\n",
      "- GVHD Prophylaxis: FKalone\n",
      "- In-vivo T-cell Depletion: Yes\n",
      "- Rituximab in Conditioning: No\n",
      "- Melphalan Dose: N/A, Mel not given\n",
      "\n",
      "COMORBIDITIES & MEDICAL HISTORY:\n",
      "6. Major Organ Systems\n",
      "- Cardiac Disease: No\n",
      "- Arrhythmia: No\n",
      "- Pulmonary Disease (Severe): No\n",
      "- Pulmonary Disease (Moderate): No\n",
      "- Renal Disease: No\n",
      "- Hepatic Disease (Severe): No\n",
      "- Hepatic Disease (Mild): No\n",
      "\n",
      "7. Other Medical Conditions\n",
      "- Diabetes: No\n",
      "- Obesity: No\n",
      "- Psychiatric Condition: No\n",
      "- Peptic Ulcer: No\n",
      "- Rheumatologic Disease: No\n",
      "- Prior Solid Tumor: No\n",
      "- History of Mechanical Ventilation: No\n",
      "\n",
      "8. Demographics\n",
      "- Race: More than one race\n",
      "- Ethnicity: Not Hispanic or Latino\n",
      "- Transplant Year: 2016\n",
      "\n",
      "Key Considerations for Survival Prediction:\n",
      "1. Disease severity and risk status\n",
      "2. HLA matching and immunological compatibility\n",
      "3. Patient fitness and comorbidity burden\n",
      "4. Donor and graft characteristics\n",
      "5. Treatment intensity and protocol\n",
      "6. Historical outcomes for similar profiles\n",
      "\n",
      "Based on these comprehensive factors, particularly noting the IEA diagnosis, Unknown conditioning, and comorbidity score of 0.0, provide your survival probability prediction.\n",
      "\n",
      "Required format:\n",
      "Survival probability: [number between 0-1]\n"
     ]
    }
   ],
   "source": [
    "print(train[\"prompt\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup model and tokenizer\n",
    "def setup_model_and_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)\n",
    "    tokenizer.add_eos_token = True\n",
    "    tokenizer.padding_side = \"right\"  # 文末に<eos>トークンを追加\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # <eos>をpad_tokenとして設定\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=CFG.lora_r,\n",
    "        lora_alpha=CFG.lora_alpha,\n",
    "        lora_dropout=CFG.lora_dropout,\n",
    "        bias=CFG.lora_bias,\n",
    "        inference_mode=False,\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # NOTE: device_mapを設定しないことで4foldの学習が可能になった\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        CFG.model_path,\n",
    "        num_labels=1,\n",
    "    )\n",
    "    model.config.use_cache = False  # キャッシュを使用しない\n",
    "    model = prepare_model_for_kbit_training(model)  # 量子化したモデルをファインチューニング可能にする\n",
    "    model = get_peft_model(model, peft_config)  # モデルにLoRAを適用\n",
    "    # model.print_trainable_parameters()\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = setup_model_and_tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize function\n",
    "def tokenize(sample):\n",
    "    return tokenizer(sample[\"prompt\"], padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # metricをRMSEに変更\n",
    "# def compute_metrics(eval_pred):\n",
    "#     preds, labels = eval_pred\n",
    "#     preds = preds.squeeze()  # (バッチサイズ, 1) -> (バッチサイズ,)\n",
    "#     rmse = np.sqrt(np.mean((preds - labels) ** 2))\n",
    "#     return {\"rmse\": rmse}\n",
    "\n",
    "\n",
    "# # # 実験結果格納用のディレクトリを作成\n",
    "# # cfg.run_name = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "# # Path(cfg.data.results_dir).mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# y_train = train[CFG.target_col].to_numpy()\n",
    "# # oof = np.zeros(len(y_train))\n",
    "# oof_rmse = []\n",
    "\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y_train)):\n",
    "#     # Setup model and tokenizer\n",
    "#     model, tokenizer = setup_model_and_tokenizer()\n",
    "\n",
    "#     # Setup dataset\n",
    "#     ds_train = Dataset.from_pandas(train_df[train_idx][[\"prompt\", \"labels\"]].clone().to_pandas())\n",
    "#     ds_val = Dataset.from_pandas(train_df[val_idx][[\"prompt\", \"labels\"]].clone().to_pandas())\n",
    "#     ds_test = Dataset.from_pandas(test_df.select(\"prompt\").clone().to_pandas())\n",
    "\n",
    "#     ds_train = ds_train.map(tokenize).remove_columns(\"prompt\")\n",
    "#     ds_val = ds_val.map(tokenize).remove_columns(\"prompt\")\n",
    "#     ds_test = ds_test.map(tokenize).remove_columns(\"prompt\")\n",
    "\n",
    "#     # Setup trainer\n",
    "#     output_dir = os.path.join(cfg.data.results_dir, f\"fold{fold}\")\n",
    "\n",
    "#     train_args = TrainingArguments(\n",
    "#         output_dir=output_dir,  # 学習結果の出力ディレクトリ\n",
    "#         fp16=cfg.gemma.fp16,  # 16ビット浮動小数点演算を使用するかどうか\n",
    "#         learning_rate=cfg.gemma.learning_rate,  # 学習率\n",
    "#         num_train_epochs=cfg.gemma.epochs,  # 学習エポック数\n",
    "#         per_device_train_batch_size=cfg.gemma.per_device_train_batch_size,  # デバイスあたりの訓練バッチサイズ\n",
    "#         per_device_eval_batch_size=cfg.gemma.per_device_eval_batch_size,  # デバイスあたりの評価バッチサイズ\n",
    "#         gradient_accumulation_steps=cfg.gemma.gradient_accumulation_steps,  # 勾配蓄積ステップ数\n",
    "#         gradient_checkpointing=True,  # 勾配チェックポイントを使用するかどうか\n",
    "#         report_to=\"none\",  # レポート出力先（なし）\n",
    "#         evaluation_strategy=\"steps\",  # 評価戦略（ステップごと）\n",
    "#         do_eval=True,  # 評価を行うかどうか\n",
    "#         eval_steps=cfg.gemma.steps,  # 評価を行うステップ間隔\n",
    "#         save_total_limit=1,  # 保存するモデルの最大数\n",
    "#         save_strategy=\"steps\",  # 保存戦略（ステップごと）\n",
    "#         save_steps=cfg.gemma.steps,  # モデルを保存するステップ間隔\n",
    "#         logging_steps=cfg.gemma.steps,  # ログを出力するステップ間隔\n",
    "#         load_best_model_at_end=True,  # 学習終了時に最良のモデルをロードするかどうか\n",
    "#         lr_scheduler_type=cfg.gemma.lr_scheduler_type,  # 学習率スケジューラーの種類\n",
    "#         metric_for_best_model=cfg.gemma.metric,  # 最良モデルを判断するための評価指標\n",
    "#         greater_is_better=True,  # 評価指標が大きいほど良いかどうか\n",
    "#         warmup_ratio=0.1,  # ウォームアップの比率\n",
    "#         weight_decay=cfg.gemma.weight_decay,  # 重み減衰\n",
    "#         save_safetensors=True,  # SafeTensorsフォーマットで保存するかどうか\n",
    "#         seed=cfg.seed,  # 乱数シード\n",
    "#         data_seed=cfg.seed,  # データシャッフル用の乱数シード\n",
    "#         optim=cfg.gemma.optim,  # 最適化アルゴリズム\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=train_args,\n",
    "#         train_dataset=ds_train,\n",
    "#         eval_dataset=ds_val,\n",
    "#         data_collator=DataCollatorWithPadding(tokenizer),\n",
    "#         tokenizer=tokenizer,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "\n",
    "#     # Train the model\n",
    "#     trainer.train()\n",
    "\n",
    "#     # Perform inference on val and test datasets\n",
    "#     pred_val = torch.softmax(torch.tensor(trainer.predict(ds_val).predictions), dim=1).numpy()[:, 1]\n",
    "#     pred_test = torch.softmax(torch.tensor(trainer.predict(ds_test).predictions), dim=1).numpy()[:, 1]\n",
    "\n",
    "#     # Save the model, predictions\n",
    "#     final_output_dir = f\"{cfg.data.results_dir}/fold{fold}/final\"\n",
    "#     trainer.save_model(final_output_dir)\n",
    "#     np.save(f\"{final_output_dir}/val.npy\", pred_val)\n",
    "#     np.save(f\"{final_output_dir}/test.npy\", pred_test)\n",
    "#     # tokenizer.save_pretrained(final_output_dir)\n",
    "\n",
    "#     # Calculate and log AUC score\n",
    "#     roc_auc = roc_auc_score(y_train[val_idx], pred_val)\n",
    "#     print(f\"Fold {fold} AUC: {roc_auc}\")\n",
    "#     oof_auc.append(roc_auc)\n",
    "\n",
    "#     # Clean up to free memory\n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "\n",
    "# print(f\"Mean AUC score across all folds: {np.mean(oof_auc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb78cb3347b94bba96eed7b362bc4d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78b6c01ff8149968a3fefbc611eb0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d5ab03c222455b8299ac0207f151e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804bff169cb14aef9c075351ba1265bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaf580fc1c1a410dbbc19a966a06c07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea2978e317d4b109d44f71472ba68c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Gemma2ForSequenceClassification were not initialized from the model checkpoint at unsloth/gemma-2-2b-it-bnb-4bit and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e953ccab08ec446fa391582919292054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e7eccc92014ea9b4d69c24d9e3a84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8c77051379457489675f7af1241249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='267' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [267/720 2:37:27 < 4:29:09, 0.03 it/s, Epoch 0.74/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>11.181000</td>\n",
       "      <td>0.064283</td>\n",
       "      <td>0.253542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.032488</td>\n",
       "      <td>0.180244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.156900</td>\n",
       "      <td>0.041320</td>\n",
       "      <td>0.203272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.196644</td>\n",
       "      <td>0.443446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.173800</td>\n",
       "      <td>0.128024</td>\n",
       "      <td>0.357805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7c56405af240>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2196\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# metricをRMSEに変更\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.squeeze()  # (バッチサイズ, 1) -> (バッチサイズ,)\n",
    "    rmse = np.sqrt(np.mean((preds - labels) ** 2))\n",
    "    return {\"rmse\": rmse}\n",
    "\n",
    "\n",
    "y_train = train[CFG.target_col].to_numpy()\n",
    "oof = np.zeros(len(y_train))\n",
    "\n",
    "for fold in range(1, CFG.n_folds + 1):\n",
    "    # Setup model and tokenizer\n",
    "    model, tokenizer = setup_model_and_tokenizer()\n",
    "\n",
    "    # Setup dataset\n",
    "    ds_train = Dataset.from_polars(\n",
    "        train.filter(pl.col(\"fold\") != fold)\n",
    "        .select([\"prompt\", CFG.target_col])\n",
    "        .rename({CFG.target_col: \"labels\"})\n",
    "        .clone()\n",
    "    )\n",
    "    ds_val = Dataset.from_polars(\n",
    "        train.filter(pl.col(\"fold\") == fold)\n",
    "        .select([\"prompt\", CFG.target_col])\n",
    "        .rename({CFG.target_col: \"labels\"})\n",
    "        .clone()\n",
    "    )\n",
    "    ds_test = Dataset.from_polars(test.select([\"prompt\"]).clone())\n",
    "\n",
    "    ds_train = ds_train.map(tokenize).remove_columns(\"prompt\")\n",
    "    ds_val = ds_val.map(tokenize).remove_columns(\"prompt\")\n",
    "    ds_test = ds_test.map(tokenize).remove_columns(\"prompt\")\n",
    "\n",
    "    # Setup trainer\n",
    "    output_dir = os.path.join(CFG.MODEL_PATH, f\"fold{fold}\")\n",
    "\n",
    "    train_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        fp16=CFG.fp16,\n",
    "        bf16=CFG.bf16,\n",
    "        learning_rate=CFG.learning_rate,\n",
    "        num_train_epochs=CFG.epochs,\n",
    "        per_device_train_batch_size=CFG.per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "        per_device_eval_batch_size=CFG.per_device_eval_batch_size,\n",
    "        gradient_checkpointing=True,\n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        do_eval=True,\n",
    "        eval_steps=CFG.steps,\n",
    "        save_total_limit=1,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=CFG.steps,\n",
    "        logging_steps=CFG.steps,\n",
    "        load_best_model_at_end=True,\n",
    "        lr_scheduler_type=CFG.lr_scheduler_type,\n",
    "        metric_for_best_model=CFG.metric,\n",
    "        greater_is_better=False,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=CFG.weight_decay,\n",
    "        optim=CFG.optim,\n",
    "        seed=CFG.SEED,\n",
    "        data_seed=CFG.SEED,\n",
    "        save_safetensors=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
