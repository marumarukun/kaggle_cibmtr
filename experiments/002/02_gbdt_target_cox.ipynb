{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import config  # edit config.py as needed\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import scipy as sp\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter, NelsonAalenFitter\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from metric import score  # edit metric.py as needed\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import rankdata\n",
    "from seed import seed_everything  # edit seed.py as needed\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COXを適用してy_coxを作成してgbdtx3作成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configurations\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    DRY_RUN = False\n",
    "    EXP_NAME = config.EXP_NAME\n",
    "    AUTHOR = \"marumarukun\"\n",
    "    COMPETITION = config.KAGGLE_COMPETITION_NAME\n",
    "    DATA_PATH = config.COMP_DATASET_DIR\n",
    "    OUTPUT_DIR = config.OUTPUT_DIR\n",
    "    MODEL_PATH = config.OUTPUT_DIR / \"models\"  # モデル作成・実験時はこちらを使用\n",
    "    # MODEL_PATH = config.ARTIFACT_EXP_DIR(config.EXP_NAME) / \"models\"  # 提出時はこちらを使用\n",
    "    METHOD_LIST = [\"lightgbm\", \"xgboost\", \"catboost\"]\n",
    "    SEED = 42\n",
    "    n_folds = 2 if DRY_RUN else 10\n",
    "    target_col_list = [\"y_cox\"]\n",
    "    # cox_target_col_list = [\"efs_time2\"]\n",
    "    # group_col = \"race_group\"  # Required for GroupKFold (edit as needed)\n",
    "    stratified_col = \"race_group_efs\"  # Required for StratifiedKFold (edit as needed)\n",
    "    num_boost_round = 100 if DRY_RUN else 1000000\n",
    "    early_stopping_round = 10 if DRY_RUN else 500  # 10÷lrで設定\n",
    "    verbose = 500\n",
    "\n",
    "    # https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html\n",
    "    regression_lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        # \"metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"subsample_freq\": 1,\n",
    "        \"seed\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "    # https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
    "    regression_xgb_params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        # \"eval_metric\": \"mae\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"max_depth\": 5,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"enable_categorical\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"device\": \"cuda\",  # cpu/gpu/cuda\n",
    "    }\n",
    "    # https://catboost.ai/docs/en/references/training-parameters/\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostregressor\n",
    "    # https://catboost.ai/docs/en/concepts/python-reference_catboostclassifier\n",
    "    regression_cat_params = {\n",
    "        \"loss_function\": \"RMSE\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"iterations\": num_boost_round,\n",
    "        # \"depth\": 5,\n",
    "        \"grow_policy\": \"Lossguide\",\n",
    "        \"random_seed\": SEED,\n",
    "        \"task_type\": \"GPU\",  # CPU/GPU\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Seed everything\n",
    "# ====================================================\n",
    "seed_everything(CFG.SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Read data\n",
    "# ====================================================\n",
    "train = pl.read_csv(CFG.DATA_PATH / \"train.csv\", try_parse_dates=True)\n",
    "test = pl.read_csv(CFG.DATA_PATH / \"test.csv\", try_parse_dates=True)\n",
    "# make index column\n",
    "# train = train.with_row_index()\n",
    "# test = test.with_row_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Preprocess(ここに前処理や特徴量エンジニアリングを記述)\n",
    "# ====================================================\n",
    "def transform_survival_probability(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    kmf.fit(durations=df[time_col], event_observed=df[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).to_numpy()\n",
    "    return y\n",
    "\n",
    "\n",
    "def transform_cumulative_hazard(df, time_col=\"efs_time\", event_col=\"efs\"):\n",
    "    naf = NelsonAalenFitter()\n",
    "    naf.fit(durations=df[time_col], event_observed=df[event_col])\n",
    "    y = naf.cumulative_hazard_at_times(df[time_col]).to_numpy()\n",
    "    return -y\n",
    "\n",
    "\n",
    "# def preprocess(df: pl.DataFrame) -> pl.DataFrame:\n",
    "#     output = df.clone()\n",
    "#     return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = preprocess(train)\n",
    "# test = preprocess(test)\n",
    "\n",
    "# Kaplan-Meier\n",
    "y_kaplan = transform_survival_probability(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_kaplan).alias(\"y_kaplan\"))\n",
    "\n",
    "# Nelson-Aalen\n",
    "y_nelson = transform_cumulative_hazard(train, time_col=\"efs_time\", event_col=\"efs\")\n",
    "train = train.with_columns(pl.Series(y_nelson).alias(\"y_nelson\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Make fold column\n",
    "# ====================================================\n",
    "# race_group_efs列を作成\n",
    "train = train.with_columns((pl.col(\"race_group\").cast(str) + \"_\" + pl.col(\"efs\").cast(str)).alias(\"race_group_efs\"))\n",
    "\n",
    "fold_array = np.zeros(train.height)\n",
    "skf = StratifiedKFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "for fold, (_, val_idx) in enumerate(skf.split(train, train[CFG.stratified_col]), start=1):\n",
    "    fold_array[val_idx] = fold\n",
    "train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n",
    "\n",
    "# fold_array = np.zeros(train.height)\n",
    "# kf = KFold(n_splits=CFG.n_folds, shuffle=True, random_state=CFG.SEED)\n",
    "# for fold, (_, val_idx) in enumerate(kf.split(train), start=1):\n",
    "#     fold_array[val_idx] = fold\n",
    "# train = train.with_columns(pl.Series(fold_array, dtype=pl.Int8).alias(\"fold\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.to_pandas()\n",
    "test = test.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57 FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'hla_match_c_high', 'hla_high_res_8', 'tbi_status', 'arrhythmia', 'hla_low_res_6', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'hla_high_res_6', 'cmv_status', 'hla_high_res_10', 'hla_match_dqb1_high', 'tce_imm_match', 'hla_nmdp_6', 'hla_match_c_low', 'rituximab', 'hla_match_drb1_low', 'hla_match_dqb1_low', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'year_hct', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hla_match_a_high', 'hepatic_severe', 'donor_age', 'prior_tumor', 'hla_match_b_low', 'peptic_ulcer', 'age_at_hct', 'hla_match_a_low', 'gvhd_proph', 'rheum_issue', 'sex_match', 'hla_match_b_high', 'race_group', 'comorbidity_score', 'karnofsky_score', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'hla_low_res_8', 'cardiac', 'hla_match_drb1_high', 'pulm_moderate', 'hla_low_res_10']\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Set categorical columns etc. (pandas operation from here)\n",
    "# ====================================================\n",
    "RMV = [\"ID\", \"efs\", \"efs_time\", \"y_kaplan\", \"y_nelson\", \"fold\", \"race_group_efs\"]\n",
    "FEATURES = [c for c in train.columns if c not in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In these features, there are 35 CATEGORICAL FEATURES: ['dri_score', 'psych_disturb', 'cyto_score', 'diabetes', 'tbi_status', 'arrhythmia', 'graft_type', 'vent_hist', 'renal_issue', 'pulm_severe', 'prim_disease_hct', 'cmv_status', 'tce_imm_match', 'rituximab', 'prod_type', 'cyto_score_detail', 'conditioning_intensity', 'ethnicity', 'obesity', 'mrd_hct', 'in_vivo_tcd', 'tce_match', 'hepatic_severe', 'prior_tumor', 'peptic_ulcer', 'gvhd_proph', 'rheum_issue', 'sex_match', 'race_group', 'hepatic_mild', 'tce_div_match', 'donor_related', 'melphalan_dose', 'cardiac', 'pulm_moderate']\n"
     ]
    }
   ],
   "source": [
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype == \"object\":\n",
    "        CATS.append(c)\n",
    "        train[c] = train[c].fillna(\"NAN\")\n",
    "        test[c] = test[c].fillna(\"NAN\")\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We LABEL ENCODE the CATEGORICAL FEATURES: dri_score, psych_disturb, cyto_score, diabetes, tbi_status, arrhythmia, graft_type, vent_hist, renal_issue, pulm_severe, prim_disease_hct, cmv_status, tce_imm_match, rituximab, prod_type, cyto_score_detail, conditioning_intensity, ethnicity, obesity, mrd_hct, in_vivo_tcd, tce_match, hepatic_severe, prior_tumor, peptic_ulcer, gvhd_proph, rheum_issue, sex_match, race_group, hepatic_mild, tce_div_match, donor_related, melphalan_dose, cardiac, pulm_moderate, "
     ]
    }
   ],
   "source": [
    "combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "# print(\"Combined data shape:\", combined.shape )\n",
    "\n",
    "# LABEL ENCODE CATEGORICAL FEATURES\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \", end=\"\")\n",
    "for c in FEATURES:\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \", end=\"\")\n",
    "        combined[c], _ = combined[c].factorize()\n",
    "        combined[c] -= combined[c].min()\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype == \"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype == \"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "\n",
    "train = combined.iloc[: len(train)].copy()\n",
    "test = combined.iloc[len(train) :].reset_index(drop=True).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Survival Cox model\n",
    "# ====================================================\n",
    "\n",
    "# create cox model's target\n",
    "train[\"efs_time2\"] = train.efs_time.copy()\n",
    "train.loc[train.efs == 0, \"efs_time2\"] *= -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Training functions\n",
    "# ====================================================\n",
    "def lightgbm_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    model = LGBMRegressor(\n",
    "        **CFG.regression_lgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        categorical_feature=categorical_features,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=CFG.early_stopping_round),\n",
    "            lgb.log_evaluation(CFG.verbose),\n",
    "        ],\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def xgboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "):\n",
    "    model = XGBRegressor(\n",
    "        **CFG.regression_xgb_cox_params,\n",
    "        n_estimators=CFG.num_boost_round,\n",
    "    )\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        verbose=CFG.verbose,\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def catboost_cox_training(\n",
    "    x_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    x_valid: pd.DataFrame,\n",
    "    y_valid: pd.DataFrame,\n",
    "    categorical_features: list,\n",
    "):\n",
    "    cat_train = Pool(data=x_train, label=y_train, cat_features=categorical_features)\n",
    "    cat_valid = Pool(data=x_valid, label=y_valid, cat_features=categorical_features)\n",
    "    model = CatBoostRegressor(**CFG.regression_cat_cox_params)\n",
    "    model.fit(\n",
    "        cat_train,\n",
    "        eval_set=[cat_valid],\n",
    "        early_stopping_rounds=CFG.early_stopping_round,\n",
    "        verbose=CFG.verbose,\n",
    "        use_best_model=True,\n",
    "    )\n",
    "    # Predict validation\n",
    "    valid_pred = model.predict(x_valid)\n",
    "    return model, valid_pred\n",
    "\n",
    "\n",
    "def gradient_boosting_model_cv_training(\n",
    "    method: str, train_df: pd.DataFrame, target_col_list: list, features: list, categorical_features: list\n",
    "):\n",
    "    # Create a numpy array to store out of folds predictions\n",
    "    for target_col in target_col_list:\n",
    "        oof_predictions = np.zeros(len(train_df))\n",
    "        for fold in range(CFG.n_folds):\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"{method} training fold {fold+1} {target_col}\")\n",
    "            x_train = train_df[train_df[\"fold\"] != fold + 1][features]\n",
    "            y_train = train_df[train_df[\"fold\"] != fold + 1][target_col]\n",
    "            x_valid = train_df[train_df[\"fold\"] == fold + 1][features]\n",
    "            y_valid = train_df[train_df[\"fold\"] == fold + 1][target_col]\n",
    "            if method == \"lightgbm\":\n",
    "                model, valid_pred = lightgbm_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            elif method == \"xgboost\":\n",
    "                model, valid_pred = xgboost_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost\":\n",
    "                model, valid_pred = catboost_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            # Cox models\n",
    "            elif method == \"xgboost_cox\":\n",
    "                model, valid_pred = xgboost_cox_training(x_train, y_train, x_valid, y_valid)\n",
    "            elif method == \"catboost_cox\":\n",
    "                model, valid_pred = catboost_cox_training(x_train, y_train, x_valid, y_valid, categorical_features)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "            # Save best model\n",
    "            save_model_path = (\n",
    "                CFG.MODEL_PATH / f\"{method}_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\"\n",
    "            )\n",
    "            save_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            pickle.dump(\n",
    "                model,\n",
    "                open(\n",
    "                    save_model_path,\n",
    "                    \"wb\",\n",
    "                ),\n",
    "            )\n",
    "            # Add to out of folds array\n",
    "            oof_predictions[train_df[\"fold\"] == fold + 1] = valid_pred\n",
    "            del x_train, x_valid, y_train, y_valid, model, valid_pred\n",
    "            gc.collect()\n",
    "\n",
    "        # Create a dataframe to store out of folds predictions\n",
    "        oof_predictions_df = pd.DataFrame()\n",
    "        oof_predictions_df[\"ID\"] = train_df[\"ID\"].values\n",
    "        oof_predictions_df[\"prediction\"] = oof_predictions\n",
    "        oof_predictions_df.to_csv(\n",
    "            CFG.OUTPUT_DIR / f\"oof_{method}_{target_col}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.csv\", index=False\n",
    "        )\n",
    "\n",
    "        # Compute out of folds metric\n",
    "        y_true = train_df[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        m = score(y_true.copy(), oof_predictions_df.copy(), \"ID\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{method} our out of folds CV score is {m}\")\n",
    "        print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "lightgbm training fold 1 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 840\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.539329\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.05871\n",
      "[1000]\tvalid_0's l2: 0.0585537\n",
      "[1500]\tvalid_0's l2: 0.0586387\n",
      "Early stopping, best iteration is:\n",
      "[1075]\tvalid_0's l2: 0.0585038\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 2 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 842\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.538669\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0572834\n",
      "[1000]\tvalid_0's l2: 0.0568498\n",
      "[1500]\tvalid_0's l2: 0.0568253\n",
      "Early stopping, best iteration is:\n",
      "[1098]\tvalid_0's l2: 0.0567936\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 3 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 842\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.538856\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0564036\n",
      "[1000]\tvalid_0's l2: 0.0557072\n",
      "[1500]\tvalid_0's l2: 0.055689\n",
      "[2000]\tvalid_0's l2: 0.0557495\n",
      "Early stopping, best iteration is:\n",
      "[1526]\tvalid_0's l2: 0.0556473\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 4 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 842\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.539416\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0589728\n",
      "[1000]\tvalid_0's l2: 0.0582357\n",
      "[1500]\tvalid_0's l2: 0.0581008\n",
      "[2000]\tvalid_0's l2: 0.058114\n",
      "Early stopping, best iteration is:\n",
      "[1948]\tvalid_0's l2: 0.0580626\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 5 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.539032\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0586197\n",
      "[1000]\tvalid_0's l2: 0.0582596\n",
      "[1500]\tvalid_0's l2: 0.0581521\n",
      "[2000]\tvalid_0's l2: 0.0582339\n",
      "Early stopping, best iteration is:\n",
      "[1724]\tvalid_0's l2: 0.0581122\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 6 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.538973\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0576635\n",
      "[1000]\tvalid_0's l2: 0.057572\n",
      "Early stopping, best iteration is:\n",
      "[832]\tvalid_0's l2: 0.0574664\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 7 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 842\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.539673\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.059047\n",
      "[1000]\tvalid_0's l2: 0.0586098\n",
      "[1500]\tvalid_0's l2: 0.058568\n",
      "[2000]\tvalid_0's l2: 0.0585474\n",
      "[2500]\tvalid_0's l2: 0.0586892\n",
      "Early stopping, best iteration is:\n",
      "[2091]\tvalid_0's l2: 0.0585189\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 8 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.539898\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0583876\n",
      "[1000]\tvalid_0's l2: 0.0577851\n",
      "[1500]\tvalid_0's l2: 0.0576021\n",
      "[2000]\tvalid_0's l2: 0.0575298\n",
      "Early stopping, best iteration is:\n",
      "[1906]\tvalid_0's l2: 0.0575027\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 9 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.539493\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0588237\n",
      "[1000]\tvalid_0's l2: 0.0584221\n",
      "[1500]\tvalid_0's l2: 0.0584588\n",
      "Early stopping, best iteration is:\n",
      "[1230]\tvalid_0's l2: 0.0583273\n",
      "--------------------------------------------------\n",
      "lightgbm training fold 10 y_nelson\n",
      "[LightGBM] [Warning] Using sparse features with CUDA is currently not supported.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 25920, number of used features: 57\n",
      "[LightGBM] [Info] Start training from score -0.539971\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\tvalid_0's l2: 0.0584059\n",
      "[1000]\tvalid_0's l2: 0.0572916\n",
      "[1500]\tvalid_0's l2: 0.0570825\n",
      "[2000]\tvalid_0's l2: 0.0570607\n",
      "Early stopping, best iteration is:\n",
      "[1573]\tvalid_0's l2: 0.0570385\n",
      "==================================================\n",
      "lightgbm our out of folds CV score is 0.6755930688356382\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "xgboost training fold 1 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27153\n",
      "[500]\tvalidation_0-rmse:0.24195\n",
      "[1000]\tvalidation_0-rmse:0.24040\n",
      "[1500]\tvalidation_0-rmse:0.24007\n",
      "[1864]\tvalidation_0-rmse:0.24020\n",
      "--------------------------------------------------\n",
      "xgboost training fold 2 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26745\n",
      "[500]\tvalidation_0-rmse:0.23987\n",
      "[1000]\tvalidation_0-rmse:0.23876\n",
      "[1500]\tvalidation_0-rmse:0.23881\n",
      "[1634]\tvalidation_0-rmse:0.23884\n",
      "--------------------------------------------------\n",
      "xgboost training fold 3 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26945\n",
      "[500]\tvalidation_0-rmse:0.23799\n",
      "[1000]\tvalidation_0-rmse:0.23640\n",
      "[1500]\tvalidation_0-rmse:0.23639\n",
      "[1606]\tvalidation_0-rmse:0.23638\n",
      "--------------------------------------------------\n",
      "xgboost training fold 4 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27373\n",
      "[500]\tvalidation_0-rmse:0.24272\n",
      "[1000]\tvalidation_0-rmse:0.24068\n",
      "[1500]\tvalidation_0-rmse:0.24051\n",
      "[2000]\tvalidation_0-rmse:0.24073\n",
      "[2080]\tvalidation_0-rmse:0.24073\n",
      "--------------------------------------------------\n",
      "xgboost training fold 5 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26870\n",
      "[500]\tvalidation_0-rmse:0.24241\n",
      "[1000]\tvalidation_0-rmse:0.24172\n",
      "[1500]\tvalidation_0-rmse:0.24161\n",
      "[1924]\tvalidation_0-rmse:0.24190\n",
      "--------------------------------------------------\n",
      "xgboost training fold 6 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.26906\n",
      "[500]\tvalidation_0-rmse:0.24065\n",
      "[1000]\tvalidation_0-rmse:0.23992\n",
      "[1500]\tvalidation_0-rmse:0.24007\n",
      "[1719]\tvalidation_0-rmse:0.24039\n",
      "--------------------------------------------------\n",
      "xgboost training fold 7 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27214\n",
      "[500]\tvalidation_0-rmse:0.24435\n",
      "[1000]\tvalidation_0-rmse:0.24287\n",
      "[1500]\tvalidation_0-rmse:0.24277\n",
      "[1847]\tvalidation_0-rmse:0.24285\n",
      "--------------------------------------------------\n",
      "xgboost training fold 8 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27537\n",
      "[500]\tvalidation_0-rmse:0.24240\n",
      "[1000]\tvalidation_0-rmse:0.24060\n",
      "[1500]\tvalidation_0-rmse:0.24023\n",
      "[2000]\tvalidation_0-rmse:0.24049\n",
      "[2252]\tvalidation_0-rmse:0.24062\n",
      "--------------------------------------------------\n",
      "xgboost training fold 9 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27384\n",
      "[500]\tvalidation_0-rmse:0.24337\n",
      "[1000]\tvalidation_0-rmse:0.24178\n",
      "[1500]\tvalidation_0-rmse:0.24167\n",
      "[1642]\tvalidation_0-rmse:0.24163\n",
      "--------------------------------------------------\n",
      "xgboost training fold 10 y_nelson\n",
      "[0]\tvalidation_0-rmse:0.27301\n",
      "[500]\tvalidation_0-rmse:0.24228\n",
      "[1000]\tvalidation_0-rmse:0.23978\n",
      "[1500]\tvalidation_0-rmse:0.23901\n",
      "[2000]\tvalidation_0-rmse:0.23911\n",
      "[2322]\tvalidation_0-rmse:0.23901\n",
      "==================================================\n",
      "xgboost our out of folds CV score is 0.6758812073682919\n",
      "==================================================\n",
      "--------------------------------------------------\n",
      "catboost training fold 1 y_nelson\n",
      "0:\tlearn: 0.2713375\ttest: 0.2714844\tbest: 0.2714844 (0)\ttotal: 25.7ms\tremaining: 7h 7m 56s\n",
      "500:\tlearn: 0.2340378\ttest: 0.2434884\tbest: 0.2434884 (500)\ttotal: 5.21s\tremaining: 2h 53m 18s\n",
      "1000:\tlearn: 0.2264156\ttest: 0.2414915\tbest: 0.2414813 (994)\ttotal: 10.9s\tremaining: 3h 1m 43s\n",
      "1500:\tlearn: 0.2206735\ttest: 0.2406414\tbest: 0.2406414 (1500)\ttotal: 16.5s\tremaining: 3h 2m 42s\n",
      "2000:\tlearn: 0.2157236\ttest: 0.2403415\tbest: 0.2403368 (1986)\ttotal: 22.2s\tremaining: 3h 4m 30s\n",
      "2500:\tlearn: 0.2110167\ttest: 0.2401422\tbest: 0.2401103 (2465)\ttotal: 27.7s\tremaining: 3h 4m 19s\n",
      "3000:\tlearn: 0.2069050\ttest: 0.2398837\tbest: 0.2398820 (2991)\ttotal: 33.4s\tremaining: 3h 5m 1s\n",
      "3500:\tlearn: 0.2029795\ttest: 0.2398046\tbest: 0.2397569 (3401)\ttotal: 39s\tremaining: 3h 4m 49s\n",
      "bestTest = 0.2397569442\n",
      "bestIteration = 3401\n",
      "Shrink model to first 3402 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 2 y_nelson\n",
      "0:\tlearn: 0.2717946\ttest: 0.2673893\tbest: 0.2673893 (0)\ttotal: 13.4ms\tremaining: 3h 42m 58s\n",
      "500:\tlearn: 0.2343056\ttest: 0.2410600\tbest: 0.2410600 (500)\ttotal: 4.77s\tremaining: 2h 38m 32s\n",
      "1000:\tlearn: 0.2267404\ttest: 0.2397343\tbest: 0.2397227 (973)\ttotal: 10.2s\tremaining: 2h 50m 11s\n",
      "1500:\tlearn: 0.2212052\ttest: 0.2392716\tbest: 0.2392585 (1491)\ttotal: 15.6s\tremaining: 2h 53m 8s\n",
      "2000:\tlearn: 0.2164230\ttest: 0.2388434\tbest: 0.2388374 (1997)\ttotal: 21.5s\tremaining: 2h 58m 44s\n",
      "2500:\tlearn: 0.2120486\ttest: 0.2387016\tbest: 0.2387003 (2493)\ttotal: 27.4s\tremaining: 3h 2m 4s\n",
      "3000:\tlearn: 0.2079645\ttest: 0.2386457\tbest: 0.2385811 (2859)\ttotal: 33.1s\tremaining: 3h 3m 20s\n",
      "3500:\tlearn: 0.2042222\ttest: 0.2386575\tbest: 0.2385612 (3204)\ttotal: 38.3s\tremaining: 3h 1m 54s\n",
      "bestTest = 0.2385611913\n",
      "bestIteration = 3204\n",
      "Shrink model to first 3205 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 3 y_nelson\n",
      "0:\tlearn: 0.2715784\ttest: 0.2694130\tbest: 0.2694130 (0)\ttotal: 9.61ms\tremaining: 2h 40m 13s\n",
      "500:\tlearn: 0.2345866\ttest: 0.2402411\tbest: 0.2402411 (500)\ttotal: 5.8s\tremaining: 3h 13m 2s\n",
      "1000:\tlearn: 0.2268519\ttest: 0.2382096\tbest: 0.2382096 (1000)\ttotal: 11.9s\tremaining: 3h 17m 13s\n",
      "1500:\tlearn: 0.2210485\ttest: 0.2373457\tbest: 0.2373401 (1495)\ttotal: 17.3s\tremaining: 3h 11m 48s\n",
      "2000:\tlearn: 0.2161833\ttest: 0.2369056\tbest: 0.2369017 (1998)\ttotal: 23.1s\tremaining: 3h 11m 41s\n",
      "2500:\tlearn: 0.2118067\ttest: 0.2366616\tbest: 0.2366561 (2457)\ttotal: 29s\tremaining: 3h 12m 58s\n",
      "3000:\tlearn: 0.2078017\ttest: 0.2365682\tbest: 0.2365429 (2966)\ttotal: 35.3s\tremaining: 3h 15m 23s\n",
      "3500:\tlearn: 0.2040610\ttest: 0.2364270\tbest: 0.2363962 (3257)\ttotal: 41.1s\tremaining: 3h 14m 57s\n",
      "bestTest = 0.2363961562\n",
      "bestIteration = 3257\n",
      "Shrink model to first 3258 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 4 y_nelson\n",
      "0:\tlearn: 0.2711050\ttest: 0.2736921\tbest: 0.2736921 (0)\ttotal: 9.6ms\tremaining: 2h 39m 59s\n",
      "500:\tlearn: 0.2336522\ttest: 0.2448346\tbest: 0.2448346 (500)\ttotal: 5.31s\tremaining: 2h 56m 37s\n",
      "1000:\tlearn: 0.2260200\ttest: 0.2426420\tbest: 0.2426420 (1000)\ttotal: 10.8s\tremaining: 2h 59m 49s\n",
      "1500:\tlearn: 0.2201697\ttest: 0.2417104\tbest: 0.2416982 (1481)\ttotal: 16.6s\tremaining: 3h 3m 47s\n",
      "2000:\tlearn: 0.2150226\ttest: 0.2411790\tbest: 0.2411764 (1999)\ttotal: 22.1s\tremaining: 3h 3m 53s\n",
      "2500:\tlearn: 0.2104545\ttest: 0.2409825\tbest: 0.2409821 (2491)\ttotal: 27.9s\tremaining: 3h 5m 32s\n",
      "3000:\tlearn: 0.2061128\ttest: 0.2407993\tbest: 0.2407922 (2976)\ttotal: 33.8s\tremaining: 3h 6m 55s\n",
      "3500:\tlearn: 0.2021895\ttest: 0.2408419\tbest: 0.2407876 (3167)\ttotal: 39.6s\tremaining: 3h 8m 5s\n",
      "4000:\tlearn: 0.1983951\ttest: 0.2408663\tbest: 0.2407867 (3605)\ttotal: 45.3s\tremaining: 3h 7m 56s\n",
      "bestTest = 0.2407867284\n",
      "bestIteration = 3605\n",
      "Shrink model to first 3606 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 5 y_nelson\n",
      "0:\tlearn: 0.2716340\ttest: 0.2686761\tbest: 0.2686761 (0)\ttotal: 9.71ms\tremaining: 2h 41m 50s\n",
      "500:\tlearn: 0.2337762\ttest: 0.2433935\tbest: 0.2433935 (500)\ttotal: 5.3s\tremaining: 2h 56m 9s\n",
      "1000:\tlearn: 0.2260150\ttest: 0.2422311\tbest: 0.2422311 (1000)\ttotal: 10.7s\tremaining: 2h 58m 17s\n",
      "1500:\tlearn: 0.2201331\ttest: 0.2418375\tbest: 0.2418350 (1430)\ttotal: 16.2s\tremaining: 2h 59m 36s\n",
      "2000:\tlearn: 0.2149740\ttest: 0.2416960\tbest: 0.2416760 (1841)\ttotal: 21.7s\tremaining: 3h 3s\n",
      "2500:\tlearn: 0.2102818\ttest: 0.2416504\tbest: 0.2416383 (2484)\ttotal: 27.3s\tremaining: 3h 1m 14s\n",
      "bestTest = 0.2416383047\n",
      "bestIteration = 2484\n",
      "Shrink model to first 2485 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 6 y_nelson\n",
      "0:\tlearn: 0.2716045\ttest: 0.2689873\tbest: 0.2689873 (0)\ttotal: 11.1ms\tremaining: 3h 4m 24s\n",
      "500:\tlearn: 0.2343361\ttest: 0.2416987\tbest: 0.2416987 (500)\ttotal: 5.02s\tremaining: 2h 46m 57s\n",
      "1000:\tlearn: 0.2267332\ttest: 0.2401218\tbest: 0.2401206 (996)\ttotal: 10.6s\tremaining: 2h 56m 38s\n",
      "1500:\tlearn: 0.2209000\ttest: 0.2394145\tbest: 0.2394145 (1500)\ttotal: 16.2s\tremaining: 2h 59m 16s\n",
      "2000:\tlearn: 0.2159360\ttest: 0.2392065\tbest: 0.2392018 (1987)\ttotal: 21.7s\tremaining: 3h 41s\n",
      "2500:\tlearn: 0.2115435\ttest: 0.2390750\tbest: 0.2390402 (2418)\ttotal: 27s\tremaining: 2h 59m 41s\n",
      "3000:\tlearn: 0.2073507\ttest: 0.2389463\tbest: 0.2389140 (2944)\ttotal: 32.7s\tremaining: 3h 1m\n",
      "3500:\tlearn: 0.2034132\ttest: 0.2388393\tbest: 0.2388151 (3248)\ttotal: 38.2s\tremaining: 3h 1m 19s\n",
      "4000:\tlearn: 0.1998078\ttest: 0.2388910\tbest: 0.2388112 (3522)\ttotal: 43.6s\tremaining: 3h 41s\n",
      "bestTest = 0.2388112107\n",
      "bestIteration = 3522\n",
      "Shrink model to first 3523 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 7 y_nelson\n",
      "0:\tlearn: 0.2712678\ttest: 0.2720812\tbest: 0.2720812 (0)\ttotal: 9.42ms\tremaining: 2h 36m 56s\n",
      "500:\tlearn: 0.2336129\ttest: 0.2457964\tbest: 0.2457964 (500)\ttotal: 5.3s\tremaining: 2h 56m 15s\n",
      "1000:\tlearn: 0.2255288\ttest: 0.2439271\tbest: 0.2439219 (970)\ttotal: 10.9s\tremaining: 3h 37s\n",
      "1500:\tlearn: 0.2195552\ttest: 0.2433098\tbest: 0.2433098 (1500)\ttotal: 16.5s\tremaining: 3h 3m 7s\n",
      "2000:\tlearn: 0.2144044\ttest: 0.2428014\tbest: 0.2427841 (1989)\ttotal: 22s\tremaining: 3h 2m 47s\n",
      "2500:\tlearn: 0.2098511\ttest: 0.2427903\tbest: 0.2427470 (2124)\ttotal: 27.6s\tremaining: 3h 3m 10s\n",
      "bestTest = 0.2427469771\n",
      "bestIteration = 2124\n",
      "Shrink model to first 2125 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 8 y_nelson\n",
      "0:\tlearn: 0.2709085\ttest: 0.2752972\tbest: 0.2752972 (0)\ttotal: 9.31ms\tremaining: 2h 35m 12s\n",
      "500:\tlearn: 0.2339837\ttest: 0.2440783\tbest: 0.2440783 (500)\ttotal: 4.98s\tremaining: 2h 45m 42s\n",
      "1000:\tlearn: 0.2262737\ttest: 0.2419735\tbest: 0.2419735 (1000)\ttotal: 9.93s\tremaining: 2h 45m 11s\n",
      "1500:\tlearn: 0.2205399\ttest: 0.2411265\tbest: 0.2411265 (1500)\ttotal: 14.8s\tremaining: 2h 44m 33s\n",
      "2000:\tlearn: 0.2154602\ttest: 0.2407063\tbest: 0.2406829 (1934)\ttotal: 19.9s\tremaining: 2h 45m 43s\n",
      "2500:\tlearn: 0.2109620\ttest: 0.2403800\tbest: 0.2403800 (2500)\ttotal: 25s\tremaining: 2h 46m 25s\n",
      "3000:\tlearn: 0.2068313\ttest: 0.2401435\tbest: 0.2401362 (2998)\ttotal: 30.2s\tremaining: 2h 47m 1s\n",
      "3500:\tlearn: 0.2028870\ttest: 0.2401485\tbest: 0.2400900 (3188)\ttotal: 35.3s\tremaining: 2h 47m 23s\n",
      "bestTest = 0.2400900196\n",
      "bestIteration = 3188\n",
      "Shrink model to first 3189 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 9 y_nelson\n",
      "0:\tlearn: 0.2710991\ttest: 0.2737969\tbest: 0.2737969 (0)\ttotal: 14.8ms\tremaining: 4h 6m 37s\n",
      "500:\tlearn: 0.2341318\ttest: 0.2447980\tbest: 0.2447970 (499)\ttotal: 5.46s\tremaining: 3h 1m 31s\n",
      "1000:\tlearn: 0.2264272\ttest: 0.2431155\tbest: 0.2431145 (988)\ttotal: 11s\tremaining: 3h 2m 44s\n",
      "1500:\tlearn: 0.2204641\ttest: 0.2422560\tbest: 0.2422560 (1500)\ttotal: 16.3s\tremaining: 3h 41s\n",
      "2000:\tlearn: 0.2155562\ttest: 0.2417099\tbest: 0.2417011 (1992)\ttotal: 21.6s\tremaining: 2h 59m 41s\n",
      "2500:\tlearn: 0.2110247\ttest: 0.2415347\tbest: 0.2414997 (2471)\ttotal: 26.9s\tremaining: 2h 58m 52s\n",
      "3000:\tlearn: 0.2067107\ttest: 0.2413295\tbest: 0.2413000 (2958)\ttotal: 32.6s\tremaining: 3h 14s\n",
      "bestTest = 0.241299978\n",
      "bestIteration = 2958\n",
      "Shrink model to first 2959 iterations.\n",
      "--------------------------------------------------\n",
      "catboost training fold 10 y_nelson\n",
      "0:\tlearn: 0.2711859\ttest: 0.2729184\tbest: 0.2729184 (0)\ttotal: 10.9ms\tremaining: 3h 1m 37s\n",
      "500:\tlearn: 0.2343477\ttest: 0.2444093\tbest: 0.2444010 (499)\ttotal: 5.18s\tremaining: 2h 52m 20s\n",
      "1000:\tlearn: 0.2267384\ttest: 0.2418752\tbest: 0.2418733 (999)\ttotal: 11s\tremaining: 3h 3m 33s\n",
      "1500:\tlearn: 0.2210785\ttest: 0.2407605\tbest: 0.2407605 (1500)\ttotal: 17.1s\tremaining: 3h 9m 7s\n",
      "2000:\tlearn: 0.2161495\ttest: 0.2402944\tbest: 0.2402870 (1976)\ttotal: 22.1s\tremaining: 3h 3m 45s\n",
      "2500:\tlearn: 0.2117665\ttest: 0.2398824\tbest: 0.2398787 (2497)\ttotal: 27.4s\tremaining: 3h 2m 22s\n",
      "3000:\tlearn: 0.2075275\ttest: 0.2396221\tbest: 0.2396178 (2997)\ttotal: 33.4s\tremaining: 3h 4m 49s\n",
      "3500:\tlearn: 0.2037353\ttest: 0.2395317\tbest: 0.2394900 (3459)\ttotal: 38.8s\tremaining: 3h 4m 15s\n",
      "4000:\tlearn: 0.2000773\ttest: 0.2394182\tbest: 0.2393889 (3898)\ttotal: 44.7s\tremaining: 3h 5m 16s\n",
      "4500:\tlearn: 0.1966364\ttest: 0.2394148\tbest: 0.2393654 (4367)\ttotal: 49.9s\tremaining: 3h 4m 4s\n",
      "5000:\tlearn: 0.1933859\ttest: 0.2393178\tbest: 0.2392872 (4876)\ttotal: 55.2s\tremaining: 3h 2m 56s\n",
      "bestTest = 0.2392872183\n",
      "bestIteration = 4876\n",
      "Shrink model to first 4877 iterations.\n",
      "==================================================\n",
      "catboost our out of folds CV score is 0.6776141438882187\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Training\n",
    "# ====================================================\n",
    "# for method in CFG.METHOD_LIST:\n",
    "#     gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n",
    "\n",
    "# # Cox models\n",
    "# for method in [\"xgboost_cox\", \"catboost_cox\"]:\n",
    "#     gradient_boosting_model_cv_training(method, train, CFG.cox_target_col_list, FEATURES, CATS)\n",
    "# Non-Cox models\n",
    "for method in [\"lightgbm\", \"xgboost\", \"catboost\"]:\n",
    "    gradient_boosting_model_cv_training(method, train, CFG.target_col_list, FEATURES, CATS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Inference functions\n",
    "# ====================================================\n",
    "def lightgbm_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"lightgbm_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def xgboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        # pred = model.predict(xgb.DMatrix(x_test, enable_categorical=True))\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_{target_col}_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "# Cox models\n",
    "def xgboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"xgboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def catboost_cox_inference(x_test: pd.DataFrame, target_col: str):\n",
    "    test_pred = np.zeros(len(x_test))\n",
    "    for fold in range(CFG.n_folds):\n",
    "        model = pickle.load(\n",
    "            open(\n",
    "                CFG.MODEL_PATH / f\"catboost_cox_efs_time2_fold{fold + 1}_seed{CFG.SEED}_ver{CFG.EXP_NAME}.pkl\",\n",
    "                \"rb\",\n",
    "            )\n",
    "        )\n",
    "        # Predict\n",
    "        pred = model.predict(x_test)\n",
    "        test_pred += pred\n",
    "    return test_pred / CFG.n_folds\n",
    "\n",
    "\n",
    "def gradient_boosting_model_inference(method: str, test_df: pd.DataFrame, features: list, target_col: str):\n",
    "    x_test = test_df[features]\n",
    "    if method == \"lightgbm\":\n",
    "        test_pred = lightgbm_inference(x_test, target_col)\n",
    "    if method == \"xgboost\":\n",
    "        test_pred = xgboost_inference(x_test, target_col)\n",
    "    if method == \"catboost\":\n",
    "        test_pred = catboost_inference(x_test, target_col)\n",
    "    # Cox models\n",
    "    elif method == \"xgboost_cox\":\n",
    "        test_pred = xgboost_cox_inference(x_test, target_col)\n",
    "    elif method == \"catboost_cox\":\n",
    "        test_pred = catboost_cox_inference(x_test, target_col)\n",
    "    return test_pred\n",
    "\n",
    "\n",
    "def predicting(input_df: pd.DataFrame, features: list):\n",
    "    output_df = input_df.copy()\n",
    "    for target_col in CFG.target_col_list:\n",
    "        # output_df[target_col] = 0\n",
    "        for method in CFG.METHOD_LIST:\n",
    "            output_df[f\"{method}_pred_{target_col}\"] = gradient_boosting_model_inference(\n",
    "                method, input_df, features, target_col\n",
    "            )\n",
    "            # output_df[target_col] += CFG.model_weight_dict[method] * output_df[f\"{method}_pred_{target_col}\"]\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28801</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28802</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  prediction\n",
       "0  28800        10.0\n",
       "1  28801        15.0\n",
       "2  28802         5.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Inference\n",
    "# ====================================================\n",
    "output_df = predicting(test, FEATURES)\n",
    "pred_lgb = output_df[\"lightgbm_pred_y\"]\n",
    "pred_xgb = output_df[\"xgboost_pred_y\"]\n",
    "pred_cat = output_df[\"catboost_pred_y\"]\n",
    "# Cox models\n",
    "pred_cox_xgb = output_df[\"xgboost_cox_pred_y\"]\n",
    "pred_cox_cat = output_df[\"catboost_cox_pred_y\"]\n",
    "\n",
    "submission = pd.read_csv(CFG.DATA_PATH / \"sample_submission.csv\")\n",
    "submission[\"prediction\"] = (\n",
    "    rankdata(pred_lgb) + rankdata(pred_xgb) + rankdata(pred_cat) + rankdata(pred_cox_xgb) + rankdata(pred_cox_cat)\n",
    ")\n",
    "submission.to_csv(CFG.OUTPUT_DIR / \"submission.csv\", index=False)\n",
    "print(\"Sub shape:\", submission.shape)\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
